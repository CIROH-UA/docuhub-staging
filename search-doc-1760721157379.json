{"searchDocs":[{"title":"AORC Data in Your Hands: User-Friendly Jupyter Notebooks for Data Retrieval and Analysis via CIROH JupyterHub Notebooks","type":0,"sectionRef":"#","url":"/docuhub-staging/blog/aorc-data-access","content":"","keywords":"","version":null},{"title":"References:​","type":1,"pageTitle":"AORC Data in Your Hands: User-Friendly Jupyter Notebooks for Data Retrieval and Analysis via CIROH JupyterHub Notebooks","url":"/docuhub-staging/blog/aorc-data-access#references","content":" Salehabadi, H., D. Tarboton, A. Nassar, A. M. Castronova, P. Dash (2025). Jupyter Notebooks for the Retrieval of AORC Data for Hydrologic Analysis, HydroShare, http://www.hydroshare.org/resource/72ea9726187e43d7b50a624f2acf591fThe development versions of these notebooks are available on GitHub: https://github.com/CUAHSI/notebooks in the Data Access Examples / AORC - Retrieval of AORC Data for Hydrologic Analysis folder.Patel, A., A. Castronova (2025). CIROH 2i2c JupyterHub, HydroShare, http://www.hydroshare.org/resource/2dd1ac86e8854d4fb9fe5fbafaec2b98 ","version":null,"tagName":"h3"},{"title":"Google Cloud Next 2025: Innovation at Scale ✨","type":0,"sectionRef":"#","url":"/docuhub-staging/blog/april-2025-update","content":"","keywords":"","version":null},{"title":"📢 Google Distributed Cloud: Bringing Gemini On-Premises​","type":1,"pageTitle":"Google Cloud Next 2025: Innovation at Scale ✨","url":"/docuhub-staging/blog/april-2025-update#-google-distributed-cloud-bringing-gemini-on-premises","content":" Perhaps the most exciting announcement was that Gemini is now available on-premises via Google Distributed Cloud. This is a game-changer for organizations with strict data sovereignty requirements or specialized computing needs. Running on NVIDIA Confidential Computing and NVIDIA Blackwell infrastructure, this solution brings together:  Cutting-edge AI innovationRobust data sovereignty controlsEnterprise-grade security  This addresses the most significant barriers many organizations face when adopting advanced AI capabilities. Learn more about this integration here: NVIDIA and Google Cloud Partnership  ","version":null,"tagName":"h2"},{"title":"📢 Infrastructure Advances: Power to Transform​","type":1,"pageTitle":"Google Cloud Next 2025: Innovation at Scale ✨","url":"/docuhub-staging/blog/april-2025-update#-infrastructure-advances-power-to-transform","content":" The infrastructure announcements were equally impressive, with serious power upgrades including:  Ironwood TPUs offering massive acceleration for machine learning workloadsNVIDIA Blackwell GPUs providing unprecedented computing powerRubin GPUs extending capabilities for specialized workloads  These hardware innovations will enable entirely new classes of applications and make existing workloads significantly more efficient.    Check out all the online content that's now available, including:  A 10-minute recap of the opening keynote A replay of the developer keynote All 200+ announcements at Next 25 More than 150 sessions, demos, and workshops A recap of the event's biggest announcements, all in one place    ","version":null,"tagName":"h2"},{"title":"📢 Agent Development: The Next Frontier​","type":1,"pageTitle":"Google Cloud Next 2025: Innovation at Scale ✨","url":"/docuhub-staging/blog/april-2025-update#-agent-development-the-next-frontier","content":" What particularly caught my attention was Google's focus on agent development technologies:  google-adk: The Agent Development Kit makes building multi-agent applications accessible to developers without deep AI expertise. Learn moreA2A (Agent-to-Agent Communication): The frameworks for agent-to-agent interaction open up possibilities for complex automated workflows that were previously impossible. Explore A2A or jump straight to the quickstart code samplesAgentspace: This collaboration environment for multiple agents feels like the beginning of something truly transformative. Check it out  ","version":null,"tagName":"h2"},{"title":"Final Thoughts​","type":1,"pageTitle":"Google Cloud Next 2025: Innovation at Scale ✨","url":"/docuhub-staging/blog/april-2025-update#final-thoughts","content":" As I reflect on my time at Google Cloud Next 2025, I'm struck by how the landscape of cloud computing and AI is evolving faster than ever. The convergence of powerful infrastructure, sophisticated AI models like Gemini, and new paradigms for agent-based computing signals a fundamental shift in how we'll architect and manage enterprise systems in the coming years.  I left the conference not just with new knowledge but with a renewed sense of excitement about the possibilities ahead. The future of cloud computing isn't just about faster or cheaper computing resources—it's about fundamentally new ways of solving problems and creating value.  #GoogleCloudNext25 #CloudAdvocate #AI #A2A #VertexAI #GoogleDistributedCloud #DevOps #EnterpriseArchitecture ","version":null,"tagName":"h2"},{"title":"AWRA 2024 Spring Conference","type":0,"sectionRef":"#","url":"/docuhub-staging/blog/AWRA 2024 Spring Conference","content":"AWRA 2024 Spring Conference The CIROH CyberInfrastructure team recently participated in the AWRA 2024 Spring Conference, co-hosted by the Alabama Water Institute at the University of Alabama. Themed &quot;Water Risk and Resilience: Research and Sustainable Solutions,&quot; the conference brought together a diverse group of water professionals to exchange knowledge and explore cutting-edge research in the field. CIROH CyberInfrastructure team presented on these topics: Accelerating Community Contribution to the Next Generation Water Resources Modeling FrameworkCreating a community dataset for high-speed national water model data accessModel structure selection for the flood and drought predictions using the NextGen Framework based on the extreme event simulations CIROH team member James Halgren presented the work on &quot;Accelerating Community Contribution to the Next Generation Water Resources Modeling Framework.&quot; The presentation focused on building and sharing a continuous research data stream using the NextGen Water Resources Modeling Framework with NextGen IN A Box (NGIAB). This project, a collaboration with Lynker members, showcases the potential for open-source tools and community-driven efforts to advance water resources modeling and research. CIROH team member Sepehr Karimi presented the work on &quot;Creating a community dataset for high-speed national water model data access&quot; CIROH team member Shahab Alam presented the work on &quot;Model structure selection for the flood and drought predictions using the NextGen Framework based on the extreme event simulations&quot; These presentations showcased CIROH's expertise in open-source tools, community-driven efforts, and water resources modeling. The team's contributions sparked insightful discussions and potential collaborations for future projects. Call to Action: To learn more about CIROH's work or connect with the team, visit our website at CIROH-website. Conference Website: AWRA 2024 Spring Conference website link","keywords":"","version":null},{"title":"Monthly News Update - March 2024","type":0,"sectionRef":"#","url":"/docuhub-staging/blog/Community NextGen Updates March 2024","content":"Accelerating Innovation: CIROH's March 2024 Update The CIROH team has been diligently accelerating research cyberinfrastructure capabilities this month. We're thrilled to share key milestones achieved in enhancing the Community NextGen project and our cloud/on-premises platforms. A significant highlight was the successful launch of our new fully operational on-premises infrastructure. Comprehensive documentation is now available here, ensuring seamless access and utilization. Additionally, we've fortified the NextGen in a Box (NGIAB) ecosystem with bug fixes, repository enhancements, and initiated work on automating the CI pipeline for the Singularity Repo Empowering our community remains a top priority. We've expanded the DocuHub knowledge base with dedicated sections on on-premises access guidelines, as well as policies and best practices for optimized infrastructure usage here . Furthermore, our team represented CIROH at the AWRA Geospatial Water Technology Conference in Orlando, sharing insights on leveraging geospatial data for water research. Refer here As we continue driving advancements, we extend our gratitude for your unwavering support of the Community NextGen project and CIROH's cyberinfrastructure endeavors. Be on the lookout for more exciting updates next month as we strive to unlock new frontiers in water science through robust computing capabilities. Click Here to Visit Community NextGen and NGIAB News from March 2024","keywords":"","version":null},{"title":"CIROH Cloud User Success Story","type":0,"sectionRef":"#","url":"/docuhub-staging/blog/August Monthly Blog Update","content":"","keywords":"","version":null},{"title":"1. ngen-datastream and NGIAB​","type":1,"pageTitle":"CIROH Cloud User Success Story","url":"/docuhub-staging/blog/August Monthly Blog Update#1-ngen-datastream-and-ngiab","content":"     ","version":null,"tagName":"h2"},{"title":"Overview:​","type":1,"pageTitle":"CIROH Cloud User Success Story","url":"/docuhub-staging/blog/August Monthly Blog Update#overview","content":" CIROH’s cloud computing resources have allowed for the development of ngen-datastream, which automates the process of collecting and formatting input data for NextGen, orchestrating the NextGen run through NextGen In a Box (NGIAB), and handling outputs. This software allows users to run NextGen in an efficient, relatively painless, and reproducible fashion, increasing community access to the NextGen framework. ngen-datastream is already community accessible (https://github.com/CIROH-UA/ngen-datastream/tree/main) and making an impact on research. A major component of this software is the Amazon Web Services (AWS) cloud-based research datastream (https://github.com/CIROH-UA/ngen-datastream/tree/main/research_datastream). The research datastream is a CONUS-wide recurring NextGen simulation configured by the community. The terraform to build the AWS infrastructure exists in the ngen-datastream repository and current development focuses on CI/CD and enabling community contribution to the research datastream via edits to the NextGen configuration. Ultimately, these tools help distribute access throughout the community to cutting edge hydrologic research, maximizing the pace of progress of research to operations in hydrology.  ","version":null,"tagName":"h3"},{"title":"Contribution to CIROH:​","type":1,"pageTitle":"CIROH Cloud User Success Story","url":"/docuhub-staging/blog/August Monthly Blog Update#contribution-to-ciroh","content":" Automation: It automates the process of collecting, formatting, and validating input data for NextGen, streamlining model preparation.Flexibility: It allows users to provide their own input files to run NextGen.Scalable Infrastructure: It utilizes AWS state machine to provide access to high-performance computing (HPC) resources.  ","version":null,"tagName":"h3"},{"title":"Infrastructure Utilized:​","type":1,"pageTitle":"CIROH Cloud User Success Story","url":"/docuhub-staging/blog/August Monthly Blog Update#infrastructure-utilized","content":" Elastic Compute Cloud (EC2)Simple Storage Service (S3)AWS Lamda and Step Functions  ","version":null,"tagName":"h3"},{"title":"2. TEEHR​","type":1,"pageTitle":"CIROH Cloud User Success Story","url":"/docuhub-staging/blog/August Monthly Blog Update#2-teehr","content":" PI : Katie van WekhovenCo-PI : Matt Denno (Development Lead)Developer : Sam Lamont  ","version":null,"tagName":"h2"},{"title":"Project Overview:​","type":1,"pageTitle":"CIROH Cloud User Success Story","url":"/docuhub-staging/blog/August Monthly Blog Update#project-overview","content":" The goal of this project is to investigate, design, and build a prototype hydrologic model/forecast evaluation system (TEEHR) that will significantly improve our ability to evaluate continental-scale datasets and will provide a robust and consistent evaluation tool for CIROH and OWP research. Design priorities include easy integration into common research workflows, rapid execution of large-scale evaluations, simplified exploration of performance trends and drivers, inclusion of common and emergent evaluation methods, efficient data structures, open-source and community development, and easy extensibility.    ","version":null,"tagName":"h3"},{"title":"Contribution to CIROH:​","type":1,"pageTitle":"CIROH Cloud User Success Story","url":"/docuhub-staging/blog/August Monthly Blog Update#contribution-to-ciroh-1","content":" TEEHR-HUB: It is a JupyterHub environment, running the TEEHR image, with AWS services (EFS and S3) to provide a scalable platform for hydrologic research.Data Processing: TEEHR-HUB has successfully processed the AORC (v3.0 retrospective) gridded precipitation data to the MERIT basins, as well as the CONUS 40-year retrospective (v3.0 and USGS).Testbed Integration: TEEHR-HUB’s compatibility with various testbeds allows researchers to experiment with different hydrologic models and datasets.Evaluation - TEEHR is being used (or is planned for use) by several CIROH research teams to evaluate large scale model results.  ","version":null,"tagName":"h3"},{"title":"Infrastructure Utilized:​","type":1,"pageTitle":"CIROH Cloud User Success Story","url":"/docuhub-staging/blog/August Monthly Blog Update#infrastructure-utilized-1","content":" Elastic Kubernetes Service (EKS) (including supporting AWS services) - Scalable computing resources to host JupyterHub Dask and SparkElastic File System (EFS) - Shared data drive for cached data and shared documents (notebooks, etc.)Simple Storage Service (S3) - Bucket storage for large public and private datasets ","version":null,"tagName":"h3"},{"title":"Monthly News Update - February 2024","type":0,"sectionRef":"#","url":"/docuhub-staging/blog/Community NextGen Updates Feb 2024","content":"Welcome to the February edition of the CIROH DocuHub blog, where we bring you the latest updates and news about the Community NextGen project and CIROH's Cloud and on-premise Infrastructure. Our team has been hard at work enhancing CIROH's Infrastructure and Community NextGen tools. Here are some highlights from February 2024: We successfully launched our new On-premises Infrastructure, which is now fully operational. You can find documentation for it here. For NGIAB, we've made improvements to the CI pipeline for pull requests submitted with forked repositories. Now, we automatically build and test these submissions using the CI pipeline. We've added documentation for the NWMURL python package, which offers utility functions tailored for accessing National Water Model (NWM) data URLs. This library streamlines the process of accessing NWM data for various purposes, including analysis, modeling, and visualization. You can explore the documentation here. We're thrilled to announce the NextGen Track for DevCon24. The schedule is now available at: DevCon24 Schedule. Thank you for your ongoing interest and support in the Community NextGen project. Stay tuned for more exciting updates and developments next month. 😊 Click Here to Visit Community NextGen and NGIAB News from Feb 2024","keywords":"","version":null},{"title":"CIROH at AGU 2024","type":0,"sectionRef":"#","url":"/docuhub-staging/blog/December Monthly Blog Update","content":"","keywords":"","version":null},{"title":"Presentations and Posters 📊​","type":1,"pageTitle":"CIROH at AGU 2024","url":"/docuhub-staging/blog/December Monthly Blog Update#presentations-and-posters-","content":" The conference provided an excellent platform for CIROH researchers to present their groundbreaking work. Our team delivered impactful presentations and poster sessions highlighting CIROH’s innovative work, including advancements in water prediction systems and community water modeling.  These sessions sparked thought-provoking discussions and fostered collaborations with other researchers. For those who missed it, posters and presentation slides are now available here. Feel free to explore these materials and share your thoughts. 📝    ","version":null,"tagName":"h2"},{"title":"Contribute Your Posters or Presentations!​","type":1,"pageTitle":"CIROH at AGU 2024","url":"/docuhub-staging/blog/December Monthly Blog Update#contribute-your-posters-or-presentations","content":" If any CIROH researchers would like to add their own posters or presentations to this repository, please feel free to send a pull request (PR). We welcome contributions and encourage collaboration!  ","version":null,"tagName":"h3"},{"title":"Photo Gallery 📸​","type":1,"pageTitle":"CIROH at AGU 2024","url":"/docuhub-staging/blog/December Monthly Blog Update#photo-gallery-","content":" We captured some unforgettable moments from AGU 2024. From vibrant sessions to meaningful conversations at our booth, here’s a glimpse:   ","version":null,"tagName":"h2"},{"title":"CIROH Developers Conference 2024","type":0,"sectionRef":"#","url":"/docuhub-staging/blog/devcon24-retrospective","content":"CIROH Developers Conference 2024 The CIROH team recently participated in the 2nd Annual CIROH Developers Conference (DevCon24), held from May 29th to June 1st,2024. The conference brought together a diverse group of water professionals to exchange knowledge and explore cutting-edge research in the field of hydrological forecasting. Reflecting CIROH's current research focus, the conference explored topics including hydrological modeling (NextGen), flood inundation mapping, hydroinformatics, social science, and community engagement. Attendees got the opportunity to delve deeper into specific areas through its well-structured training track. This year the tracks were: NextGenFlood Inundation Mapping (FIM)Hydrological Applications of Machine Learning (ML)HydroinformaticsCross-cutting This year, various workshops leveraged cloud technologies. Notably, we provided access to the 2i2c JupyterHub environment, a cloud-based platform for interactive computing, for ten workshops. This facilitated seamless access to powerful computing resources for participants. Additionally, we provided AWS instances to support four workshops. Presentation Slides: You can find the presentation slides here. To learn more about CIROH's work or connect with the team, visit our website at CIROH-website. Conference Website: Learn More","keywords":"","version":null},{"title":"DevCon 2025: A DevOps and Cyberinfrastructure Success Story","type":0,"sectionRef":"#","url":"/docuhub-staging/blog/devcon25-infra","content":"","keywords":"","version":null},{"title":"The Power of Collaboration​","type":1,"pageTitle":"DevCon 2025: A DevOps and Cyberinfrastructure Success Story","url":"/docuhub-staging/blog/devcon25-infra#the-power-of-collaboration","content":" DevCon 2025 represented an outstanding example of what's possible when public institutions and private enterprises work together:  Corporate Sponsors: AWS and Google Cloud provided funding and infrastructure for the event.Infrastructure Partners: NSF JetStream and 2i2c JupyterHub delivered the computational backbone.Technical Implementation: The CIROH Research Cyberinfrastructure and DevOps Team managed IT access and software packaging.    ","version":null,"tagName":"h2"},{"title":"Our Technical Approach​","type":1,"pageTitle":"DevCon 2025: A DevOps and Cyberinfrastructure Success Story","url":"/docuhub-staging/blog/devcon25-infra#our-technical-approach","content":" Supporting 200+ workshop attendees required careful planning and execution. Our DevOps team and partners implemented:  ","version":null,"tagName":"h2"},{"title":"Infrastructure as Code​","type":1,"pageTitle":"DevCon 2025: A DevOps and Cyberinfrastructure Success Story","url":"/docuhub-staging/blog/devcon25-infra#infrastructure-as-code","content":" Automated deployment of workshop environmentsScalable JupyterHub instances configured for concurrent usersPre-packaged software environments ensuring consistency across all workstations  ","version":null,"tagName":"h3"},{"title":"Cloud-Native Solutions​","type":1,"pageTitle":"DevCon 2025: A DevOps and Cyberinfrastructure Success Story","url":"/docuhub-staging/blog/devcon25-infra#cloud-native-solutions","content":" Leveraged AWS S3 bucket and Google Cloud BigQueryImplemented auto-scaling to handle peak workshop loadsEnsured high availability across multiple availability zones  ","version":null,"tagName":"h3"},{"title":"Seamless User Experience​","type":1,"pageTitle":"DevCon 2025: A DevOps and Cyberinfrastructure Success Story","url":"/docuhub-staging/blog/devcon25-infra#seamless-user-experience","content":" Single sign-on authentication for 100+ participants using CIROH 2i2c JupyterHub's new WORKSHOP Hub!!Pre-configured JetStream2 environments with all necessary toolsReal-time support slack channels for immediate issue resolution  ","version":null,"tagName":"h3"},{"title":"Key Achievements​","type":1,"pageTitle":"DevCon 2025: A DevOps and Cyberinfrastructure Success Story","url":"/docuhub-staging/blog/devcon25-infra#key-achievements","content":" Zero Downtime: Maintained 100% uptime throughout the entire eventRapid Onboarding: All 200 attendees were able to access their environments within minutesConsistent Experience: Every participant had identical, fully-functional development environmentsReal-time Support: Our team resolved technical issues with average response time under 5 minutes  ","version":null,"tagName":"h3"},{"title":"Lessons Learned​","type":1,"pageTitle":"DevCon 2025: A DevOps and Cyberinfrastructure Success Story","url":"/docuhub-staging/blog/devcon25-infra#lessons-learned","content":" This event perfectly demonstrated how DevOps principles—bridging development and operations—combined with cloud computing can transform organizational capabilities. Key takeaways include:  Automation is Essential: Pre-event automation allowed us to focus on attendee support rather than infrastructure management.Public-Private Partnerships Work: Combining Google Cloud services, AWS services, 2i2c services and NSF's research infrastructure created a best-of-class research workshops solution.Preparation Prevents Problems: Our extensive pre-event testing and redundancy planning paid dividends during the live event.  ","version":null,"tagName":"h2"},{"title":"Looking Forward​","type":1,"pageTitle":"DevCon 2025: A DevOps and Cyberinfrastructure Success Story","url":"/docuhub-staging/blog/devcon25-infra#looking-forward","content":" The success of DevCon 2025's IT infrastructure demonstrates that large-scale technical events no longer need to be limited by traditional IT constraints. By embracing DevOps practices and leveraging cloud partnerships, we can create learning environments that scale effortlessly while maintaining reliability and performance.  We're proud to have supported the DevCon 2025 community and look forward to applying these lessons to future events. The combination of public research infrastructure, private cloud resources, and dedicated DevOps expertise created an environment where 200+ developers could focus on learning and innovation without worrying about technical barriers. ","version":null,"tagName":"h2"},{"title":"DevCon 2025: Hydroinformatics and Research CyberInfrastructure Keynote","type":0,"sectionRef":"#","url":"/docuhub-staging/blog/devcon25-keynote","content":"","keywords":"","version":null},{"title":"Value and Impact​","type":1,"pageTitle":"DevCon 2025: Hydroinformatics and Research CyberInfrastructure Keynote","url":"/docuhub-staging/blog/devcon25-keynote#value-and-impact","content":" During the keynote, we emphasized the value that CIROH brings to new students and researchers. This includes access to:  Computational resources (free for researchers) that would normally cost thousands of dollars.Datasets that would take months to compile.Tools that streamline research and increase its impact.Publications - https://portal.ciroh.org/publications  CIROH Ecosystem Tools  We also showcased several transformative tools within the CIROH ecosystem that are advancing the field of hydrology:  NGIAB (NextGen In A Box) ecosystem (ngiab.ciroh.org): Revolutionizes water modeling by making the NextGen framework portable and accessible through containerized, open-source solutions.CIROH DocuHub (docs.ciroh.org): Your comprehensive knowledge center featuring technical documentation, monthly insights, and the latest developments in water science.CIROH Portal (portal.ciroh.org): A unified gateway connecting researchers to essential data, cutting-edge tools, and collaborative research opportunities.Google BigQuery NWM API (Documentation): Streamlines National Water Model data access with powerful cloud-based querying capabilities for efficient analysis.Tethys Platform: Transforms complex hydrological data and models into intuitive web applications, making advanced water science accessible to all stakeholders.AWS, Google Cloud, 2i2c JupyterHub and NSF Access: CIROH's AWS, Google Cloud accounts, CIROH 2i2c JupyuterHub and NSF Allocations for cloud computing in hydrology.Pantarhei and Wukong HPC: CIROH's state-of-the-art high-performance computing infrastructure, powering computationally intensive research and simulations.And many more innovative solutions driving water science forward!  Our overall message was that CIROH’s hydroinformatics and research cyberinfrastructure ecosystem is designed to support and amplify research efforts. We encouraged attendees to explore these resources and consider how they could be applied to their own work. Whether it's streamlining data workflows, tackling computationally intensive tasks, or sharing findings, CIROH provides the tools and infrastructure to push the boundaries of hydrological science.  We want to thank everyone who attended our keynote and engaged in the Mentimeter quiz. It’s an exciting time for CIROH, and we’re thrilled to be a part of this dynamic community!    Video voiced by Quinn Lee and prepared by Manjila Singh, Nia Minor, and Arpita Patel. ","version":null,"tagName":"h2"},{"title":"Application of NOAA-OWP's NextGen Framework: DevCon 2025 and EWRI Congress 2025 Highlights","type":0,"sectionRef":"#","url":"/docuhub-staging/blog/ewri-devcon25-ngiab","content":"","keywords":"","version":null},{"title":"Empowering the Community at the 2025 CIROH Developers Conference​","type":1,"pageTitle":"Application of NOAA-OWP's NextGen Framework: DevCon 2025 and EWRI Congress 2025 Highlights","url":"/docuhub-staging/blog/ewri-devcon25-ngiab#empowering-the-community-at-the-2025-ciroh-developers-conference","content":" The annual CIROH Developers Conference is a premier three-day event that convenes hundreds of scientists, engineers, and software developers from across the consortium and its partner institutions. It serves as a vital hub for innovation and collaboration, where attendees engage with the latest research findings, project highlights, and hands-on workshops on cutting-edge tools, workflows, and methodologies.  For me, this year's conference in the beautiful city of Burlington, Vermont, felt like coming full circle, as it marked my one-year anniversary since joining the consortium through The University of Alabama.  I had the privilege of leading a hands-on training workshop on hydrological model calibration. It was a fantastic experience engaging with the talented developers, researchers, and scientists who are all dedicated to the advancement and acceleration of Community Water Resources Modeling. Our session provided comprehensive, hands-on guidance for calibrating one of core components of the NextGen National Water Model model-confifuration—specifically the Conceptial Functional Equivalent (CFE) coupled with Noah-OWP-Modular—within the NextGen In A Box (NGIAB) ecosystem.  The NGIAB ecosystem is a comprehensive platform that revolutionizes how we approach hydrological research with integrated capabilities for:  ✨ 1. Data preprocessing⚙️ 2. Hydrology model simulations and calibration📊 3. Advanced evaluation tools📈 4. Rich visualization capabilities  A huge thank you to CIROH for organizing such an insightful conference, and to the University of Vermont for being an excellent host!    For everyone who attended or is interested in learning more, all training materials, codes, and data from the workshop are openly available in our GitHub repository:  ➡️ NGIAB-Calibration-DevCon25 Workshop Materials ➡️ More NextGen Workshops: DocuHub News; May 2025 Updates  I'm inspired by the passion for innovation shown and look forward to seeing how the community leverages these tools to advance water modeling.    ","version":null,"tagName":"h3"},{"title":"Disseminating Research at the EWRI Congress​","type":1,"pageTitle":"Application of NOAA-OWP's NextGen Framework: DevCon 2025 and EWRI Congress 2025 Highlights","url":"/docuhub-staging/blog/ewri-devcon25-ngiab#disseminating-research-at-the-ewri-congress","content":" The EWRI Congress was a valuable opportunity to disseminate our research and connect with hundreds of engineers and scientists. I had the privilege of delivering three oral presentations that showcase the use-inspired science outcomes emerging from the CIROH Science and Technology Team and our partners.  1. Advancing Hydrologic Modeling through Community-Driven Development: The NextGen Framework​  This presentation introduced the Community NextGen initiative, a collaborative effort aimed at fostering open innovation and accelerating the framework's development. These efforts aim to facilitate efficient research-to-operations (R2O) and operations-to-research (O2R) transitions, ultimately enhancing our ability to model and predict water resources at various scales.  2. Enhancing Catchment Based Hydrological Model Performance through Dynamic Sub-Discretization Using Land Surface Attributes​  This talk addressed how the simplifications in lumped models can lead to reduced accuracy when applied uniformly in catchments with significant land-surface heterogeneity. The study explores the potential to enhance lumped model performance by introducing dynamic discretization based on key land-surface heterogeneities such as land use, soil type and soil layering, slope aspect, and elevation.  3. Multi-model Predictions of Design Low Flow Conditions in the Great Salt Lake Basin Using NextGen Water Resources Modeling Framework​  This research focused on the challenge of accurately estimating low flow conditions in ungauged basins, such as those in the Great Salt Lake (GSL) region. The study paves the way to investigate how different models perform in predicting design low flow, a critical standard for water supply planning, management, and ensuring water quality in the GSL basin at both gauged and ungauged locations.  These works directly reflects the ongoing efforts and use-inspired science outcomes of the CIROH Science and Technology Team and Partners across the consortium. The congress served as a successful platform for showcasing these advancements and CIROH's leadership in community-driven, next-generation water resource prediction.   ","version":null,"tagName":"h3"},{"title":"Google Cloud Next '24: A Flood of Innovation and Inspiration","type":0,"sectionRef":"#","url":"/docuhub-staging/blog/Google Cloud Next 2024","content":"Google Cloud Next '24 Hello everyone, and thanks for stopping by! I recently had the incredible opportunity to attend Google Cloud Next 2024 in person for the first time, and it was truly an amazing experience. From insightful keynote presentations and workshops to vibrant booths buzzing with connections, the event was a whirlwind of innovation and inspiration. One of the highlights was undoubtedly the abundance of AI announcements and advancements. Google continues to push the boundaries of what's possible, and it was exciting to witness the future of technology unfold. Among the many highlights, CIROH achieved a significant milestone with its first-ever session at Google Cloud Next. The presentation, titled &quot;Channel the Flood Data Deluge: Unlocking the American National Water Model,&quot; link led by Kel Markert (Google), Dr. Dan Ames (BYU), and Michael Ames(SADA) was a resounding success. The session link shed light on the immense potential of the National Water Model and its ability to revolutionize water resource management. The conference was a truly enjoyable experience, especially collaborating with Dan, Kel, Michael and others. We had a great time together and sharing our insights. The energy and enthusiasm throughout the event were contagious, and I left feeling incredibly motivated and inspired. I connected with numerous individuals from diverse backgrounds, fostering new collaborations and sparking exciting ideas for the future of water research and technology. If you're curious to see more about my Google Cloud Next experience, head over to my LinkedIn post link where I've shared pictures from all three days. Thank you for reading and stay tuned for more updates on the exciting advancements in water research and technology! Want to delve deeper into the insights and announcements from Google Cloud Next? Check out these valuable resources: SADA Live: Recap Key Cloud Technology Insights from Google Cloud Next '24: linkThis LinkedIn event offers a comprehensive overview of the key takeaways and technological advancements unveiled at the conference. Day 2 Google Blog Recap: Dive into the specifics of Day 2 at Google Cloud Next with this insightful blog post, covering topics ranging from AI and data analytics to infrastructure and security. link AI Takes Center Stage: Gemini for Google Cloud: The introduction of Gemini 1.5 Pro, integrated with various Google Cloud services, promises enhanced functionality, security, and AI performance across diverse applications.AI Infrastructure Advancements: The AI Hypercomputer provides exceptional computational power for complex AI tasks, while Gemini API now offers models tailored for various scales, enriching the development environment.Vertex AI Enhancements: New tools for low-latency applications and improved Gemini integration empower developers to build more efficient and sophisticated AI-driven applications.Secure AI Framework (SAIF): Establishes rigorous security standards for AI implementations, ensuring secure and responsible AI integrations.AI Database Assistant: Leverages Gemini to simplify complex queries and deepen AI integration into database management.Google Vids: This innovative Workspace feature utilizes Gemini and Vertex AI to enhance digital storytelling and collaboration, revolutionizing workplace communication. Infrastructure and Development: Google Axion Processor: This cutting-edge processor boasts significant performance and energy efficiency improvements compared to traditional x86 instances, setting a new standard for computational efficiency.link Google Distributed Cloud (GDC) Sandbox: Enables developers to build and test services for GDC within a Google Cloud environment, simplifying the development process.link Migrate to Containers (M2C) CLI: This new tool facilitates seamless migration of applications to containers, supporting deployment on GKE or Cloud Run.link Security and Data Analytics: AI Cyber Defense Initiative: Revolutionizes cybersecurity by leveraging AI for innovative solutions against cyber threats.BigQuery as a Unified Platform: Transforms BigQuery into a comprehensive platform for managing multimodal data and executing AI tasks, seamlessly integrated with Gemini. Check out all the announcements: link","keywords":"","version":null},{"title":"Pennsylvania State University Researchers Leverage CIROH Cyberinfrastructure for Advanced Hydrological Modeling","type":0,"sectionRef":"#","url":"/docuhub-staging/blog/February Monthly Blog Update","content":"","keywords":"","version":null},{"title":"💻 The Computing and Storage Infrastructure​","type":1,"pageTitle":"Pennsylvania State University Researchers Leverage CIROH Cyberinfrastructure for Advanced Hydrological Modeling","url":"/docuhub-staging/blog/February Monthly Blog Update#-the-computing-and-storage-infrastructure","content":" ","version":null,"tagName":"h2"},{"title":"Wukong Computing Platform​","type":1,"pageTitle":"Pennsylvania State University Researchers Leverage CIROH Cyberinfrastructure for Advanced Hydrological Modeling","url":"/docuhub-staging/blog/February Monthly Blog Update#wukong-computing-platform","content":" The PSU team has been utilizing Wukong, a high-performance computing (HPC) cluster specifically designed for data-intensive scientific applications, such as the high-resolution physics-informed machine learning for national water modeling (Song et al. 2024[YS1] ). Wukong provides the computational power necessary for running complex simulations and processing large environmental datasets that traditional computing resources would struggle with. 🔍  Key advantages of Wukong include:​  🎯 Large GPU capacity for high-resolution ML/differentiable process-based models⚙️ Scalable parallel processing capabilities🚀 Optimized performance for data-intensive workloads⏳ Reduced processing time for big data🌐 Support for multi-node computation to handle larger geographical areas  S3 Bucket Integration:​  To complement Wukong’s computational power, the PSU researchers and AWI DevOps staff implemented Amazon S3 (Simple Storage Service) buckets as their secondary data storage solution. This integration offers several benefits:  🗄️ Virtually unlimited storage capacity for growing datasets🔒 Data durability and redundancy💰 Cost-effective long-term storage, with the use of S3 intelligent tiering to automate the storage cost savings by moving data when access patterns change🔄 Seamless data transfer between computing nodes📝 Version control for dataset iterations🤝 Easy data sharing with users not on Wukong    ","version":null,"tagName":"h3"},{"title":"🔬 Research Applications​","type":1,"pageTitle":"Pennsylvania State University Researchers Leverage CIROH Cyberinfrastructure for Advanced Hydrological Modeling","url":"/docuhub-staging/blog/February Monthly Blog Update#-research-applications","content":" The PSU team has applied this powerful computing infrastructure to several critical research areas:  National Streamflow Modeling 🌊 Training differentiable hydrologic models with high-resolution forcing and static attribute data across extensive geographical regions using observations from thousands of gauges, followed by whole-domain forwarding. National River Routing 🗺️ Conducting river routing on MERIT/HydroFabric river networks, combined with neural network-supported routing parameter learning. NextGen Candidate Models &amp; Data Assimilation 🔄 Applying multiple NextGen candidate models and data assimilation algorithms within the differentiable modeling framework, which supports compliance with BMI. Foundation Model Development 🏞️ Developing a foundation model to explore co-evolution between landscapes.    ","version":null,"tagName":"h2"},{"title":"Thank you to all those who contributed towards this effort.​","type":1,"pageTitle":"Pennsylvania State University Researchers Leverage CIROH Cyberinfrastructure for Advanced Hydrological Modeling","url":"/docuhub-staging/blog/February Monthly Blog Update#thank-you-to-all-those-who-contributed-towards-this-effort","content":" ","version":null,"tagName":"h3"},{"title":"🔗 Learn More​","type":1,"pageTitle":"Pennsylvania State University Researchers Leverage CIROH Cyberinfrastructure for Advanced Hydrological Modeling","url":"/docuhub-staging/blog/February Monthly Blog Update#-learn-more","content":" For more details on the Wukong computing platform, check out the official documentation: 👉 Wukong Documentation  For the full research paper by Song et al. (2024), visit: 👉 DOI: 10.22541/essoar.172736277.74497104/v1   ","version":null,"tagName":"h2"},{"title":"Assessing Streamflow Forecast Over the Hackensack River Watershed Using NGIAB","type":0,"sectionRef":"#","url":"/docuhub-staging/blog/ismart-ngiab-application","content":"A poster presented by the I-SMART team at the CIROH Developers Conference, held at the University of Vermont in Burlington from May 28 to 30, 2025. The densely populated Hackensack River watershed lies within the New York City Metropolitan Area, which spans northern New Jersey and southern New York. Accurate streamflow forecasting within this region is therefore essential to enable effective water resource management, flood prediction, and disaster preparedness. Precipitation data is critical for effective hydrological modeling, making the identification of reliable data sources a key priority. This is why the Integrated Spatial Modeling and Remote Sensing Technologies Laboratory (I-SMART), an interdisciplinary research unit within the Davidson Laboratory at Stevens Institute of Technology in Hoboken, New Jersey, uses the latest developments in both atmospheric and hydrological modeling to address flood risks in the Hackensack Watershed with solutions that could be expanded to the entire New York City Metropolitan Area. In the past, this work has included key early applications of the Next Generation Water Resources Modeling Framework (NextGen). Notably, the I-SMART group was among the first to force the NextGen framework with multiple atmospheric models for comparative analysis during a real-world event: the passage of Superstorm Ida over the New York metropolitan area in September 2021. The recent advent of NextGen In a Box (NGIAB) has provided an opportunity to accelerate these applications even further by taking full advantage of NGIAB's containerized, user-friendly, and easily deployed environment. Recently, the I-SMART team has been testing various regional atmospheric models grounded in physical equations, including traditional models like WRF and next-generation atmospheric models such as MPAS. Additionally, given the increasing popularity and adoption of AI/ML-based approaches, the team has also begun exploring their potential. The goal of this work is to assess the performance of these approaches in the Hackensack Watershed, along with investigating the sensitivity of the model to various meteorological forcings, including forcings based on the National Water Model.(The initial and/or boundary conditions for all the models were determined using the Global Forecast System.) This investigation required handling a large volume of precipitation data from various models, each with different spatial resolutions and in some cases, such as MPAS, using unstructured grids. As such, one of the key challenges was finding a hydrological modeling framework flexible enough to accommodate such diversity. This made the NextGen framework a natural choice, allowing them to integrate precipitation forcings from various sources with the appropriate pre-processing to align them with the model requirements in terms of spatial and temporal scales. The complex implementation and execution of these models was faciliated by NextGen In A Box (NGIAB), which successfully enabled the integration of diverse precipitation sources. By simplifying local deployment and providing full control over model inputs, configurations, and runtime operations, NGIAB has given the I-SMART team the tools to conduct their groundbreaking research with even greater efficiency.","keywords":"","version":null},{"title":"CIROH Research CyberInfrastructure Update","type":0,"sectionRef":"#","url":"/docuhub-staging/blog/July Monthly IT Update","content":"","keywords":"","version":null},{"title":"Cloud Infrastructure​","type":1,"pageTitle":"CIROH Research CyberInfrastructure Update","url":"/docuhub-staging/blog/July Monthly IT Update#cloud-infrastructure","content":" CIROH's Google Cloud Account is now fully operational and managed by our team. You can find more information here.We're in the process of migrating our 2i2c JupyterHub to CIROH's Google Cloud account.We've successfully deployed the Google BigQuery API (developed by BYU and Google) for NWM data in our cloud. To access this API, please contact us at ciroh-it-admin@ua.edu. Please refer to NWM BigQuery API to learn more.    ","version":null,"tagName":"h3"},{"title":"Support and Services​","type":1,"pageTitle":"CIROH Research CyberInfrastructure Update","url":"/docuhub-staging/blog/July Monthly IT Update#support-and-services","content":" Monthly AWS office hours are ongoing. For more details on how to join, email us at ciroh-it-admin@ua.edu.We provided IT support for the Summer Institute 2024, REU students, and team leads this summer.      ","version":null,"tagName":"h3"},{"title":"Security Enhancements​","type":1,"pageTitle":"CIROH Research CyberInfrastructure Update","url":"/docuhub-staging/blog/July Monthly IT Update#security-enhancements","content":" New security features have been added to our CIROH-UA GitHub repository to prevent commits containing sensitive information.We've updated our AWS best practices, particularly regarding key management. If your project uses CIROH AWS resources, please review these updates at AWS Best Practices.  ","version":null,"tagName":"h3"},{"title":"Resources and Access​","type":1,"pageTitle":"CIROH Research CyberInfrastructure Update","url":"/docuhub-staging/blog/July Monthly IT Update#resources-and-access","content":" For external IT resources needed for your projects, check out NSF Access Allocations here.GPU allocation is now available on CIROH's 2i2c JupyterHub. To request access, please fill out this form.    For more information on our services, please refer to our services page.  We're continually working to improve our IT infrastructure and support. If you have any questions or need assistance, don't hesitate to reach out to us at ciroh-it-admin@ua.edu. ","version":null,"tagName":"h3"},{"title":"NextGen Framework Forcings","type":0,"sectionRef":"#","url":"/docuhub-staging/blog/NextGen Forcings","content":"","keywords":"","version":null},{"title":"NextGen Framework Forcings​","type":1,"pageTitle":"NextGen Framework Forcings","url":"/docuhub-staging/blog/NextGen Forcings#nextgen-framework-forcings","content":" A new forcing processor tool has been made public. This tool converts any National Water Model based forcing files into ngen forcing files. This process can be an intensive operation in compute, memory, and IO, so this tool facilitates generating ngen input and ultimately makes running ngen more accessible.    ","version":null,"tagName":"h2"},{"title":"Read more​","type":1,"pageTitle":"NextGen Framework Forcings","url":"/docuhub-staging/blog/NextGen Forcings#read-more","content":" Visit Github ","version":null,"tagName":"h3"},{"title":"🌟 UA's Alabama Water Institute Showcases 30-Minute Hydrological Modeling Revolution🌟","type":0,"sectionRef":"#","url":"/docuhub-staging/blog/march-2025-update","content":"","keywords":"","version":null},{"title":"🌍 AWI News​","type":1,"pageTitle":"🌟 UA's Alabama Water Institute Showcases 30-Minute Hydrological Modeling Revolution🌟","url":"/docuhub-staging/blog/march-2025-update#-awi-news","content":" The Alabama Water Institute (AWI) at the University of Alabama (UA) recently published an article highlighting how NextGen In A Box (NGIAB) could transform hydrological modeling. This article provides great insight into NGIAB's real-world impact:  🚀30-minute setup vs days/weeks of configuration📖 Provo River Basin Case Study demonstrating rapid deployment    ➡️ Read the full press release here!     ","version":null,"tagName":"h2"},{"title":"Cross-Institutional Collaboration Enhances Hydrologic Modeling in the Logan River Watershed","type":0,"sectionRef":"#","url":"/docuhub-staging/blog/logan-river-collaboration","content":"Figure 1. A corrected reach arising from the UA-USU collaboration. Recent collaboration between researchers in the Cooperative Institute for Research to Operations in Hydrology (CIROH) from University of Alabama (UA) and Utah State University (USU) highlighted the value of cross-institutional partnerships in improving community hydrologic modeling. Focused on the Logan River watershed, this joint effort demonstrated how sharing tools, knowledge, and infrastructure can accelerate both model development and scientific discovery. Through this engagement, USU researchers gained deeper understanding of the NextGen framework and T-Route modeling library, empowering them to improve physical process representations for the Logan River watershed for heightened simulation fidelity. The collaboration also provided valuable exposure to the developmental side of complex modeling tools, offering insights into framework design, automation workflows, and best practices for model setup and calibration. Both teams benefited from exposure to alternative research tools and methods, which helped enhance and refine the community development pipeline. One key outcome was the correction of a spatial representation in the Logan River hydrofabric (Figure 1) using the USU’s specific local knowledge. The reach for this region (highlighted in the zoomed subfigure) was updated to reflect its real world path, where it flows into an underwater tunnel that reconnects to Logan River further downstream. This correction will be integrated into the community hydrofabric, benefiting the wider community by improving model realism and reliability. Furthermore, in combination with freshly calibrated model parameters, these updates have improved the model's KGE metric by 0.2 units — a significant gain in model accuracy. NGIAB played a central role in this success, streamlining the process of setting up, running, and analyzing multiple model configurations. By removing the typical installation and troubleshooting barriers, researchers could focus their time on advancing physical representations rather than infrastructure issues. This collaboration sets a precedent for how community-driven, open science efforts can improve hydrologic model performance and usability. By combining localized knowledge with shared national tools, we move closer to creating reliable, reusable, and scalable flood forecasting systems, a crucial step for Research-to-Operations (R2O) success. We look forward to repeating this model of collaboration with other regional experts and academic institutions to continually refine the National Water Model ecosystem and its supporting frameworks.","keywords":"","version":null},{"title":"NextGen Monthly News Update - December 2023","type":0,"sectionRef":"#","url":"/docuhub-staging/blog/NextGen Updates Dec 2023","content":"Happy New Year!!! We are back from SFO after attending AGU last month. We are excited to share the latest updates for NGIAB, NextGen, T-route, Hydrofabric, NextGen forcings, and Community Support from December 2023. Visit NGIAB News","keywords":"","version":null},{"title":"NextGen In A Box v1.1.0 Release","type":0,"sectionRef":"#","url":"/docuhub-staging/blog/NextGen In A Box Release Notes","content":"We've released NGIAB v1.1.0! This release fixes issues: #21#67#44 More info: https://github.com/CIROH-UA/NGIAB-CloudInfra/releases/tag/v1.1.0 Visit NGIAB News","keywords":"","version":null},{"title":"NextGen Monthly News Update - November 2023","type":0,"sectionRef":"#","url":"/docuhub-staging/blog/NextGen Updates Nov 2023","content":"We are excited to share the latest updates for NGIAB, NextGen, T-route, Hydrofabric, NextGen forcings and Community Support. Visit NGIAB News","keywords":"","version":null},{"title":"δHBV2.0: How NGIAB and Wukong HPC Streamlined Advanced Hydrologic Modeling","type":0,"sectionRef":"#","url":"/docuhub-staging/blog/may-2025-update","content":"","keywords":"","version":null},{"title":"🔗 Learn more​","type":1,"pageTitle":"δHBV2.0: How NGIAB and Wukong HPC Streamlined Advanced Hydrologic Modeling","url":"/docuhub-staging/blog/may-2025-update#-learn-more","content":" δHBV2.0: https://doi.org/10.5281/zenodo.14827983  Song et al. 2024: https://agupubs.onlinelibrary.wiley.com/doi/10.1029/2024WR038928  Dataset: https://doi.org/10.5281/zenodo.13774373 ","version":null,"tagName":"h2"},{"title":"NextGen Monthly News Update - January 2024","type":0,"sectionRef":"#","url":"/docuhub-staging/blog/NextGen Updates Jan 2024","content":"Welcome to the January edition of the CIROH DocuHub blog, where we share the latest updates and news about the Community NextGen project monthly. NextGen is a cutting-edge hydrologic modeling framework that aims to advance the science and practice of hydrology and water resources management. In this month's blog, we will highlight some of the recent achievements and developments of the Community NextGen team. First, we are excited to announce that NextGen In A Box (NGIAB) is now available with Singularity support. This means that you can run NGIAB on any HPC system that does not support Docker, using Singularity containers. Singularity is a popular tool for creating and running portable and reproducible computational environments. To learn how to use NGIAB with Singularity, please visit our GitHub repository: Ngen-Singularity. Second, we have made several improvements and enhancements to NGIAB, such as updating the sample input data, upgrading the Boost library, adding auto mode run, and supporting geopackage format. You can find more details about these updates on our GitHub repository: NGIAB-CloudInfra. Third, we would like to share with you is the development of NextGen Datastream, a tool that automates the process of collecting and formatting input data for NextGen, orchestrating the NextGen run through NextGen In a Box (NGIAB), and handling outputs. The NextGen Datastream is a shell script that orchestrates each step in the process, using a configuration file that specifies the data sources, parameters, and options for the NextGen run. The NextGen Datastream can also generate its own internal configs and modify the configuration file as needed. You can find more details and instructions on how to use the NextGen Datastream on our GitHub repository: ngen-datastream. We hope you enjoyed this blog and found it informative and useful. If you have any questions, comments, or feedback, please feel free to contact us at ciroh-it-admin@ua.edu. Thank you for your interest and support in the Community NextGen project. Stay tuned for more exciting news and developments in the next month. 😊 Visit NGIAB News","keywords":"","version":null},{"title":"NextGen In A Box Updates","type":0,"sectionRef":"#","url":"/docuhub-staging/blog/NextGen-In-A-Box Release Notes","content":"We've introduced a fresh addition within DocuHub, offering the most up-to-date insights on NGIAB and NextGen monthly updates. Visit NGIAB Release Notes Page","keywords":"","version":null},{"title":"Evaluating NextGen’s Performance in the MARFC Region with NGIAB","type":0,"sectionRef":"#","url":"/docuhub-staging/blog/nextgen-marfc-performance","content":"","keywords":"","version":null},{"title":"References​","type":1,"pageTitle":"Evaluating NextGen’s Performance in the MARFC Region with NGIAB","url":"/docuhub-staging/blog/nextgen-marfc-performance#references","content":" National Oceanic and Atmospheric Administration. Community Hydrologic Prediction System (CHPS) Documentation [Internet]. National Oceanic and Atmospheric Administration; [updated 2024 Nov 26; cited 2025 Jul 28]. Available from: https://vlab.noaa.gov/web/chps Araki R, Ogden FL, McMillan HK. Testing Soil Moisture Performance Measures in the Conceptual‐Functional Equivalent to the WRF-Hydro National Water Model. JAWRA Journal of the American Water Resources Association. 2025 Feb;61(1):e70002.National Oceanic and Atmospheric Administration. The National Water Model [Internet]. National Water Prediction Service; [cited 2025 Jul 29]. Available from: https://water.noaa.gov/about/nwm Tijerina‐Kreuzer D, Condon L, FitzGerald K, Dugger A, O’Neill MM, Sampson K, Gochis D, Maxwell R. Continental hydrologic intercomparison project, phase 1: A large‐scale hydrologic model comparison over the continental United States. Water Resources Research. 2021 Jul;57(7):e2020WR028931.Moriasi DN, Arnold JG, Van Liew MW, Bingner RL, Harmel RD, Veith TL. Model evaluation guidelines for systematic quantification of accuracy in watershed simulations. Transactions of the ASABE. 2007;50(3):885-900.Gupta HV, Kling H, Yilmaz KK, Martinez GF. Decomposition of the mean squared error and NSE performance criteria: Implications for improving hydrological modelling. Journal of Hydrology. 2009 Oct 20;377(1-2):80-91.Vrugt JA, de Oliveira DY. Confidence intervals of the Kling-Gupta efficiency. Journal of Hydrology. 2022 Sep 1;612:127968. ","version":null,"tagName":"h2"},{"title":"CIROH Researchers Showcase Cutting-Edge Hydrologic Science at NHWC 2025","type":0,"sectionRef":"#","url":"/docuhub-staging/blog/nhwc-2025","content":"","keywords":"","version":null},{"title":"Advancing Model Reliability in Hydrologic Forecasting​","type":1,"pageTitle":"CIROH Researchers Showcase Cutting-Edge Hydrologic Science at NHWC 2025","url":"/docuhub-staging/blog/nhwc-2025#advancing-model-reliability-in-hydrologic-forecasting","content":" During the NextGen presentation track, moderated by Dr. Steve Burian, CIROH researchers focused on improving evaluation techniques for hydrologic models—particularly in the context of the National Water Model (NWM).  ","version":null,"tagName":"h2"},{"title":"NWS NextGen Program​","type":1,"pageTitle":"CIROH Researchers Showcase Cutting-Edge Hydrologic Science at NHWC 2025","url":"/docuhub-staging/blog/nhwc-2025#nws-nextgen-program","content":"   Edwin Welles from Deltares USA presenting the NextGen framework.  Edwin Welles from Deltares USA kicked things off with insights into a groundbreaking initiative. The Office of Water Prediction, of the National Weather Service has undertaken an ambitious project to implement and apply a new framework for hydrological modeling: the The Next Genearation Water Resources Modeling Framework, or the NextGen framework, is designed to emphasize modularity, interoperability, and reproducibility, creating an environment where disparate modeling components can be freely combined to enhance modeling capabilities. These advantages offer critical support to the National Water Model, alongside other key initiatives such as national Flood Inundation Mapping research. The system is being implemented by Raytheon with subcontract support from Deltares USA. His presentation provided a summary of the project and envisioned outcomes.  ","version":null,"tagName":"h2"},{"title":"Leading the NextGen Revolution​","type":1,"pageTitle":"CIROH Researchers Showcase Cutting-Edge Hydrologic Science at NHWC 2025","url":"/docuhub-staging/blog/nhwc-2025#leading-the-nextgen-revolution","content":"   Arpita Patel presenting “Navigating the NextGen Ecosystem and NextGen In A Box (NGIAB).”  Arpita Patel followed with “Navigating the NextGen Ecosystem and NextGen In A Box (NGIAB),” a presentation focused on making the NextGen framework more accessible to researchers, forecasters, and water managers. This was achieved through the NextGen In A Box platform, which harnesses containerization and supportive tooling to make the NextGen framework accessible and easy to use. This work directly supports CIROH’s mission to democratize access to advanced hydrologic modeling capabilities and foster broader adoption of next-generation water prediction tools.  🥉 Patel also demonstrated exceptional engagement throughout the conference. Her outstanding participation earned her an impressive #3 ranking on the NHWC 2025 conference leaderboard with 70,200 points. This demonstrated not only her technical expertise, but also her active involvement in conference activities and community engagement.  ","version":null,"tagName":"h2"},{"title":"Rethinking Evaluation​","type":1,"pageTitle":"CIROH Researchers Showcase Cutting-Edge Hydrologic Science at NHWC 2025","url":"/docuhub-staging/blog/nhwc-2025#rethinking-evaluation","content":"   Dr. Md Shahabul Alam presenting different model evaluation methods.  Dr. Md Shahabul Alam also delivered a compelling talk titled “Rethinking Model Reliability: A Dual Evaluation of NWM Using Time Series and Extreme Events” during the NextGen program track. His work focuses on advancing operational hydrology by developing more rigorous and resilient methods to evaluate the National Water Model across varying temporal scales and extreme event conditions—a crucial step in improving forecasting and decision-making under a changing climate. This work represents CIROH’s commitment to enhancing the reliability and accuracy of national-scale hydrologic prediction systems, directly supporting the operational forecasting community’s need for trustworthy model outputs during both routine conditions and extreme weather events.  🥇 Dr. Alam earned also earned the #1 ranking on the NHWC 2025 conference leaderboard, maintaining a commanding lead of over 100,000 points throughout the week and finishing with a remarkable 194,700 points. His name quickly became a focal point of the conference. Each morning, attendees checked the Whova app with anticipation—only to see Shahab had once again pulled further ahead. His energetic participation and engaging presence made him a standout at the conference, combining technical depth with social and technological savvy. Dr. Alam not only demonstrated exceptional leadership and charisma at the NHWC 2025 conference, but also brought well-deserved visibility to CIROH and its ongoing mission.  ","version":null,"tagName":"h2"},{"title":"Strengthening Community Connections​","type":1,"pageTitle":"CIROH Researchers Showcase Cutting-Edge Hydrologic Science at NHWC 2025","url":"/docuhub-staging/blog/nhwc-2025#strengthening-community-connections","content":" The inclusion of these presentations within the NextGen presentation track highlights CIROH’s central role in advancing the NextGen Water Resources Modeling Framework. This alignment demonstrates our organization’s commitment to translating research innovations into operational capabilities that serve the broader water resources community.  Following the presentations, many researchers expressed immediate interest in using NextGen In A Box for both their research applications and operational forecasting. The tool's modular design, ease of deployment, and compatibility with diverse datasets resonated strongly with attendees seeking flexible and scalable modeling solutions. Several participants noted its potential for accelerating innovation in hydrologic modeling and expressed intent to integrate it into their ongoing and future projects.  The conference provided valuable opportunities for our researchers to connect with operational forecasters, emergency managers, and fellow scientists, fostering collaborations that will enhance the real-world impact of CIROH’s research initiatives. These interactions are essential for ensuring our scientific advances address the practical challenges faced by those responsible for flood warning and water resource management.  ","version":null,"tagName":"h2"},{"title":"Looking Ahead​","type":1,"pageTitle":"CIROH Researchers Showcase Cutting-Edge Hydrologic Science at NHWC 2025","url":"/docuhub-staging/blog/nhwc-2025#looking-ahead","content":" CIROH’s strong presence at NHWC 2025 underscores our dedication to bridging the gap between research and operations in hydrologic science. Our researchers’ contributions to model evaluation methodologies and NextGen ecosystem development represent key steps toward more reliable, accessible, and effective water prediction systems.  As we continue to advance hydrologic science, conferences like NHWC 2025 provide essential platforms for sharing innovations, gathering feedback from the operational community, and building the collaborative relationships that drive progress in water resources management and flood warning systems.  The success of our team at NHWC 2025 reflects CIROH’s growing impact in the hydrologic sciences community and our commitment to developing solutions that serve both scientific advancement and societal needs. ","version":null,"tagName":"h2"},{"title":"CIROH Science Meeting 2024","type":0,"sectionRef":"#","url":"/docuhub-staging/blog/October Monthly Blog Update","content":"","keywords":"","version":null},{"title":"CIROH 2024 Science Meeting: Fostering Innovation in Hydrology​","type":1,"pageTitle":"CIROH Science Meeting 2024","url":"/docuhub-staging/blog/October Monthly Blog Update#ciroh-2024-science-meeting-fostering-innovation-in-hydrology","content":" The third annual CIROH Science Meeting, held at The University of Alabama from October 14-17, 2024, brought together our vibrant community of researchers, practitioners, and stakeholders. This four-day event showcased the remarkable progress in our mission to advance national water prediction capabilities.  ","version":null,"tagName":"h2"},{"title":"Meeting Highlights​","type":1,"pageTitle":"CIROH Science Meeting 2024","url":"/docuhub-staging/blog/October Monthly Blog Update#meeting-highlights","content":" Over 300 participants gathered to share knowledge and forge new collaborations Engaging discussions spanning critical topics in hydrology Special focus on CIROH's four key research themes: Next Generation Water Prediction Community Modeling Hydroinformatics Social Science in Water Resources  ","version":null,"tagName":"h2"},{"title":"Community Engagement​","type":1,"pageTitle":"CIROH Science Meeting 2024","url":"/docuhub-staging/blog/October Monthly Blog Update#community-engagement","content":" The meeting featured presentations from our consortium members, research partners, and key stakeholders including NOAA/NWS. The diverse agenda included keynotes, project presentations, roundtable discussions, poster sessions, and HIF tour that fostered meaningful discussions about the future of water prediction.  Representatives from our member institutions and various partnering organizations contributed to rich dialogues about advancing the National Water Model and related technologies. The collaborative atmosphere exemplified CIROH's commitment to building a stronger hydroinformatics community.  ","version":null,"tagName":"h2"},{"title":"Key Outcomes​","type":1,"pageTitle":"CIROH Science Meeting 2024","url":"/docuhub-staging/blog/October Monthly Blog Update#key-outcomes","content":" Strengthened partnerships across the consortium Shared progress on ongoing research initiatives Identified new opportunities for cross-institutional collaboration Advanced discussions on improving national water prediction capabilities Enhanced understanding of community needs and priorities  ","version":null,"tagName":"h2"},{"title":"Looking Forward​","type":1,"pageTitle":"CIROH Science Meeting 2024","url":"/docuhub-staging/blog/October Monthly Blog Update#looking-forward","content":" The success of the 2024 Science Meeting reinforces CIROH's role in shaping the future of water prediction. The connections made and ideas shared during these four days will continue to influence our work throughout the year as we strive to improve water prediction capabilities for the nation.  ","version":null,"tagName":"h2"},{"title":"Photo Gallery​","type":1,"pageTitle":"CIROH Science Meeting 2024","url":"/docuhub-staging/blog/October Monthly Blog Update#photo-gallery","content":"               ","version":null,"tagName":"h2"},{"title":"Conclusion​","type":1,"pageTitle":"CIROH Science Meeting 2024","url":"/docuhub-staging/blog/October Monthly Blog Update#conclusion","content":" The 2024 CIROH Science Meeting demonstrated the power of collaboration in advancing hydrology. As we move forward, the energy and insights from this gathering will fuel our continued efforts to enhance water prediction capabilities and serve our communities better. ","version":null,"tagName":"h2"},{"title":"Community NextGen Updates","type":0,"sectionRef":"#","url":"/docuhub-staging/blog/November Monthly Blog Update","content":"","keywords":"","version":null},{"title":"🎉 NGIAB-CloudInfra v1.3.0: A Major Milestone​","type":1,"pageTitle":"Community NextGen Updates","url":"/docuhub-staging/blog/November Monthly Blog Update#-ngiab-cloudinfra-v130-a-major-milestone","content":" The release of NGIAB-CloudInfra v1.3.0 marks a significant evolution in our Docker-based solution. This version introduces several groundbreaking improvements:  Integration with CIROH-UA's forked repositories of ngen and t-routeFull compatibility with the latest hydrofabric v2.2New sample input data AWI-007Enhanced visualization capabilities through the NGIAB VisualizerStreamlined CI/CD pipeline with a unified DockerfileTEEHR integration for improved evaluation capabilities  ","version":null,"tagName":"h2"},{"title":"📝 New End-to-End Workflow Guide​","type":1,"pageTitle":"Community NextGen Updates","url":"/docuhub-staging/blog/November Monthly Blog Update#-new-end-to-end-workflow-guide","content":" We're excited to announce a comprehensive new end-to-end workflow video tutorial for NGIAB! This resource walks users through the entire process, making it easier than ever to get started with NextGen modeling. The tutorial is available in our documentation and provides step-by-step guidance for both new and experienced users.  ","version":null,"tagName":"h2"},{"title":"🔄 Infrastructure Updates​","type":1,"pageTitle":"Community NextGen Updates","url":"/docuhub-staging/blog/November Monthly Blog Update#-infrastructure-updates","content":" ","version":null,"tagName":"h2"},{"title":"NGIAB-HPCInfra​","type":1,"pageTitle":"Community NextGen Updates","url":"/docuhub-staging/blog/November Monthly Blog Update#ngiab-hpcinfra","content":" The Singularity-based solution for high-performance computing has been updated to maintain parity with the Docker implementation, including:  Updated repositories alignmentNew sample data compatibilityEnhanced documentation  ","version":null,"tagName":"h3"},{"title":"Data Processing Tools​","type":1,"pageTitle":"Community NextGen Updates","url":"/docuhub-staging/blog/November Monthly Blog Update#data-processing-tools","content":" NGIAB-data-preprocess v3.1.2: Major update ensuring seamless compatibility with hydrofabric v2.2ngen-datastream: Significant improvements with 22 new commits enhancing data handling capabilities  ","version":null,"tagName":"h3"},{"title":"🌟 New Features​","type":1,"pageTitle":"Community NextGen Updates","url":"/docuhub-staging/blog/November Monthly Blog Update#-new-features","content":" ","version":null,"tagName":"h2"},{"title":"Hydrofabric v2.2​","type":1,"pageTitle":"Community NextGen Updates","url":"/docuhub-staging/blog/November Monthly Blog Update#hydrofabric-v22","content":" The latest hydrofabric data model brings improved spatial representation and data organization. This update enhances the framework's ability to represent complex hydrologic systems accurately.  ","version":null,"tagName":"h3"},{"title":"Repository Updates​","type":1,"pageTitle":"Community NextGen Updates","url":"/docuhub-staging/blog/November Monthly Blog Update#repository-updates","content":" The NextGen Model Framework now operates from CIROH-UA's main branchT-route implementation has been updated to use the specialized datastream branchBoth updates provide better integration and enhanced functionality within the NGIAB ecosystem  ","version":null,"tagName":"h3"},{"title":"What's Next?​","type":1,"pageTitle":"Community NextGen Updates","url":"/docuhub-staging/blog/November Monthly Blog Update#whats-next","content":" These November updates represent our ongoing commitment to improving the NextGen framework's accessibility and capabilities. With the new workflow video and documentation in place, we're making it easier than ever for the hydrologic modeling community to leverage these powerful tools.  We encourage users to:  Explore the new v1.3.0 releaseWatch the end-to-end workflow videoTest out the updated data preprocess and NGIAB toolsProvide feedback on your experience  Stay tuned for more updates as we continue to enhance and expand the NextGen In A Box ecosystem!    For detailed information about these updates, visit our repositories:  NGIAB-CloudInfraNGIAB-HPCInfrangen-datastreamNGIAB-data-preprocess ","version":null,"tagName":"h2"},{"title":"Accessing National Water Model (NWM) Data via Google Cloud BigQuery API","type":0,"sectionRef":"#","url":"/docuhub-staging/blog/September Monthly Blog Update","content":"","keywords":"","version":null},{"title":"Public-Private Partnership: Advancing Water Resource Management​","type":1,"pageTitle":"Accessing National Water Model (NWM) Data via Google Cloud BigQuery API","url":"/docuhub-staging/blog/September Monthly Blog Update#public-private-partnership-advancing-water-resource-management","content":" The National Water Model (NWM) BigQuery project exemplifies a successful collaboration between public and private sectors, uniting government-generated data with modern, cutting-edge cloud technology. This collaboration addresses several key aspects:  Improved Data Access: By leveraging Google Cloud BigQuery, a private sector platform, the project dramatically improves access to public NWM data. This partnership makes valuable water resource information more readily available to researchers, policymakers, and the public.Technological Innovation: The integration of NWM data with BigQuery showcases how private sector technology can enhance the utility of public sector data. This synergy promotes innovation in data analysis and visualization techniques.Cost-Effective Solutions: The CIROH DevOps team's commitment to covering query costs demonstrates how public funding can be strategically used to make private sector tools accessible to a wider audience, particularly in the academic and research communities.Capacity Building: This initiative helps build capacity across sectors by providing researchers and organizations with powerful tools to analyze water resource data, potentially leading to better-informed decision-making in water management.Scalability and Efficiency: By utilizing Google's cloud infrastructure, the project ensures that the growing volume of NWM data can be efficiently stored, accessed, and analyzed, addressing the scalability challenges often faced by public sector IT resources.Cross-Sector Collaboration: This project fosters collaboration between government agencies, academic institutions, and private technology companies, creating a model for future partnerships in environmental and resource management.Open Science Promotion: By making NWM data more accessible, this partnership supports the principles of open science, encouraging transparency and reproducibility in water resource research.  This public-private partnership not only enhances the value of the National Water Model but also sets a precedent for future collaborations that can drive innovation in environmental data management and analysis. ","version":null,"tagName":"h2"},{"title":"From Research to Impact: CIROH Science Meeting 2025 Resources and Reflections","type":0,"sectionRef":"#","url":"/docuhub-staging/blog/science-meeting-2025","content":"","keywords":"","version":null},{"title":"Meeting Highlights: Transforming Research into Real-World Impact​","type":1,"pageTitle":"From Research to Impact: CIROH Science Meeting 2025 Resources and Reflections","url":"/docuhub-staging/blog/science-meeting-2025#meeting-highlights-transforming-research-into-real-world-impact","content":" Over four days, our community showed how scientific innovation translates into tools and systems that support communities, enhance resilience, and improve decision-making across the nation. From AI advances in water prediction to flood inundation mapping, the presentations and discussions demonstrated the collaborative, open-science approach that defines CIROH.    ","version":null,"tagName":"h2"},{"title":"Access Meeting Resources​","type":1,"pageTitle":"From Research to Impact: CIROH Science Meeting 2025 Resources and Reflections","url":"/docuhub-staging/blog/science-meeting-2025#access-meeting-resources","content":" We are excited to share a comprehensive collection of assets from the 2025 CIROH Science Meeting, including session recordings, presentation slides, and group photos! Whether you attended in person, joined virtually, or could not participate, these materials are available to you.  ","version":null,"tagName":"h2"},{"title":"🔗 Access the Shared Drive​","type":1,"pageTitle":"From Research to Impact: CIROH Science Meeting 2025 Resources and Reflections","url":"/docuhub-staging/blog/science-meeting-2025#-access-the-shared-drive","content":" Please note that materials are being uploaded continuously as we receive images and additional files from presenters and participants. We encourage you to check back periodically for updates. If you are a presenter and prefer not to have your slides or poster included in the shared drive, please contact Charity McCalpin (cnmccalpin@ua.edu) for removal.  ","version":null,"tagName":"h3"},{"title":"🌐 Visit the Event Microsite​","type":1,"pageTitle":"From Research to Impact: CIROH Science Meeting 2025 Resources and Reflections","url":"/docuhub-staging/blog/science-meeting-2025#-visit-the-event-microsite","content":" Explore the agenda, highlights, and media on the CIROH Science Meeting site.  👉 Ciroh Science Meeting  ","version":null,"tagName":"h3"},{"title":"Your Feedback Matters​","type":1,"pageTitle":"From Research to Impact: CIROH Science Meeting 2025 Resources and Reflections","url":"/docuhub-staging/blog/science-meeting-2025#your-feedback-matters","content":" ","version":null,"tagName":"h2"},{"title":"📝 Complete Our Post-Event Survey​","type":1,"pageTitle":"From Research to Impact: CIROH Science Meeting 2025 Resources and Reflections","url":"/docuhub-staging/blog/science-meeting-2025#-complete-our-post-event-survey","content":" If you attended the meeting, either in person or virtually, we kindly ask that you complete our post-event survey. Your feedback is invaluable in helping us improve future meetings and ensure they continue to serve our community's needs effectively.  👉 Complete the Post-Event Survey  ","version":null,"tagName":"h3"},{"title":"Looking Forward: Building on Our Collaborative Foundation​","type":1,"pageTitle":"From Research to Impact: CIROH Science Meeting 2025 Resources and Reflections","url":"/docuhub-staging/blog/science-meeting-2025#looking-forward-building-on-our-collaborative-foundation","content":" The 2025 Science Meeting reinforced CIROH's commitment to building a collaborative, open-science community that transforms research into real-world impact. The connections made and innovations shared during these four days will continue to drive our mission forward throughout the year.  ","version":null,"tagName":"h2"},{"title":"Thank You​","type":1,"pageTitle":"From Research to Impact: CIROH Science Meeting 2025 Resources and Reflections","url":"/docuhub-staging/blog/science-meeting-2025#thank-you","content":" Thank you for your ongoing support and participation in making the 2025 CIROH Science Meeting a tremendous success. Your commitment to advancing water science and translating research into operational tools that benefit communities nationwide is what makes CIROH's mission possible.  Together, we continue building a future where scientific innovation directly serves society's water-related challenges and opportunities.    The CIROH Science Meeting 2025 was held September 15-18 in Tuscaloosa, Alabama. For more information about future meetings and CIROH initiatives, visit our website (https://ciroh.ua.edu/) and stay connected with our community. ","version":null,"tagName":"h2"},{"title":"NGIAB Reaches 10,000 Docker Pulls: NextGen In A Box Makes Water Modeling More Accessible","type":0,"sectionRef":"#","url":"/docuhub-staging/blog/ngiab-10k","content":"","keywords":"","version":null},{"title":"From Research Innovation to Community Tool​","type":1,"pageTitle":"NGIAB Reaches 10,000 Docker Pulls: NextGen In A Box Makes Water Modeling More Accessible","url":"/docuhub-staging/blog/ngiab-10k#from-research-innovation-to-community-tool","content":" When we first containerized the NextGen Water Resources Modeling Framework into NGIAB, our goal was simple yet ambitious: remove the technical barriers that prevented many researchers from accessing NOAA's next-generation water modeling capabilities.  Today, with over 10,000 downloads, it's clear the community was ready for this transformation.  The University of Alabama recently highlighted NGIAB's impact in their news feature, &quot;UA Software Makes Water Modeling More Accessible&quot;, recognizing how this tool is changing the landscape of hydrologic research and education. As the article notes, NGIAB turns what was once a complex, infrastructure-heavy process into something that researchers can run on their laptops in minutes.  ","version":null,"tagName":"h2"},{"title":"What 10,000 Pulls Really Means​","type":1,"pageTitle":"NGIAB Reaches 10,000 Docker Pulls: NextGen In A Box Makes Water Modeling More Accessible","url":"/docuhub-staging/blog/ngiab-10k#what-10000-pulls-really-means","content":" Behind this number are stories of:  🎓 Graduate students exploring advanced modeling techniques with no setup headaches  🏫 Educators bringing cutting-edge tools into the classroom  🏢 Researchers at smaller institutions gaining access to national-scale modeling  🌍 International collaborators contributing to water modeling advancement without infrastructure constraints  🚨 Emergency managers rapidly deploying models for flood prediction  ","version":null,"tagName":"h2"},{"title":"Community Growth and Impact​","type":1,"pageTitle":"NGIAB Reaches 10,000 Docker Pulls: NextGen In A Box Makes Water Modeling More Accessible","url":"/docuhub-staging/blog/ngiab-10k#community-growth-and-impact","content":" The rapid adoption of NGIAB reflects a broader movement:  1. Democratization of Advanced Modeling No longer do researchers need access to specialized HPC resources or deep DevOps knowledge to run sophisticated water models. NGIAB levels the playing field.  2. Reproducible Science Every one of those 10,000 pulls represents the exact same computational environment, ensuring that research results can be replicated anywhere in the world.  3. Accelerated Innovation By removing setup friction, researchers can focus on science rather than software configuration, leading to faster iterations and discoveries.  ","version":null,"tagName":"h2"},{"title":"Looking Forward: The Next 10,000​","type":1,"pageTitle":"NGIAB Reaches 10,000 Docker Pulls: NextGen In A Box Makes Water Modeling More Accessible","url":"/docuhub-staging/blog/ngiab-10k#looking-forward-the-next-10000","content":" As we celebrate this milestone, we're already working on what's next:  📦 Expanded model library: Adding more BMI-compliant models to the NGIAB ecosystem  ☁️ Cloud integration: Seamless deployment on AWS, Google Cloud, and Azure  📊 Enhanced visualization: Built-in tools for analyzing and presenting model outputs  🤝 Community contributions: Making it easier for users to share their configurations and improvements  ","version":null,"tagName":"h2"},{"title":"Join the NGIAB Community​","type":1,"pageTitle":"NGIAB Reaches 10,000 Docker Pulls: NextGen In A Box Makes Water Modeling More Accessible","url":"/docuhub-staging/blog/ngiab-10k#join-the-ngiab-community","content":" Whether you're pull number 10,001 or have been with us since the beginning, you're part of a growing community that's transforming water resources modeling.  Here's how to get involved:  👉 Try NGIAB: Visit the NGIAB 101 tutorial to get started  ✍️ Share your use case: Tell us how you're using NGIAB in your research or operations  🛠 Contribute: Submit bug reports, feature requests, or code contributions  📢 Spread the word: Help others discover how NGIAB can accelerate their water modeling work  ","version":null,"tagName":"h2"},{"title":"Thank You to Our Community​","type":1,"pageTitle":"NGIAB Reaches 10,000 Docker Pulls: NextGen In A Box Makes Water Modeling More Accessible","url":"/docuhub-staging/blog/ngiab-10k#thank-you-to-our-community","content":" This milestone belongs to everyone who downloaded, tested, provided feedback, contributed code, or helped spread the word about NGIAB. Your engagement drives our continuous improvement and motivates us to make water modeling even more accessible.  As we reflect on reaching 10,000 Docker pulls, we're reminded that each download represents a researcher, student, or practitioner working to better understand and predict our water resources. Together, we're building a more resilient future through accessible, advanced water modeling.  Here's to the next 10,000 pulls and the continued growth of our community!    NGIAB is developed and maintained by the Cooperative Institute for Research to Operations in Hydrology (CIROH) at the University of Alabama. Learn more about our work in making water modeling accessible at ciroh.org. ","version":null,"tagName":"h2"},{"title":"Focusing on Streamflow Data: New Software for Camera-Based Hydrologic Modeling","type":0,"sectionRef":"#","url":"/docuhub-staging/blog/streamflow-data-camera-based-hydrologic-modeling-software","content":"","keywords":"","version":null},{"title":"Resources and Publications​","type":1,"pageTitle":"Focusing on Streamflow Data: New Software for Camera-Based Hydrologic Modeling","url":"/docuhub-staging/blog/streamflow-data-camera-based-hydrologic-modeling-software#resources-and-publications","content":" ","version":null,"tagName":"h2"},{"title":"Open Source Software​","type":1,"pageTitle":"Focusing on Streamflow Data: New Software for Camera-Based Hydrologic Modeling","url":"/docuhub-staging/blog/streamflow-data-camera-based-hydrologic-modeling-software#open-source-software","content":" The HydrocamCollect and HydrocamCompute software programs are available in GitHub repositories within the hydrocam GitHub Organization.  ","version":null,"tagName":"h3"},{"title":"Publications​","type":1,"pageTitle":"Focusing on Streamflow Data: New Software for Camera-Based Hydrologic Modeling","url":"/docuhub-staging/blog/streamflow-data-camera-based-hydrologic-modeling-software#publications","content":" Access the pre-prints of papers related to the Hydrocam software and camera-based monitoring:  Neupane, S., Horsburgh, J. S., Bin Issa, R., Young, S. (2025). HydrocamCollect: A robust data acquisition and cloud data transfer workflow for camera-based hydrological monitoring. Environmental Modelling &amp; Software (submitted). Available at SSRN: https://ssrn.com/abstract=5451102 or https://doi.org/10.2139/ssrn.5451102 Neupane, S., Horsburgh, J. S., Bin Issa, R., Young, S. (2025). HydrocamCompute: Serverless cloud computing workflow for camera-based hydrological monitoring. Environmental Modelling &amp; Software (submitted). Available at SSRN: https://ssrn.com/abstract=5451100 or https://doi.org/10.2139/ssrn.5451100 Bin Issa, R., Neupane, S., Khan, S., Horsburgh, J. S., Young, S. (2025). Towards Real-Time Water Level and Discharge Measurements using Imagery, Machine Learning, and Edge Computing. Journal of Hydrology (submitted). Available at SSRN: https://ssrn.com/abstract=5531964 or https://doi.org/10.2139/ssrn.5531964   ","version":null,"tagName":"h3"},{"title":"Contact​","type":1,"pageTitle":"Focusing on Streamflow Data: New Software for Camera-Based Hydrologic Modeling","url":"/docuhub-staging/blog/streamflow-data-camera-based-hydrologic-modeling-software#contact","content":" Dr. Sierra Young, Utah State University, sierra.young@usu.eduDr. Jeff Horsburgh, Utah State University, jeff.horsburgh@usu.edu ","version":null,"tagName":"h2"},{"title":"Tethys Summit 2025: Advancing Geoscience with Open-Source Web Apps","type":0,"sectionRef":"#","url":"/docuhub-staging/blog/tethys-summit-2025","content":"","keywords":"","version":null},{"title":"My Experience at Tethys Summit 2025​","type":1,"pageTitle":"Tethys Summit 2025: Advancing Geoscience with Open-Source Web Apps","url":"/docuhub-staging/blog/tethys-summit-2025#my-experience-at-tethys-summit-2025","content":" Earlier this month, I had the opportunity to attend Tethys Summit 2025 in Tampa, FL. It was a rewarding experience to learn about the Tethys Platform and how researchers, hydrologists, and geospatial scientists are applying it in their work. Through workshops and technical demonstrations, I gained insights into how this open-source Earth science platform is advancing environmental problem-solving.  On Day 1, I participated in the Tethys Dash workshop led by Corey Krewson, where I learned to build a Tethys dashboard without writing any code. During the hands-on session, I was surprised by how easily I was able to build dashboards using a drag-and-drop interface, as much of the technical implementation was abstracted for simplicity. I also attended the Tethys Component Apps (ReactPy) workshop by Shawn Crawley. Unlike the no-code approach of the first session, this workshop focused on building a basic interactive Tethys web app in under 50 lines of Python code.  On Day 2, I also took part in the Contributing to Tethys Core workshop led by Nathan Swain. This workshop outlined the steps for first-time contributors to get involved and submit pull requests to the Tethys Core repository. Aside from the workshops, I took part in Tethys App Competition where I presented the Tethys app I developed. It was a great learning experience to understand more about web app developments using Tethys. I ended up winning second place and got a bonsai tree LEGO set!  Overall, it was a very engaging and informative summit. For anyone who missed the session, the workshop materials are available here: Talks and Workshop Materials  — Manjila    ","version":null,"tagName":"h2"},{"title":"What is Tethys?​","type":1,"pageTitle":"Tethys Summit 2025: Advancing Geoscience with Open-Source Web Apps","url":"/docuhub-staging/blog/tethys-summit-2025#what-is-tethys","content":" Tethys Platform is an open-source, Python-powered framework that lets geoscientists and developers turn complex environmental data and models into browser-based, interactive web GIS apps—without having to become web gurus. Built on Django, it bundles a full spatial stack (PostGIS, GeoServer, OpenLayers/Cesium) plus handy &quot;Gizmos&quot; for maps, charts, and dashboards, and it integrates seamlessly with scientific libraries like NumPy, Pandas, Plotly, and Bokeh. A built-in job manager runs heavy climate or hydrologic models—locally or in the cloud—while APIs make it easy to pull data from THREDDS servers or PostGIS databases. The result is a low-barrier path to custom interfaces, 2D/3D visualizations, and scalable computing that's already powering water-resource tools, climate-scenario explorers, and disaster-monitoring apps worldwide. Backed by the Tethys Geoscience Foundation, the platform's active open-source community keeps it evolving to meet new Earth-science challenges.  ","version":null,"tagName":"h2"},{"title":"Community Impact​","type":1,"pageTitle":"Tethys Summit 2025: Advancing Geoscience with Open-Source Web Apps","url":"/docuhub-staging/blog/tethys-summit-2025#community-impact","content":" Thanks to these capabilities, Tethys Platform is widely adopted in the geoscience community. Scientists around the world use it for a range of applications—water resource management, climate change analysis, natural disaster monitoring, environmental planning, and more. In each case, Tethys helps turn complex datasets and model outputs into accessible tools for decision-makers. As an open-source project under the Tethys Geoscience Foundation, it benefits from an active user community that continually contributes improvements and new features. This collaborative growth ensures the platform keeps pace with emerging needs in Earth science, effectively lowering the barrier for geoscientists to share data and models as powerful web applications.  ","version":null,"tagName":"h2"},{"title":"Tethys Platform Resources​","type":1,"pageTitle":"Tethys Summit 2025: Advancing Geoscience with Open-Source Web Apps","url":"/docuhub-staging/blog/tethys-summit-2025#tethys-platform-resources","content":" Tethys Platform offers several resources to help you get started, including comprehensive official documentation, step-by-step tutorials, and a live demo portal. The official documentation covers everything from installation and &quot;Hello World&quot; examples to advanced topics and architecture (explaining how the Django backend and various services fit together). For hands-on learning, the tutorials guide you through setting up your first app, incorporating spatial data with built-in map layouts, connecting to GeoServer or THREDDS data servers, and even running cloud computations. And if you want to see Tethys in action without installing anything, the Tethys Demo Portal showcases a gallery of live example apps, such as a Gizmo Showcase of interactive widgets and a Wildfire Visualizer that displays wildfire data on a map, giving you a clear idea of the platform's capabilities and inspiring ideas for your own geoscientific web applications.  Each of these resources will help you dive deeper into the Tethys Platform. Whether you're looking to develop a water resources tool, a climate data viewer, or any geoscientific web application, the combination of documentation, tutorials, and live demos will support you on your journey with Tethys. Happy app building!  — Gio ","version":null,"tagName":"h2"},{"title":"04-08-2025 Release Notes","type":0,"sectionRef":"#","url":"/docuhub-staging/release-notes/v20250408","content":"","keywords":"","version":null},{"title":"New content​","type":1,"pageTitle":"04-08-2025 Release Notes","url":"/docuhub-staging/release-notes/v20250408#new-content","content":" Community Hydrologic Modeling page now features tables detailing features and access methods (#290)Added NGIAB-TEEHR and NGIAB-CLIENT to products section (#320)Added Flood Inundation Mapping Framework to products section (#300)Added documentation on debugging Python package conflicts in CIROH JupyterHub (#308)Added tutorial for running datapreprocess on CIROH JupyterHub (#299)  ","version":null,"tagName":"h2"},{"title":"Blog posts and news updates​","type":1,"pageTitle":"04-08-2025 Release Notes","url":"/docuhub-staging/release-notes/v20250408#blog-posts-and-news-updates","content":" Blog post: &quot;UA's Alabama Water Institute Showcases 30-Minute Hydrological Modeling Revolution&quot; (#310)News update: March 2025 (#306)  ","version":null,"tagName":"h2"},{"title":"Updates​","type":1,"pageTitle":"04-08-2025 Release Notes","url":"/docuhub-staging/release-notes/v20250408#updates","content":" Office Hours page updated; added to homepage and Contact page (#337)Contribute page heavily reworked; now features GitHub issue templates (#309)Added link to National Water Model API documentation (#307)Added graphical headers to News tab (#305)Guidance on accessing services and resources refactored (#287, #294) ","version":null,"tagName":"h2"},{"title":"04-16-2025 Release Notes","type":0,"sectionRef":"#","url":"/docuhub-staging/release-notes/v20250416","content":"","keywords":"","version":null},{"title":"Blog posts and news updates​","type":1,"pageTitle":"04-16-2025 Release Notes","url":"/docuhub-staging/release-notes/v20250416#blog-posts-and-news-updates","content":" Blog post: &quot;Google Cloud Next 2025: Innovation at Scale&quot; (#345)  ","version":null,"tagName":"h2"},{"title":"Updates​","type":1,"pageTitle":"04-16-2025 Release Notes","url":"/docuhub-staging/release-notes/v20250416#updates","content":" Blog's left sidebar now correctly displays all posts (#350)News and Contribute pages improved on mobile (#340) ","version":null,"tagName":"h2"},{"title":"05-05-2025 Release Notes","type":0,"sectionRef":"#","url":"/docuhub-staging/release-notes/v20250505","content":"","keywords":"","version":null},{"title":"New content​","type":1,"pageTitle":"05-05-2025 Release Notes","url":"/docuhub-staging/release-notes/v20250505#new-content","content":" Added researcher testimonials to homepage (#358)DocuHub now has a release notes feed! (#318)  ","version":null,"tagName":"h2"},{"title":"Updates​","type":1,"pageTitle":"05-05-2025 Release Notes","url":"/docuhub-staging/release-notes/v20250505#updates","content":" Added workshop IT support form to services access page (#362)Reorganized links to blogs for cloud services (#360)Video tutorial added to &quot;HydroShare and CIROH JupyterHub Intergration&quot; product page (#357)Reworked homepage display of CIROH sponsors, members, and partners (#356)Added link to CIROH Portal in DocuHub navbar (#355)All page tags have been refactored and condensed (#352)New landing page images for Products, Public Cloud Services, and Policies pages (#351)DataStream CLI key features clarified (#349)New social media icons in footer (#341) ","version":null,"tagName":"h2"},{"title":"05-16-2025 Release Notes","type":0,"sectionRef":"#","url":"/docuhub-staging/release-notes/v20250516","content":"","keywords":"","version":null},{"title":"New content​","type":1,"pageTitle":"05-16-2025 Release Notes","url":"/docuhub-staging/release-notes/v20250516#new-content","content":" Added NGIAB-Calibration to Products section (#378)Added a dashboard for NGIAB and CIROH repositories to Products section (#374)Added Community Hydrofabic Patcher to Products section (#354)  ","version":null,"tagName":"h2"},{"title":"Blog posts and news updates​","type":1,"pageTitle":"05-16-2025 Release Notes","url":"/docuhub-staging/release-notes/v20250516#blog-posts-and-news-updates","content":" Blog post: &quot;δHBV2.0: How NGIAB Streamlined Advanced Hydrologic Modeling&quot; (#379)News update: April 2025 (#376)  ","version":null,"tagName":"h2"},{"title":"Updates​","type":1,"pageTitle":"05-16-2025 Release Notes","url":"/docuhub-staging/release-notes/v20250516#updates","content":" Added message from CIROH DocuHub team in footer (#372)Added descriptive headers to News and Blog pages (#371)Updated CIROH funding acknowledgement (#370) ","version":null,"tagName":"h2"},{"title":"05-27-2025 Release Notes","type":0,"sectionRef":"#","url":"/docuhub-staging/release-notes/v20250527","content":"","keywords":"","version":null},{"title":"New content​","type":1,"pageTitle":"05-27-2025 Release Notes","url":"/docuhub-staging/release-notes/v20250527#new-content","content":" Added NextGen on CIROH JupyterHub to Products section (#404)  ","version":null,"tagName":"h2"},{"title":"Updates​","type":1,"pageTitle":"05-27-2025 Release Notes","url":"/docuhub-staging/release-notes/v20250527#updates","content":" Updated CIROH Portal under Products section with tables, emojis, and grammar fixes (#403)Updated documentation for &quot;FIM Database for Multi-Model Visualization&quot; (#402) ","version":null,"tagName":"h2"},{"title":"06-04-2025 Release Notes","type":0,"sectionRef":"#","url":"/docuhub-staging/release-notes/v20250604","content":"","keywords":"","version":null},{"title":"Blog posts and news updates​","type":1,"pageTitle":"06-04-2025 Release Notes","url":"/docuhub-staging/release-notes/v20250604#blog-posts-and-news-updates","content":" Blog post: &quot;DevCon 2025: A DevOps and Cyberinfrastructure Success Story&quot; (#414)Blog post: &quot;Application of NOAA-OWP's NextGen Framework: DevCon 2025 and EWRI Congress 2025 Highlights&quot; (#411)Blog post: &quot;DevCon 2025: Hydroinformatics and Research CyberInfrastructure Keynote&quot; (#407)News update: May 2025 (#408)  ","version":null,"tagName":"h2"},{"title":"Updates​","type":1,"pageTitle":"06-04-2025 Release Notes","url":"/docuhub-staging/release-notes/v20250604#updates","content":" Moved several NGIAB tutorials to a new, generalized header (#415)Added &quot;CIROH Research Cyberinfrastructure&quot; video to Services landing page (#413)Improvements to CI/CD pipelining (#409, #395) ","version":null,"tagName":"h2"},{"title":"06-11-2025 Release Notes","type":0,"sectionRef":"#","url":"/docuhub-staging/release-notes/v20250611","content":"","keywords":"","version":null},{"title":"New content​","type":1,"pageTitle":"06-11-2025 Release Notes","url":"/docuhub-staging/release-notes/v20250611#new-content","content":" The &quot;Community Hydrologic Modeling&quot; tab has been completely reworked! It's now called &quot;NextGen In A Box&quot;, better reflecting the content of the folder. (#427) Lots of new content has been added alongside this change within the &quot;Intro to NGIAB&quot; sub-section.NGIAB product subpages have been reorganized into the &quot;Distributions&quot; and &quot;Components&quot; sub-sections.As a part of this transition, content regarding the Reseach Datastream (formerly &quot;NextGen Datastream&quot;) has been spun out into its own category. The &quot;Research Datastream&quot; tab now correctly includes its documentation subpages from GitHub. (#425)  ","version":null,"tagName":"h2"},{"title":"Updates​","type":1,"pageTitle":"06-11-2025 Release Notes","url":"/docuhub-staging/release-notes/v20250611#updates","content":" General corrections and upkeep to past news and release notes (#426)Reformatted the way that DocuHub renders external README files (#425)Refactored URLs site-wide to remove spaces (#423)Upgraded to Docusaurus 3.8; enabled preview functionality for 4.0 (#420)    Please note that this update alters the URLs of many DocuHub pages. We've added redirects from their old locations to avoid breaking external links, but updating links to the new URLs is strongly preferred. ","version":null,"tagName":"h2"},{"title":"05-22-2025 Release Notes","type":0,"sectionRef":"#","url":"/docuhub-staging/release-notes/v20250522","content":"","keywords":"","version":null},{"title":"New content​","type":1,"pageTitle":"05-22-2025 Release Notes","url":"/docuhub-staging/release-notes/v20250522#new-content","content":" Added CIROH RIVR App to Products section (#389)Added FIM Database for Multi-Model Visualization to Products section (#393)  ","version":null,"tagName":"h2"},{"title":"Updates​","type":1,"pageTitle":"05-22-2025 Release Notes","url":"/docuhub-staging/release-notes/v20250522#updates","content":" Updated NGIAB E2E Workflow Video: Local setup page with the latest instructions (#385)Added more information on ngiab-cal (#387)Added link to DevCon25 Workshop JupyterHub (#398) ","version":null,"tagName":"h2"},{"title":"08-01-2025 Release Notes","type":0,"sectionRef":"#","url":"/docuhub-staging/release-notes/v20250801","content":"","keywords":"","version":null},{"title":"Blog posts and news updates​","type":1,"pageTitle":"08-01-2025 Release Notes","url":"/docuhub-staging/release-notes/v20250801#blog-posts-and-news-updates","content":" Blog post: &quot;NGIAB Reaches 10,000 Docker Pulls: NextGen In A Box Makes Water Modeling More Accessible&quot; (#448)Blog post: &quot;Tethys Summit 2025: Advancing Geoscience with Open-Source Web Apps&quot; (#445)Blog post: &quot;CIROH Researchers Showcase Cutting-Edge Hydrologic Science at NHWC 2025&quot; (#444)News update: July 2025 (#446)  ","version":null,"tagName":"h2"},{"title":"Updates​","type":1,"pageTitle":"08-01-2025 Release Notes","url":"/docuhub-staging/release-notes/v20250801#updates","content":" The NextGen News page has received a new look! (#447) ","version":null,"tagName":"h2"},{"title":"07-24-2025 Release Notes","type":0,"sectionRef":"#","url":"/docuhub-staging/release-notes/v20250724","content":"","keywords":"","version":null},{"title":"Updates​","type":1,"pageTitle":"07-24-2025 Release Notes","url":"/docuhub-staging/release-notes/v20250724#updates","content":" Education tab has been deprecated (#440, #443) CIROH Portal is now CIROH's primary hub for conference material and online lessons.Relocated information on NWM-affiliated technologies to Products tabHomepage and navbars rearranged accordingly Mirrored new documentation pages from NGIAB-CloudInfra (#442)Added notice regarding storage limits on 2i2c (#438)Resized a screenshot on the blog post &quot;AORC Data in Your Hands&quot; and added a caption (#437) ","version":null,"tagName":"h2"},{"title":"08-29-2025 Release Notes","type":0,"sectionRef":"#","url":"/docuhub-staging/release-notes/v20250829","content":"","keywords":"","version":null},{"title":"New content​","type":1,"pageTitle":"08-29-2025 Release Notes","url":"/docuhub-staging/release-notes/v20250829#new-content","content":" Added product pages for CIROH's community forks of NextGen repositories (#460)  ","version":null,"tagName":"h2"},{"title":"Blog posts and news updates​","type":1,"pageTitle":"08-29-2025 Release Notes","url":"/docuhub-staging/release-notes/v20250829#blog-posts-and-news-updates","content":" Blog post: &quot;Cross-Institutional Collaboration Enhances Hydrologic Modeling in the Logan River Watershed&quot; (#457)Blog post: &quot;Evaluating NextGen’s Performance in the MARFC Region with NGIAB&quot; (#453)News update: August 2025 (#462) Hotfix, 9/2: Updated news entries for NGIAB-HPCInfra and CIROH community forks of ngen/troute.  ","version":null,"tagName":"h2"},{"title":"Updates​","type":1,"pageTitle":"08-29-2025 Release Notes","url":"/docuhub-staging/release-notes/v20250829#updates","content":" Added link to NGIAB's academic paper in &quot;NGIAB Reaches 10,000 Docker Pulls&quot; (#462)Minor updates and visual improvements to the Community Impact page (#461) ","version":null,"tagName":"h2"},{"title":"07-01-2025 Release Notes","type":0,"sectionRef":"#","url":"/docuhub-staging/release-notes/v20250701","content":"","keywords":"","version":null},{"title":"Blog posts and news updates​","type":1,"pageTitle":"07-01-2025 Release Notes","url":"/docuhub-staging/release-notes/v20250701#blog-posts-and-news-updates","content":" Blog post: &quot;AORC Data in Your Hands: User-Friendly Jupyter Notebooks for Data Retrieval and Analysis via CIROH JupyterHub Notebooks&quot; (#434)Blog post: &quot;Assessing Streamflow Forecast Over the Hackensack River Watershed Using NGIAB&quot; (#431)News update: June 2025 (#432)  ","version":null,"tagName":"h2"},{"title":"Updates​","type":1,"pageTitle":"07-01-2025 Release Notes","url":"/docuhub-staging/release-notes/v20250701#updates","content":" Site-wide improvements and additions to tags (#435) Added new &quot;Community Spotlight&quot; tag to blog, highlighting key application of CIROH's tooling and cyberinfrastructure across the communityAdded new &quot;Data Preparation&quot; tag, highlighting tools and research that accelerate the retrieval and preprocessing of dataIncreased distribution of &quot;NGIAB&quot; tag across older blog posts ","version":null,"tagName":"h2"},{"title":"09-12-2025 Release Notes","type":0,"sectionRef":"#","url":"/docuhub-staging/release-notes/v20250912","content":"","keywords":"","version":null},{"title":"Updates​","type":1,"pageTitle":"09-12-2025 Release Notes","url":"/docuhub-staging/release-notes/v20250912#updates","content":" Added redirect and announcement banner for Science Meeting 2025 Survey (#472, #470)Fixes to Community Impact page (#471, #468)Corrected error in &quot;NWM, NextGen, and NGIAB&quot; page (#469)Updated news entries for NGIAB-HPCInfra and CIROH community forks of ngen/troute (#467) ","version":null,"tagName":"h2"},{"title":"10-01-2025 Release Notes","type":0,"sectionRef":"#","url":"/docuhub-staging/release-notes/v20251001","content":"","keywords":"","version":null},{"title":"New content​","type":1,"pageTitle":"10-01-2025 Release Notes","url":"/docuhub-staging/release-notes/v20251001#new-content","content":" Reorganized 2i2c documentation, with new content pages for HydroShare integration and available server images (#479)Revamped HydroShare documentation to focus on HydroShare itself (#479)  ","version":null,"tagName":"h2"},{"title":"Blog posts and news updates​","type":1,"pageTitle":"10-01-2025 Release Notes","url":"/docuhub-staging/release-notes/v20251001#blog-posts-and-news-updates","content":" Blog post: &quot;From Research to Impact: CIROH Science Meeting 2025 Resources and Reflections&quot; (#475)Blog post: &quot;Focusing on Streamflow Data: New Software for Camera-Based Hydrologic Modeling&quot; (#474)News update: September 2025 (#478)  ","version":null,"tagName":"h2"},{"title":"Updates​","type":1,"pageTitle":"10-01-2025 Release Notes","url":"/docuhub-staging/release-notes/v20251001#updates","content":" Added local mirror of NOAA Data Sharing Directive #473 ","version":null,"tagName":"h2"},{"title":"Contributing to CIROH DocuHub","type":0,"sectionRef":"#","url":"/docuhub-staging/docs/contribute/","content":"","keywords":"","version":"Next"},{"title":"Contributing simple changes to DocuHub​","type":1,"pageTitle":"Contributing to CIROH DocuHub","url":"/docuhub-staging/docs/contribute/#contributing-simple-changes-to-docuhub","content":" 1. Visit the documentation Visit docs.ciroh.org and navigate to the page you wish to modify. 2. Edit page Click on &quot;Edit page&quot; at the bottom of the page to make any necessary changes. 3. Submit a Pull Request Submit a pull request with your changes to the repository.  ","version":"Next","tagName":"h2"},{"title":"Submitting a blog post to DocuHub​","type":1,"pageTitle":"Contributing to CIROH DocuHub","url":"/docuhub-staging/docs/contribute/#submitting-a-blog-post-to-docuhub","content":" 1. Write your blog post Blog posts submitted to CIROH DocuHub should discuss projects that make use of CIROH's cyberinfrastructure. 2. Fill out the issue template Click the button below to access and fill out the blog post issue template with your project's information and the content of your blog post. 3. Submit the request form Submit the filled-out issue template for your blog posts. CIROH's tech team will review your PR for publishing shortly.  Blog Post Request Form  ","version":"Next","tagName":"h2"},{"title":"Requesting a product page on DocuHub​","type":1,"pageTitle":"Contributing to CIROH DocuHub","url":"/docuhub-staging/docs/contribute/#requesting-a-product-page-on-docuhub","content":" 1. Upload your project to GitHub Ensure that your project is publicly visible on GitHub and includes an informative README.md file. 2. Fill out the issue template Click the button below to access and fill out the product page issue template with your project's information. 3. Submit the request form Submit the filled-out issue template for your product. CIROH's tech team will review your submission shortly.  Product Page Request Form  ","version":"Next","tagName":"h2"},{"title":"For developers​","type":1,"pageTitle":"Contributing to CIROH DocuHub","url":"/docuhub-staging/docs/contribute/#for-developers","content":" If you'd like to get more closely involved with DocuHub's development, please see the subpages below:  Working with the DocuHub repositoryLearn more about DocuHub technologiesAdding blog posts to the DocuHub blog  ","version":"Next","tagName":"h2"},{"title":"Help and support​","type":1,"pageTitle":"Contributing to CIROH DocuHub","url":"/docuhub-staging/docs/contribute/#help-and-support","content":" If you have any questions or issues contributing, please don't hesitate to reach out via Slack or email.  Contact us ","version":"Next","tagName":"h2"},{"title":"DocuHub technologies","type":0,"sectionRef":"#","url":"/docuhub-staging/docs/contribute/technologies","content":"","keywords":"","version":"Next"},{"title":"Docusaurus​","type":1,"pageTitle":"DocuHub technologies","url":"/docuhub-staging/docs/contribute/technologies#docusaurus","content":" Docusaurus is a modern static website generator specially tuned to make it easier to maintain and update documentation. Docusaurus forms the core of DocuHub, and is responsible for handling most of the site's content.  Docusaurus Homepage  Docusaurus Documentation  ","version":"Next","tagName":"h2"},{"title":"Markdown​","type":1,"pageTitle":"DocuHub technologies","url":"/docuhub-staging/docs/contribute/technologies#markdown","content":" Markdown is used by Docusaurus for interpreting plain-text documentation. If you need extra help with markdown, check out the cheatsheet below.  Markdown Cheatsheet  ","version":"Next","tagName":"h2"},{"title":"Infima​","type":1,"pageTitle":"DocuHub technologies","url":"/docuhub-staging/docs/contribute/technologies#infima","content":" Infima is an open-source UI styling framework that makes it easier to develop websites using minimal CSS and JS knowledge. All Infima classes are freely available to style and shape your work on Docuhub.  Infima Documentation ","version":"Next","tagName":"h2"},{"title":"Adding posts to the DocuHub Blog","type":0,"sectionRef":"#","url":"/docuhub-staging/docs/contribute/blog","content":"","keywords":"","version":"Next"},{"title":"Adding posts manually​","type":1,"pageTitle":"Adding posts to the DocuHub Blog","url":"/docuhub-staging/docs/contribute/blog#adding-posts-manually","content":" Before continuing, you may want to set up a local environment for DocuHub. Please see the &quot;Major Edits&quot; section of the Working with the DocuHub Repository page for more information.  All DocuHub blog posts are written in the markdown format, with the extension .md. As such, they're compatible with all of the typical markdown syntax. Among other things, you can italicize text with *one asterisk on either side*, write bold text using **two asterisks**, or create subheaders by ## prefacing a line with two or three hash marks.  A markdown cheatsheet is available here.  ","version":"Next","tagName":"h2"},{"title":"Naming files​","type":1,"pageTitle":"Adding posts to the DocuHub Blog","url":"/docuhub-staging/docs/contribute/blog#naming-files","content":" To publish in the blog, create a Markdown file within the /blog/ directory. For example, the file at /blog/2019-09-05-hello-docuhub.md represents the first post written for this site.  DocuHub will extract a YYYY-MM-DD date from many patterns such as YYYY-MM-DD-my-blog-post-title.md or YYYY/MM/DD/my-blog-post-title.md. This allows for blog posts to easily be sorted and grouped by when they were published.  ","version":"Next","tagName":"h3"},{"title":"Formatting blog posts​","type":1,"pageTitle":"Adding posts to the DocuHub Blog","url":"/docuhub-staging/docs/contribute/blog#formatting-blog-posts","content":" The top of the blog post should contain metadata, which is used to store information about your article. This includes:  Your article's title and description.Your article's &quot;slug&quot;, or the text used for its URL. Please be sure to only include letters, numbers, and dashes (-) in the slug!Author information. This includes your name, a bit of information about yourself, a link to a website that represents you, and a photo.Tags! These can be used to find articles relevant to a specific topic, so it's a good idea to several that are relevant to your article. This can include the tools discussed in your article, as well as the institutions or events where the relevant research was hosted. For some examples of relevant, helpful tags, check out the current blog posts.  This metadata is wrapped by three dashes (---) on the lines above and below it, and is parsed as YAML data. If you're not sure what that means, don't worry - the template below offers an example of the format.  Additionally, blog posts should include a line containing the text &lt;!-- truncate --&gt; directly below their introductions. This tells DocuHub where the blog post's introduction ends, which allows it to display the introduction as a short-form summary in the blog's feed.  ","version":"Next","tagName":"h3"},{"title":"Blog post template​","type":1,"pageTitle":"Adding posts to the DocuHub Blog","url":"/docuhub-staging/docs/contribute/blog#blog-post-template","content":" To use the template below, copy and paste it into an empty .md file in the /blogs/ folder.  --- title: Your article's title! description: A one-sentence summary of your blog post. slug: your-articles-title authors: - name: John Doe title: Co-creator of Product 1 url: &lt;Your github product or external article link&gt; image_url: &lt;Author pic url&gt; - name: Jane Doe title: Co-creator of Product 2 url: &lt;Your github product or external article link&gt; image_url: &lt;Author pic url&gt; tags: [hello, docuhub, nextgen] hide_table_of_contents: false --- The text above the truncate line below will be displayed as a standalone summary of your article, so it's a great place for a quick and informative introduction. &lt;!-- truncate --&gt; Below the truncate line, you can add the rest of your blog post!  ","version":"Next","tagName":"h3"},{"title":"Working with the DocuHub repository","type":0,"sectionRef":"#","url":"/docuhub-staging/docs/contribute/repository","content":"","keywords":"","version":"Next"},{"title":"Minor Edits​","type":1,"pageTitle":"Working with the DocuHub repository","url":"/docuhub-staging/docs/contribute/repository#minor-edits","content":" To make minor edits, follow these steps:  Visit docs.ciroh.org and navigate to the page you wish to modify.Click on &quot;Edit page&quot; at the bottom of the page to make any necessary changes.Submit a Pull Request.An admin will review and merge your changes.  ","version":"Next","tagName":"h2"},{"title":"Major Edits​","type":1,"pageTitle":"Working with the DocuHub repository","url":"/docuhub-staging/docs/contribute/repository#major-edits","content":" For significant modifications, please adhere to these steps:  Fork the repository from https://github.com/CIROH-UA/ciroh-ua_website.After forking, implement your changes and commit them to your local repository.Open a pull request. Once submitted, an admin will review and merge it.GitHub Actions will automatically compile and publish the updates.  If you encounter any issues or have inquiries, please feel free to email us at ciroh-it-admin@ua.edu. Your contributions are highly valued!  ","version":"Next","tagName":"h2"},{"title":"Testing changes locally​","type":1,"pageTitle":"Working with the DocuHub repository","url":"/docuhub-staging/docs/contribute/repository#testing-changes-locally","content":" Please be sure to create a fork using the steps above before continuing.  If you haven't already, download and install the LTS version of Node.js from here.To build and run the project locally, navigate to the project directory in your command line interface of choice.Execute the following commands:  npm install npm run build npm run start   To quickly test out your changes, use this command:  $ npm run start   This command launches a local development server and opens a browser window. Changes are typically reflected instantly without requiring a server restart.  To deploy the website, you can instead compile the project into a deployable package:  $ npm run build   This will create a build directory within your project folder. You can then deploy the contents of this directory to your web server.  ","version":"Next","tagName":"h2"},{"title":"Video Tutorial:​","type":1,"pageTitle":"Working with the DocuHub repository","url":"/docuhub-staging/docs/contribute/repository#video-tutorial","content":" Still need some extra help? If so, the following video is a step-by-step guide on how to make minor edits, major edits, and test changes locally:     ","version":"Next","tagName":"h2"},{"title":"Data and Code Sharing","type":0,"sectionRef":"#","url":"/docuhub-staging/docs/policies/DataAndCodeSharingPolicy/","content":"Data and Code Sharing The Cooperative Institute for Research to Operations in Hydrology (CIROH) is committed to an open data policy that will maximize the impact and broad use of data and research products produced by CIROH projects and will also ensure that Federal data sharing requirements are met. This policy document is intended to assist CIROH investigators in creating and sharing high-quality data and research products. We begin with guiding principles, after which the specific policy and recommendations are stated. This document also provides guidance and instructions that may be useful to CIROH investigators in meeting the terms of this policy. Finally, we also include an appendix with further details about the specific data sharing requirements of CIROH’s partner agencies. We consider this policy to be a living document that will be revised as the needs of CIROH investigators and CIROH partner agencies evolve. 📄️ Policy and Guidance Policy and Guidance for Data and Code Sharing Policy 📄️ Recommendations Recommendations for Data and Code Sharing Policy","keywords":"","version":"Next"},{"title":"Technical guidance for the inclusion of models/modules in the NextGen Water Resources Modeling Framework","type":0,"sectionRef":"#","url":"/docuhub-staging/docs/policies/NextGen/","content":"","keywords":"","version":"Next"},{"title":"Introduction​","type":1,"pageTitle":"Technical guidance for the inclusion of models/modules in the NextGen Water Resources Modeling Framework","url":"/docuhub-staging/docs/policies/NextGen/#introduction","content":" This document provides technical guidance for including models and modules in the Next Generation Water Resources Modeling Framework (NextGen). It covers essential aspects of model integration, best practices, and framework requirements.  ","version":"Next","tagName":"h2"},{"title":"Full Document​","type":1,"pageTitle":"Technical guidance for the inclusion of models/modules in the NextGen Water Resources Modeling Framework","url":"/docuhub-staging/docs/policies/NextGen/#full-document","content":" For the complete technical guidance, please refer to the PDF document below:  Download Technical Guidance PDF  ","version":"Next","tagName":"h2"},{"title":"Key Points​","type":1,"pageTitle":"Technical guidance for the inclusion of models/modules in the NextGen Water Resources Modeling Framework","url":"/docuhub-staging/docs/policies/NextGen/#key-points","content":" NextGen is a model-agnostic, standards-based framework for water resources modelingIt allows for flexible experimentation with hydrologic cycle representationsThe framework supports explicit coupling of models through sharing of computed states and fluxesDesign requirements include maximum flexibility, open-source development, and multi-language support ","version":"Next","tagName":"h2"},{"title":"Policy and Guidance for Data and Code Sharing Policy","type":0,"sectionRef":"#","url":"/docuhub-staging/docs/policies/DataAndCodeSharingPolicy/Policies","content":"","keywords":"","version":"Next"},{"title":"Guiding Principles​","type":1,"pageTitle":"Policy and Guidance for Data and Code Sharing Policy","url":"/docuhub-staging/docs/policies/DataAndCodeSharingPolicy/Policies#guiding-principles","content":" We provide the following principles that guide CIROH’s activities and associated data sharing:  Science is reproducible.Reproducibility of scientific work is enabled through openness.Open science is enabled through open access to data, source code, accessible computational resources, and sufficient metadata for interpretation/use.Products of CIROH research are produced at public expense and should be broadly accessible to the public.  ","version":"Next","tagName":"h2"},{"title":"Policy Statement​","type":1,"pageTitle":"Policy and Guidance for Data and Code Sharing Policy","url":"/docuhub-staging/docs/policies/DataAndCodeSharingPolicy/Policies#policy-statement","content":" CIROH follows NOAA’s Data Sharing Directive, which is included in the Terms and Conditions of CIROH’s Cooperative Agreement with NOAA and is also available here (Version 3.0 at the time of this writing). CIROH is responsible for implementing these conditions and ensuring that they are also met by CIROH sub-recipients and subcontractors. The Data Management Plan submitted with the original CIROH proposal is included as an Appendix to this document.  The specific wording included in the CIROH Cooperative Agreement is as follows:  Data Sharing: Environmental data collected or created under this Grant, Cooperative Agreement, or Contract must be made publicly visible and accessible in a timely manner, free of charge or at minimal cost that is no more than the cost of distribution to the user, except where limited by law, regulation, policy, or national security requirements. Data are to be made available in a form that would permit further analysis or reuse: data must be encoded in a machine-readable format, preferably using existing open format standards; data must be sufficiently documented, preferably using open metadata standards, to enable users to independently read and understand the data. The location (internet address) of the data should be included in the final report. Pursuant to NOAA Information Quality Guidelines, data should undergo quality control (QC), and a description of the QC process and results should be referenced in the metadata.Timeliness: Data accessibility must occur no later than publication of a peer-reviewed article based on the data, or two years after the data are collected and verified, or two years after the original end date of the grant (not including any extensions or follow-on funding), whichever is soonest unless a delay has been authorized by the NOAA funding program.Disclaimer: Data produced under this award and made available to the public must be accompanied by the following statement: &quot;These data and related items of information have not been formally disseminated by NOAA, and do not represent any agency determination, view, or policy.&quot;Failure to Share Data: Failing or delaying to make environmental data accessible in accordance with the submitted Data Management Plan, unless authorized by the NOAA Program, may lead to enforcement actions and will be considered by NOAA when making future award decisions. Funding recipients are responsible for ensuring these conditions are also met by sub-recipients and subcontractors.Funding acknowledgment: Federal funding sources shall be identified in all scholarly publications. An Acknowledgements section shall be included in the body of the publication stating the relevant Grant Programs and Award Numbers. In addition, funding sources shall be reported during the publication submission process using the FundRef mechanism (http://www.crossref.org/fundref/) if supported by the Publisher.Manuscript submission: The final pre-publication manuscripts of scholarly publications produced with NOAA funding shall be submitted to the NOAA Institutional Repository at http://library.noaa.gov/repository after acceptance and no later than upon publication of the paper by a journal. NOAA will produce a publicly-visible catalog entry directing users to the published version of the article. After an embargo period of one year after publication, NOAA shall make the manuscript itself publicly visible, free of charge, while continuing to direct users to the published version of record.Data Citation: Publications based on data, and new products derived from source data, must cite the data used according to the conventions of the Publisher, using unambiguous labels such as Digital Object Identifiers (DOIs). All data and derived products that are used to support the conclusions of a peer-reviewed publication must be made available in a form that permits verification and reproducibility of the results.  ","version":"Next","tagName":"h2"},{"title":"Important Definitions​","type":1,"pageTitle":"Policy and Guidance for Data and Code Sharing Policy","url":"/docuhub-staging/docs/policies/DataAndCodeSharingPolicy/Policies#important-definitions","content":" There are several definitions in NOAA’s Data and Publication Sharing Directive that we provide here for interpretation of the above text. For the full list and for the exact statement of these definitions, refer to the full text of NOAA’s Data Sharing Directive (Version 3.0) at the link in the section above.  Research Results: Defined as environmental data and peer-reviewed publications under NOAA’s Data Sharing Directive.Environmental Data: Defined by NOAA Administrative Order (NAO) 212-15 as: Recorded and derived observations and measurements of: Physical, chemical, biological, geological, and geophysical properties and conditions of: Oceans, atmosphere, space environment, sun, and solid earth.Correlative data such as socio-economic data, related documentation, and metadata. Includes digital audio or video recordings of environmental phenomena and numerical model outputs used to support peer-reviewed publications.Data collected in a laboratory or other controlled environment, including measurements of animals and chemical processes. Data Sharing Directive: Defines &quot;data&quot; specifically as environmental data.Sharing Data: Making data publicly visible and accessible in a timely manner at no cost or minimal cost, in a machine-readable format based on open standards, along with necessary metadata.Timeliness: Data accessibility must occur no later than publication of a peer-reviewed article based on the data or within two years of data collection or grant end date, whichever is soonest, unless authorized delay by NOAA.Applicability: Applies to new data created by extramural funding recipients; internally produced NOAA data or collaborative research data are subject to the NOAA Data Access Directive.Exclusions: Laboratory notebooks, preliminary analyses, drafts of scientific papers, plans for future research, peer review reports, communications with colleagues, or physical objects are not covered under NOAA’s Data Sharing Directive. ","version":"Next","tagName":"h2"},{"title":"Data, Code Sharing and Infrastructure Policies","type":0,"sectionRef":"#","url":"/docuhub-staging/docs/policies/intro","content":"Data, Code Sharing and Infrastructure Policies In the following sections, we provide some practical guidance for CIROH researchers designed to help them meet the terms and conditions of CIROH’s Data, Code, and Infrastructure Policy, as discussed above, for different types of research products. Each section is focused on providing guidlines and recommendations for data, code, and infrastructure use.","keywords":"","version":"Next"},{"title":"Community Flood Inundation Mapping","type":0,"sectionRef":"#","url":"/docuhub-staging/docs/products/community-fim/","content":"Community Flood Inundation Mapping 📄️ FIM as a Service FIM as a Service 📄️ FIM Evaluation Framework FIM Evaluation Framework 📄️ FIM Database for Multi-Model Visualization Documentation for creating a FIM database for flood scenarios visualization and comparison.","keywords":"","version":"Next"},{"title":"Recommendations for Data and Code Sharing Policy","type":0,"sectionRef":"#","url":"/docuhub-staging/docs/policies/DataAndCodeSharingPolicy/Recommendations","content":"","keywords":"","version":"Next"},{"title":"Recommendations for Sharing Data​","type":1,"pageTitle":"Recommendations for Data and Code Sharing Policy","url":"/docuhub-staging/docs/policies/DataAndCodeSharingPolicy/Recommendations#recommendations-for-sharing-data","content":" Depending on the type and size of data you are producing and using, we recommend the following options for data archiving and sharing data:  HydroShare (www.hydroshare.org) When to use: Use for datasets under 1GB (increases are possible)Datasets that require spatial data services (THREDDS, WMS, etc)Datasets that need to be accessed from applications through APIsDatasets that are linked to other datasetsDatasets that require formal publishing with a DOIFor links and pointers to external datasetsConsider using a Creative Commons License for releasing data When not to use: Very large datasetsRapidly changing datasetsData with extensive sharing and license restrictions Cost of use: Free for researchers up to 20 GB per userFree for permanently published data Where to go for help: HydroShare HelpEmail help@cuahsi.org to reach the CUAHSI HydroShare teamHydroShare short videos on CUAHSI YouTube channel CIROH Cloud Amazon S3 storage via CIROH’s AWS account and Google Buckets, Azure, On-Premise When to use: Use for large datasetsData that is part of the NWM workflows (https://console.cloud.google.com/storage/browser/national-water-model)Cloud computing linked dataConsider linking to cloud share from HydroShare for discoverability When not to use: Smaller datasets you want to formally publish with a DOI (May complicate formal publication with DOI) Cost of use: Some uses may be covered by CIROH core funds (contact CIROH Cloud Team to start a request)Extensive uses may be charged to individual CIROH projects How to get access to CIROH AWS: More information on obtaining accesss to CIROH's AWS resources are available at this link: https://docs.ciroh.org/docs/services/cloudservices/aws/ Where to go for help: Email ciroh-it-admin@ua.edu UA CIROH Cloud TeamCIROH Cloud Slack Channel - #ciroh-ua-it-adminAWS support Slack Channel - #aws-ciroh-support Water Prediction Node (https://waternode.ciroh.org/) Who to contact for help: ciroh-it-admin@ua.edu  ","version":"Next","tagName":"h2"},{"title":"Recommendations for Sharing Code​","type":1,"pageTitle":"Recommendations for Data and Code Sharing Policy","url":"/docuhub-staging/docs/policies/DataAndCodeSharingPolicy/Recommendations#recommendations-for-sharing-code","content":" GitHub (www.github.com) When to use: Generally post your code on your institution’s GitHub organization - some projects may be appropriately hosted on CIROH organizationWe suggest forking the CIROH template for structured readme files, etc.Example: https://github.com/NOAA-OWP/owp-open-source-project-templateWhen multiple developers are actively developing software or other productsConsider using the three clause BSD3 or MIT licenseConsider linking to Zenodo to snapshot and get a DOI for your code When not to use: Not recommended for proprietary code (although private repositories are available in GitHub at cost) Cost of use: Free for public repositoriesFree for private repositories (limited functionality - e.g. no GitHub Actions/Runners, Wiki and other features) Where to go for help: GitHub discussion forumsCIROH Slack channels e.g., #ciroh-hydroinformatics-working-group Jupyter notebooks in HydroShare When to use: Sharing code as Jupyter notebooks that you want to be launchable into a computational environment like CIROH JupyterHubWhen you want your code to accompany data in one citable resource for reproducibility purposes When not to use: When code is rapidly changingWhen you want your code to be under formal version control Cost of use: Free for researchers to store up to 20 GB of content in HydroShareCUAHSI JupyterHub is free to useCIROH 2i2c JupyterHub is free to use (cost covered by CIROH core funds) Where to go for help: Email help@cuahsi.org for help with sharing notebooks in HydroShare and/or launching notebooks into the CUAHSI JupyterHub instanceHow to get access to CIROH 2i2c: https://docs.ciroh.org/docs/services/cloudservices/google/How to get access to 2i2c using Hydroshare: https://docs.ciroh.org/docs/services/cloudservices/google/hydroshareintegration  ","version":"Next","tagName":"h2"},{"title":"Recommendations for Sharing Models​","type":1,"pageTitle":"Recommendations for Data and Code Sharing Policy","url":"/docuhub-staging/docs/policies/DataAndCodeSharingPolicy/Recommendations#recommendations-for-sharing-models","content":" Model sharing can be viewed as “code sharing” or “data sharing,” and many of the suggested methods above can be adopted for model sharing. Consider the following options for sharing models:  GitHub - Supports sharing of model source codesHydroShare - Supports sharing of model programs and models instancesNextGen In A Box (NGIAB) - Use cloud computing or local machine to modify and execute NextGen based models in a docker containerCIROH Web Sites - Downloadable executables, model instances, installers, etc can be shared on the CIROH portal web site.  ","version":"Next","tagName":"h2"},{"title":"Recommendations for Sharing Workflows​","type":1,"pageTitle":"Recommendations for Data and Code Sharing Policy","url":"/docuhub-staging/docs/policies/DataAndCodeSharingPolicy/Recommendations#recommendations-for-sharing-workflows","content":" GitHub gists (e.g., to show how to use certain modules)Post materials on the CIROH DocuHubJupyterNotebooks in HydroShare - launch into CIROH JupyterHub environment or CUAHSI JupyterHub  ","version":"Next","tagName":"h2"},{"title":"Recommendations for Sharing Published Manuscripts​","type":1,"pageTitle":"Recommendations for Data and Code Sharing Policy","url":"/docuhub-staging/docs/policies/DataAndCodeSharingPolicy/Recommendations#recommendations-for-sharing-published-manuscripts","content":" GitHub (see https://github.com/NOAA-OWP/OWP-Presentations)CIROH Portal (see https://portal.ciroh.org/publications)Per NOAA - Don’t share preprints prior to peer review  ","version":"Next","tagName":"h2"},{"title":"Recommendations for Sharing Educational Materials​","type":1,"pageTitle":"Recommendations for Data and Code Sharing Policy","url":"/docuhub-staging/docs/policies/DataAndCodeSharingPolicy/Recommendations#recommendations-for-sharing-educational-materials","content":" HydroLearn - We recommend using www.HydroLearn.org which allows for and supports learning module sharing and dissemination of educational materials. Hydrolearn modules may be linked from CIROH Portal's Lessons tab. When to use: Broadly applicable learning modules related to hydrology and NWM When not to use: Highly specific, localized, tailored learning materials for your specific university or departmental coursesMaterial that requires specific and inaccessible data, software, etc. Cost to use: Free for open access learning modules Where to go for help: HydroLearn Contact Us ","version":"Next","tagName":"h2"},{"title":"FIM Evaluation Framework","type":0,"sectionRef":"#","url":"/docuhub-staging/docs/products/community-fim/fimeval/","content":"FIM Evaluation Framework","keywords":"","version":"Next"},{"title":"FIM as a Service","type":0,"sectionRef":"#","url":"/docuhub-staging/docs/products/community-fim/fimserv/","content":"FIM as a Service","keywords":"","version":"Next"},{"title":"Data Management and Access Tools","type":0,"sectionRef":"#","url":"/docuhub-staging/docs/products/data-management/","content":"Data Management and Access Tools 🗃️ Data Access 1 item 📄️ Water Prediction Node Water Node Website 📄️ HydroServer HydroServer Portal 🗃️ NETWA 1 item 📄️ HydroShare HydroShare 📄️ NWM BigQuery API REST API backed by National Water Model data, developed on Google Cloud Platform","keywords":"","version":"Next"},{"title":"FIM Database for Multi-Model Visualization and Comparison","type":0,"sectionRef":"#","url":"/docuhub-staging/docs/products/community-fim/fim-database/","content":"","keywords":"","version":"Next"},{"title":"Introduction​","type":1,"pageTitle":"FIM Database for Multi-Model Visualization and Comparison","url":"/docuhub-staging/docs/products/community-fim/fim-database/#introduction","content":" FIM (Flood Inundation Mapping) Database is a framework designed to store compiled flood maps from multiple hydraulic models including reference flood maps. The structure and workflow of the FIM Database for Multi-Model Flood Map Retrieval and Visualization is shown in Figure 1.    Figure 1: Overall framework of the database  This documentation is intended to guide the water resources community to create a FIM (Flood Inundation Mapping) Database using flood maps generated from multiple hydraulic models. Users can upload and visualize custom flood maps, visualize flood extent scenarios for different flows, and compare flood extents by creating the database, storing it in hydroshare, and finally using the existing web application for the visualization. The application is available in the apps list in CIROH portal apps with the name &quot;FIM (Flood Information Map Visualization Deck)&quot; as shown in Figure 2.    Figure 2: FIM Visualization Application  To access this application, click the arrow symbol located in the middle of the map image. You will then be navigated to the visualization application as shown in Figure 3.    Figure 3: FIM Visualization Application  ","version":"Next","tagName":"h2"},{"title":"Creating a FIM database​","type":1,"pageTitle":"FIM Database for Multi-Model Visualization and Comparison","url":"/docuhub-staging/docs/products/community-fim/fim-database/#creating-a-fim-database","content":" It is assumed that users already have at least one hydraulic model from which flood maps can be generated for any area of interest. We guide users to be able to compare the flood map from that model with the baseline flood map for the same area from NWM HAND using FIMServ tool developed by our CIROH partners in University of Alabama. The flowchart for the overall methodology for creating the database and using it for visualization is shown in Figure 4 and explained in detail in subsequent section.    Figure 4: Methodology  Step 1:  Re-run your flood model with as many flow values as you desire. A basic database model may include flood maps for flow values corresponding to return periods ranging from 2 years to 1000 years. If you are using the NWM flows, you can find the return period values using the NWM_returnperiod_finder_Tool tool developed at Hydroinformatics Lab, BYU. You will need to have NWM feature IDs for the streams in your domain. You can use NWM_FeatureID_finder_Tool to find the feature IDs for your domain. This code gives you the IDs along the mainstream path only.  Step 2:  Run the HAND (Height Above Nearest Drainage) model for your area of interest. Follow the following steps to generate flood maps using HAND method.  Generate the shape file of the model boundary from the model you have for your area of interest.Use the FimServe_Tool to create HAND-based flood maps by providing the same model boundary shapefile and the discharge values.This tool uses NWM (National Water Model) retrospective data by default to generate flood maps.To create flood maps for specific return periods, use return-period discharge values, instead of retrospective flow values, for each NWM reach ID using the tool mentioned in Step 1.Make sure the flood maps generated from your model and HAND model correspond to same discharge values if you want to make meaningful comparison.Clip the output maps with the model boundary shape file.  Step 3:  Extract all available flood maps generated from your hydraulic model and the HAND model. Flood maps may include:  Shapefiles of flood extentsDepth rastersWater surface elevation rastersVelocity rasters (if available)  Note: The HAND model generates only depth rasters, which can be converted to shapefiles using GIS tools.  Step 4:  Reproject all raster and vector files to EPSG:4326 (WGS 84).Ensure that the spatial extents of all files match, which is necessary for comparison in the FIM Visualization Application.You can use GIS software or python codes for reprojection and alignment.  Step 5:  Prepare input files for populating the database. Here is the list of input files that you need for creating and populating ONE database (corresponding to your area of interest).  Table 1 Required input files for creating FIM database.  S.N.\tGeneric File Name\tQuantity\tDescription1.\tFIM_input_data.csv\t1\tInsert details of the models that are to be used in FIM database 2.\tHuc8.csv\t1\tAssuming that the extent for all models is the same 3.\t{River}_{model}_flows.csv\t1 for each model type\tContains rating curve information for each model. Enter the file names of the raster and vector files that were generated in step 4. 4.\tFeature_ids_{model}.csv\t1 for each model type\tContains the list of NWM feature IDs for the area and their Lat, Long values  You can use these sample input files for your reference. Remember all the column headers should be exactly as they appear in these sample files.  Also, the name of the models in “software” column in FIM_input_data.csv file should match with the ones as shown in Table 2.  Table 2 The list of software names currently included in the database.  ModelTypeID\tSoftware0\tHEC-RAS 1D 1\tHEC-RAS 2D 2\tHEC-RAS 1D/2D Combo 3\tSRH-2D 4\tFIER 5\tAutoRoute 6\tHAND 7\tTRITON 8\tSatellite Observations 9\tSurveyed Flood Extents 10\tOthers  To get the HUC8 number for the area, use the latter part of NWM_FeatureID_finder_Tool.  Step 6:  Use the FIM_database_tool for step-by-step instructions to create and upload the FIM database to your HydroShare account.  Upload all input files (from Step 5) and GIS files (from Step 4) to the content section of Google Collab notebook.Run each code cell carefully following the notebook instructions.  ","version":"Next","tagName":"h2"},{"title":"Database storage​","type":1,"pageTitle":"FIM Database for Multi-Model Visualization and Comparison","url":"/docuhub-staging/docs/products/community-fim/fim-database/#database-storage","content":" After completing the code execution from Step 6:  All GIS files will be uploaded to your HydroShare account.Two JSON files will be generated:{filename}.json is used for custom file visualization{filename}_vis.json is used for scenario visualization    Figure 5: HydroShare resource with GIS files and two JSON files used for visualization  ","version":"Next","tagName":"h2"},{"title":"Visualization of database​","type":1,"pageTitle":"FIM Database for Multi-Model Visualization and Comparison","url":"/docuhub-staging/docs/products/community-fim/fim-database/#visualization-of-database","content":" ","version":"Next","tagName":"h2"},{"title":"Custom files visualization​","type":1,"pageTitle":"FIM Database for Multi-Model Visualization and Comparison","url":"/docuhub-staging/docs/products/community-fim/fim-database/#custom-files-visualization","content":" To view the individual flood maps, paste the URL of the {databasename}.json file in the FIM database uploader section of FIM Visualization Application. This will give you the list of all the flood map files available in your database. You can select the desired files that you want to visualize. The selected files will appear in the &quot;User Files&quot; section in the map layers of the visualization application. You can toggle between each individual files as shown in Figure 5.    Figure 6: Custom files visualization in FIM visualization Application  ","version":"Next","tagName":"h3"},{"title":"Scenario visualization​","type":1,"pageTitle":"FIM Database for Multi-Model Visualization and Comparison","url":"/docuhub-staging/docs/products/community-fim/fim-database/#scenario-visualization","content":" To view the scenarios, paste the URL of the {databasename}_vis.json file in the FIM Scenarios uploader section of the Visualization application. You can select the model type from the dropdown available in the top right panel. For a model, you can move the slider button and change the flow values to visualize the corresponding flood map as shown in Figure 7.    Figure 7: Scenario visualization in FIM visualization Application  ","version":"Next","tagName":"h3"},{"title":"Comparison of flood maps​","type":1,"pageTitle":"FIM Database for Multi-Model Visualization and Comparison","url":"/docuhub-staging/docs/products/community-fim/fim-database/#comparison-of-flood-maps","content":" Any two flood maps from any two models across any scenarios can be compared. To do this, select the FIM comparison option in the Data Uploader section and choose &quot;Existing Scenarios&quot;. Select the name of database, names of model, and the scenarios that you want to compare. This will allow comparative visualization of those scenarios along with displaying the comparison metrics as shown in Figure 8. Similar comparison can also be performed by uploading any two flood map raster files (with same raster boundary) externally from your machine by selecting &quot;User file upload&quot; instead of &quot;Existing Scenarios&quot;.    Figure 8: Comparison of two flood maps indicating different inundation extents and displaying comparison metrics for the two inundation maps, including proportion correction, bias ratio, hit rate, kappa value, fitness statistics, and mixed index  ","version":"Next","tagName":"h3"},{"title":"Funding Acknowledgement​","type":1,"pageTitle":"FIM Database for Multi-Model Visualization and Comparison","url":"/docuhub-staging/docs/products/community-fim/fim-database/#funding-acknowledgement","content":" This research was supported by the Cooperative Institute for Research to Operations in Hydrology (CIROH) with funding under award NA22NWS4320003 from the NOAA Cooperative Institute Program.  ","version":"Next","tagName":"h2"},{"title":"Contact information​","type":1,"pageTitle":"FIM Database for Multi-Model Visualization and Comparison","url":"/docuhub-staging/docs/products/community-fim/fim-database/#contact-information","content":" If you have any queries, please contact:  Pitamber Wagle Ph.D. Student, Brigham Young University Email : waglep@byu.edu ","version":"Next","tagName":"h2"},{"title":"National Water Model (NWM) BigQuery API","type":0,"sectionRef":"#","url":"/docuhub-staging/docs/products/data-management/bigquery-api/","content":"National Water Model (NWM) BigQuery API info More details about “Design and implementation of a BigQuery dataset and application programmer interface (API) for the U.S. National Water Model” paper can be found here. National Water Model API Documentation available here: https://nwm-api.ciroh.org/docs Steps to use CIROH NWM API Submit the form below to request access to NWM BigQuery API. NWM BigQuery API Access Request Form For an example usage, please refer to this script. Replace your API key and API_URL = 'https://nwm-api.ciroh.org'. Estimate the costs of your query before running the actual query. If you're not sure how to approach this step, please contact CIROH IT support. note The NWM BigQuery API is available for free to CIROH members and partners who have active CIROH projects. The cost-estimate step above is strictly to ensure that CIROH can continue to provide this service in the long-term. For implementation details, please refer to this repository. News Blog: https://docs.ciroh.org/blog/September%20Monthly%20Blog%20Update","keywords":"","version":"Next"},{"title":"nwmurl","type":0,"sectionRef":"#","url":"/docuhub-staging/docs/products/data-management/dataaccess/NWMURL Library","content":"","keywords":"","version":"Next"},{"title":"Installation​","type":1,"pageTitle":"nwmurl","url":"/docuhub-staging/docs/products/data-management/dataaccess/NWMURL Library#installation","content":" You can install nwmurl using pip:  pip install nwmurl   ","version":"Next","tagName":"h2"},{"title":"Usage​","type":1,"pageTitle":"nwmurl","url":"/docuhub-staging/docs/products/data-management/dataaccess/NWMURL Library#usage","content":" In the code, you can modify the input parameters, such as start_date, end_date, fcst_cycle, lead_time, varinput, geoinput, and runinput, to customize the NWM data retrieval. The code will generate a list of JSON header URLs tailored to your specified parameters using the generate_urls function.  ","version":"Next","tagName":"h2"},{"title":"Customize Your Data Retrieval for Operational Dataset​","type":1,"pageTitle":"nwmurl","url":"/docuhub-staging/docs/products/data-management/dataaccess/NWMURL Library#customize-your-data-retrieval-for-operational-dataset","content":" start_date: A string representing the starting date in the format &quot;YYYYMMDDHHMM&quot;.end_date: A string representing the ending date in the same format.fcst_cycle: A list of integers specifying forecast cycle numbers, e.g., [0, 1, 2, 3, 4]. These cycles represent specific points in time for which URLs will be generated.lead_time: A list of integers indicating lead times in hours for forecasts. It determines the time ahead of the forecast start, e.g., [1, 2, 3, 4].varinput: An integer or string representing the variable of interest within the NWM data. Available options include: 1 or \\&quot;channel_rt\\&quot; for channel routing data.2 or \\&quot;land\\&quot; for land data.3 or \\&quot;reservoir\\&quot; for reservoir data.4 or \\&quot;terrain_rt\\&quot; for terrain routing data.5 or \\&quot;forcing\\&quot; for forcing data.geoinput: An integer or string specifying the geographic region of interest. Options include:1 or \\&quot;conus\\&quot; for the continental United States.2 or \\&quot;hawaii\\&quot; for Hawaii.3 or \\&quot;puertorico\\&quot; for Puerto Rico. runinput: An integer or string representing the NWM run configuration. Available options include: 1 or \\&quot;short_range\\&quot; for short-range forecasts.2 or \\&quot;medium_range\\&quot; for medium-range forecasts.3 or \\&quot;medium_range_no_da\\&quot; for medium-range forecasts without data assimilation.4 or \\&quot;long_range\\&quot; for long-range forecasts.5 or \\&quot;analysis_assim\\&quot; for analysis-assimilation runs.6 or \\&quot;analysis_assim_extend\\&quot; for extended analysis-assimilation runs.7 or \\&quot;analysis_assim_extend_no_da\\&quot; for extended analysis-assimilation runs without data assimilation.8 or \\&quot;analysis_assim_long\\&quot; for long analysis-assimilation runs.9 or \\&quot;analysis_assim_long_no_da\\&quot; for long analysis-assimilation runs without data assimilation.10 or \\&quot;analysis_assim_no_da\\&quot; for analysis-assimilation runs without data assimilation.11 or \\&quot;short_range_no_da\\&quot; for short-range forecasts without data assimilation. urlbaseinput : An integer representing the NWM dataset. Available options include: 1: &quot;https://nomads.ncep.noaa.gov/pub/data/nccf/com/nwm/prod/&quot;.2: &quot;https://nomads.ncep.noaa.gov/pub/data/nccf/com/nwm/post-processed/WMS/&quot;.3: &quot;https://storage.googleapis.com/national-water-model/&quot;.4: &quot;https://storage.cloud.google.com/national-water-model/&quot;.5: &quot;gs://national-water-model/&quot;.6: &quot;gcs://national-water-model/&quot;.7: &quot;https://noaa-nwm-pds.s3.amazonaws.com/&quot;.8: &quot;s3://noaa-nwm-pds/&quot;.9: &quot;https://ciroh-nwm-zarr-copy.s3.amazonaws.com/national-water-model/&quot;. meminput : An integer representing the ensemble member designation ranging from 0 to 7write_to_file: A Boolean variable that saves the output urls into a .txt file if set 'True'  ","version":"Next","tagName":"h2"},{"title":"Customize Your Data Retrieval for Retrospective Dataset​","type":1,"pageTitle":"nwmurl","url":"/docuhub-staging/docs/products/data-management/dataaccess/NWMURL Library#customize-your-data-retrieval-for-retrospective-dataset","content":" start_date: A string representing the starting date in the format &quot;YYYYMMDDHHMM&quot;. end_date: A string representing the ending date in the same format. urlbaseinput : An integer representing the NWM dataset. Available options include: 1: &quot;https://noaa-nwm-retrospective-2-1-pds.s3.amazonaws.com/&quot;.2: &quot;s3://noaa-nwm-retrospective-2-1-pds/model_output/&quot;.3: &quot;https://ciroh-nwm-zarr-retrospective-data-copy.s3.amazonaws.com/noaa-nwm-retrospective-2-1-zarr-pds/&quot;.4: &quot;https://noaa-nwm-retrospective-3-0-pds.s3.amazonaws.com/CONUS/netcdf/&quot;. selectet_object_type: An integer representing the object type. Available options include: 1 for forcing data2 for model_output Selectet_var_types: An integer or string representing the variable of interest within the NWM data. Available options include: 1: &quot;.CHRTOUT_DOMAIN1.comp&quot;2: &quot;.GWOUT_DOMAIN1.comp&quot;3: &quot;.LAKEOUT_DOMAIN1.comp&quot;4: &quot;.LDASOUT_DOMAIN1.comp&quot;5: &quot;.RTOUT_DOMAIN1.comp&quot;6: &quot;.LDASIN_DOMAIN1.comp&quot; write_to_file: A Boolean A Boolean variable that saves the output urls into a .txt file if set True  ","version":"Next","tagName":"h2"},{"title":"Examples of how to use​","type":1,"pageTitle":"nwmurl","url":"/docuhub-staging/docs/products/data-management/dataaccess/NWMURL Library#examples-of-how-to-use","content":" For Operational dataset:  import nwmurl start_date = &quot;202201120000&quot; end_date = &quot;202201130000&quot; fcst_cycle = [0,8] lead_time = [1,18] varinput = 1 geoinput = 1 runinput = 1 urlbaseinput = 2 meminput = 1 write_to_file = False file_list = nwmurl.generate_urls_operational( start_date, end_date, fcst_cycle, lead_time, varinput, geoinput, runinput, urlbaseinput, meminput, write_to_file )   For Retrospective dataset:  import nwmurl start_date = &quot;200701010000&quot; end_date = &quot;200701030800&quot; urlbaseinput = 2 selected_var_types = [1, 2] selected_object_types = [1] write_to_file = True file_list = nwmurl.generate_urls_retro( start_date, end_date, urlbaseinput, selected_object_types, selected_var_types, write_to_file )   ","version":"Next","tagName":"h2"},{"title":"How to Contribute​","type":1,"pageTitle":"nwmurl","url":"/docuhub-staging/docs/products/data-management/dataaccess/NWMURL Library#how-to-contribute","content":" We welcome contributions to nwmurl! To contribute to the development of this library, please follow these steps:  Fork the repository on GitHub. Clone your fork to your local machine:` git clone https://github.com/CIROH-UA/nwmurl.git Create a new branch for your contribution:` git checkout -b feature/your-feature-name Make your code changes and improvements. Before submitting a pull request, make sure to update the package version in setup.py if necessary. Commit your changes with descriptive commit messages. Push your changes to your fork:`` Open a pull request on the main repository, describing your changes and why they should be merged.  We appreciate your contributions and will review your pull request as soon as possible. Thank you for helping improve nwmurl! ","version":"Next","tagName":"h2"},{"title":"HydroServer","type":0,"sectionRef":"#","url":"/docuhub-staging/docs/products/data-management/hydroserver/","content":"","keywords":"","version":"Next"},{"title":"Software and Technologies​","type":1,"pageTitle":"HydroServer","url":"/docuhub-staging/docs/products/data-management/hydroserver/#software-and-technologies","content":" The HydroServer software stack includes:  A user-oriented web application for creation of monitorings sites, site metadata, information about observed variables, etc.A Python package and desktop app for loading time series data from monitoring sites into HydroServerApplication Programming Interfaces (APIs) for data ingest into HydroServer, data querying and retrieval, and data and metadata managementA highly performant time series database for storing and managing time series data  Additional planned tools include:  A Python client package for retrieving time series data from HydroServerAutomated archival of time series data to the HydroShare repositoryIntegration of data quality control functionalityWeb app(s) for data visualization  The HydroServer software stack is being build using the following technologies:  Vue.js - A JavaScript framework for building web user interfacesPython/Django - A Python web framework for backend web developmentOpen Geospatial Consortium SensorThings API - An API specification and data model for managing and retrieving observations and metadata from sensor systems.Timescale Cloud - A cloud native implementation of PostgreSQL and its Timescale extension for storing and managing time series dataAmazon Web Services (AWS) - The HydroServer web application and APIs are deployed using AWS.  ","version":"Next","tagName":"h2"},{"title":"Access​","type":1,"pageTitle":"HydroServer","url":"/docuhub-staging/docs/products/data-management/hydroserver/#access","content":" As of August 28, 2023, we are currently working on setting up domains, associated security certificates, and additional settings, but we anticipate that the CIROH HydroServer instances will be:  https://hydroserver.ciroh.org - Production instance of HydroServer for CIROHhttps://hydroserver-dev.ciroh.org - Development instance for internal development and testinghttps://hydroserver-beta.ciroh.org - Beta instance for testing and demonstration of latest functionality  ","version":"Next","tagName":"h2"},{"title":"Open-Source Code Development​","type":1,"pageTitle":"HydroServer","url":"/docuhub-staging/docs/products/data-management/hydroserver/#open-source-code-development","content":" The HydroServer software stack is developed as open-source software using the BSD3 open source license. All code development is hosted in our GitHub repositories hosted under the HydroServer GitHub Organization https://github.com/hydroserver2/  ","version":"Next","tagName":"h2"},{"title":"Bugs and Issues​","type":1,"pageTitle":"HydroServer","url":"/docuhub-staging/docs/products/data-management/hydroserver/#bugs-and-issues","content":" Bugs, issues, and feature requests related to HydroServer applications can be reported via their respective GitHub repositories at:  https://github.com/orgs/hydroserver2/repositories  ","version":"Next","tagName":"h2"},{"title":"Development Team​","type":1,"pageTitle":"HydroServer","url":"/docuhub-staging/docs/products/data-management/hydroserver/#development-team","content":" The HydroServer software stack is under development at the Utah Water Research Laboratory at Utah State University. The main contributors include:  Jeff Horsburgh - Associate Professor, Utah Water Research Laboratory and Civil and Environmental Engineering, Utah State UniversityKen Lippold - Software Engineer, Utah Water Research Laboratory, Utah State UniversityDaniel Slaugh - Software Engineer, Utah Water Research Laboratory, Utah State UniversityMaurier Ramirez - Software Engineer, Utah Water Research Laboratory, Utah State University ","version":"Next","tagName":"h2"},{"title":"HydroShare","type":0,"sectionRef":"#","url":"/docuhub-staging/docs/products/data-management/hydroshare/","content":"","keywords":"","version":"Next"},{"title":"Software and Technologies​","type":1,"pageTitle":"HydroShare","url":"/docuhub-staging/docs/products/data-management/hydroshare/#software-and-technologies","content":" HydroShare is an operational repository at https://www.hydroshare.org. HydroShare includes the following functionality:  A user-oriented web application for creation of &quot;resources&quot; within which you can share data, models, computational notebooks, and other content files.A flexible, file-based data model for storing content created within HydroShare resources.A REST application programming interface (API) for programmatic access to HydroShare resources. You can automate and code most everything through HydroShare's API that you can do through the web user interface.A Python client package called &quot;hsclient&quot; that enables easier interaction with HydroShare's REST APIAbility to link to and launch computational notebooks, code, and content files into linked JupyterHub environments, including the CIROH JupyterHub, CUASHI JupyterHub, and CyberGIS JupyterHub.  ","version":"Next","tagName":"h2"},{"title":"Access​","type":1,"pageTitle":"HydroShare","url":"/docuhub-staging/docs/products/data-management/hydroshare/#access","content":" Anyone can access HydroShare by navigating to https://www.hydroshare.org and creating a user account. All users are automatically allocated a 20GB quota for content within HydroShare, but if you need more space you can make a request to CUAHSI.  ","version":"Next","tagName":"h2"},{"title":"Open-Source Code Development​","type":1,"pageTitle":"HydroShare","url":"/docuhub-staging/docs/products/data-management/hydroshare/#open-source-code-development","content":" HydroShare is an open source software development project, with repositories and source code available at https://github.com/hydroshare. HydroShare is developed as open-source software using the BSD 3-clause open source license.  ","version":"Next","tagName":"h2"},{"title":"Bugs and Issues​","type":1,"pageTitle":"HydroShare","url":"/docuhub-staging/docs/products/data-management/hydroshare/#bugs-and-issues","content":" Bugs, issues, and feature requests related to HydroShare can be reported via the main HydroShare GitHub repository at:  https://github.com/hydroshare/hydroshare/issues  ","version":"Next","tagName":"h2"},{"title":"Development Team​","type":1,"pageTitle":"HydroShare","url":"/docuhub-staging/docs/products/data-management/hydroshare/#development-team","content":" HydroShare is the work of many individuals and organizations who have contributed to its design and development over many years. For details, see https://github.com/orgs/hydroshare/people.  ","version":"Next","tagName":"h2"},{"title":"How to cite HydroShare​","type":1,"pageTitle":"HydroShare","url":"/docuhub-staging/docs/products/data-management/hydroshare/#how-to-cite-hydroshare","content":" The following citations should be used when citing HydroShare:  Tarboton, D. G., Ames, D. P., Horsburgh, J. S., Goodall, J. L., Couch, A., Hooper, R., Bales, J., Wang, S., Castronova, A., Seul, M., Idaszak, R., Li, Z., Dash, P., Black, S., Ramirez, M., Yi, H., Calloway, C., Cogswell, C. (2024). HydroShare Retrospective: A Review of Science and Technology Advances of a Comprehensive Data and Model Publication Environment for the Water Science Domain, Environmental Modelling &amp; Software, 172, 105902, https://doi.org/10.1016/j.envsoft.2023.105902.  Horsburgh, J. S., M. M. Morsy, A. M. Castronova, J. L. Goodall, T. Gan, H. Yi, M. J. Stealey, and D. G. Tarboton (2016). HydroShare: Sharing diverse environmental data types and models as social objects with application to the hydrology domain, JAWRA Journal of the American Water Resources Association, 52(4), 873-889, https://doi.org/10.1111/1752-1688.12363.  Tarboton, D. G., R. Idaszak, J. S. Horsburgh, J. Heard, D. Ames, J. L. Goodall, L. Band, V. Merwade, A. Couch, J. Arrigo, R. Hooper, D. Valentine and D. Maidment (2014). HydroShare: Advancing Collaboration through Hydrologic Data and Model Sharing, in D. P. Ames, N. W. T. Quinn and A. E. Rizzoli (eds.), Proceedings of the 7th International Congress on Environmental Modelling and Software, San Diego, California, USA, International Environmental Modelling and Software Society (iEMSs), ISBN: 978-88-9035-744-2, https://scholarsarchive.byu.edu/iemssconference/2014/Stream-A/7/. ","version":"Next","tagName":"h2"},{"title":"NWM Data Access","type":0,"sectionRef":"#","url":"/docuhub-staging/docs/products/data-management/dataaccess/","content":"","keywords":"","version":"Next"},{"title":"Input and Output Data of the National Water Model​","type":1,"pageTitle":"NWM Data Access","url":"/docuhub-staging/docs/products/data-management/dataaccess/#input-and-output-data-of-the-national-water-model","content":" Here, you will find resources that grant access to the input data used and the output data produced by the operational national water model.  ","version":"Next","tagName":"h2"},{"title":"Official NOMADS Resource​","type":1,"pageTitle":"NWM Data Access","url":"/docuhub-staging/docs/products/data-management/dataaccess/#official-nomads-resource","content":" The official NWM meteorological inputs and hydrology and routing outputs are accessible through both HTTP and FTP. These resources are provided by the National Center for Environmental Prediction (NCEP) at the following locations:  NOMADS - NOAA Operational Model Archive and Distribution System HTTPFTP  As of October 24, 2023, these resources include the following directories:  para_post-processed/ 22-Sep-2023 20:37 - post-processed/ 02-Nov-2020 14:31 - prod/ 24-Oct-2023 00:18 - v3.0/ 24-Oct-2023 00:18 -   The para_post-processed directory lacks specific documentation, although the &quot;para&quot; designation suggests it is a &quot;parallel&quot; execution, indicating a candidate production run under testing for operational use. In the post-processed dataset, you will find the following subdirectories:  NOMADS post-processed RFC: Outputs filtered down to RFC locations.WMS: Contains re-indexed/reformatted outputs in per-forecast netCDFs suitable for rapid querying and responsive for graph visualizations on the water.noaa.gov/map site.IMAGES: .png-formatted renderings of NWM output for various domains and variables.logs: Logs. :)  ","version":"Next","tagName":"h3"},{"title":"NODD - NOAA Open Data Dissemination Program​","type":1,"pageTitle":"NWM Data Access","url":"/docuhub-staging/docs/products/data-management/dataaccess/#nodd---noaa-open-data-dissemination-program","content":" &quot;The NOAA Open Data Dissemination (NODD) Program provides public access to NOAA's open data on commercial cloud platforms through public-private partnerships. These partnerships remove obstacles to public use of NOAA data, help avoid costs and risks associated with federal data access services, and leverage operational public-private partnerships with the cloud computing and information services industries.&quot; (For more information, visit NODD)  The NODD datasets made available through several public cloud vendors are an incredible resource for accessing NWM data for research and evaluative purposes. The NWS NODD datasets are listed on this page and include the following:  AWS​  AWS hosts two repositories as part of their sustainability data initiative.  The first repository contains the operational data (now hosts 4 week rolling collection of all output; it used to only be short range and the registry entry retains the description only for the short_range data here; alternatively, the same resource is described under the sustainability initiative page here.)  The catalog of AWS-hosted operational NWM data can be browsed here.  The second (and more useful) AWS repository contains several versions of the retrospective dataset each described on the main page under the open data registry here. (The same information is also on the AWS sustainability initiative webpage here )  The different catalogs of those [currently] five versions of that resource are linked below:  Two versions of NWM v2.1 retrospective netCDF, herezarr, here Two versions of NWM v2.0 retrospective netCDF, herezarr, here NWM v1.2 retrospective data netCDF, here  The AWS retrospective resource is the primary publicly available source for the version 1.0 of the “AORC” Analysis of Record for Calibration dataset, which is a 40-year best-available estimate of most common meteorological parameters required for hydrological modeling. Version 1.1 of the dataset will accompany the release of the NWM model version 3.0 retrospective (or 2.2 version??), hopefully in the next few weeks.  Jupyter notebook instructions for processing NWM Zarr and NetCDF output formats here  An example of pulling data from the channel output zarr 2.1 archive and writing the results to csv follows:  ''' #install these libraries if they aren't already installed !pip install zarr !pip install xarray !pip install s3fs !pip install numpy ''' # Import needed libraries import xarray as xr import numpy as np import s3fs from datetime import datetime, timedelta # open the zarr store url = &quot;s3://noaa-nwm-retrospective-2-1-zarr-pds/chrtout.zarr&quot; fs = s3fs.S3FileSystem(anon=True) store = xr.open_zarr(s3fs.S3Map(url, s3=fs)) # Function to get the time series for a specified reach id and and time range # then write it out to a csv file. def GetAndWriteTimeSeriesAtReach(reach_id, start_time_index, end_time_index): flows = streamflow_array.where(feature_id_array==reach_id, drop=True) df_flows = flows[start_time_index:end_time_index].to_dataframe() df_flows.to_csv(f'flows_{reach_id}.csv') # get an xarray array of the various values time_array = store['time'] feature_id_array = store['feature_id'] streamflow_array = store['streamflow'] # Define the feature IDs to check for feature_ids = [5781221, 5781223, 5781703] # Specify the start and end times of interest start_time = datetime(2015, 5, 23, 0, 0, 0) end_time = datetime(2015, 6, 24, 0, 0, 0) # Get the indices for the needed dates zero_start_time = start_date = datetime(1979, 2, 1, 0, 0, 0) start_time_index = int((start_time - zero_start_time).total_seconds() / 3600) end_time_index = int((end_time - zero_start_time).total_seconds() / 3600) for reach_id in feature_ids: GetAndWriteTimeSeriesAtReach(reach_id, start_time_index, end_time_index) ''' Simple Script for Retrieving Retrospective NWM Data from AWS Store Dan Ames, 11/17/2023 dan.ames@byu.edu '''   Google – Operational NWM Data​  Google hosts the most complete operational data archive of inputs and outputs from the National Water Model, with nearly every file since August 2018. The Google open data registry provides additional explanations here.  Operational data can be browsed here.Google also hosts a copy of the NWM v1.2 retrospective here.  Coming soon: Big Query  Efforts are underway to make some of the datasets from the NWM operational and retrospective simulations available on BigQuery for ultra-high-bandwidth access. Stay tuned...  Azure/Planetary Computer​  Microsoft hosts the NWM input and output datasets in Azure Blob Storage, associated with the Microsoft Planetary Computer.Microsoft Planetary ComputerTom Augspurger of Microsoft has a series of notebooks providing examples of how to use this data from his workshop at the first CIROH developers conference.Tom Augspurger's Notebooks  ","version":"Next","tagName":"h3"},{"title":"CIROH Resources​","type":1,"pageTitle":"NWM Data Access","url":"/docuhub-staging/docs/products/data-management/dataaccess/#ciroh-resources","content":" More detailed information and example usage will be available soon.  Kerchunk Retro (points to AWS 2.1 NetCDF Retro) Kerchunk Retro - Forcing complete; model output 2011-2020 Kerchunk Operational (points to Google assets – a simple text change can point to AWS short range, if desired) Kerchunk Operational  ","version":"Next","tagName":"h3"},{"title":"Other resources​","type":1,"pageTitle":"NWM Data Access","url":"/docuhub-staging/docs/products/data-management/dataaccess/#other-resources","content":" ESRI Living Atlas​  ESRI Living Atlas provides a map-enabled version of the NWM output, which can be accessed here.  Description of WRF-Hydro code:​  A detailed description of various aspects of the WRF-Hydro code, which produces the current NWM, can be found here.      📄️ nwmurl nwmurl is a Python library developed by CIROH 2023. It provides utility functions specifically designed to subset and generate National Water Model (NWM) data URLs. This library simplifies the process of accessing NWM data for various purposes such as analysis, modeling, and visualization. ","version":"Next","tagName":"h3"},{"title":"Getting Started","type":0,"sectionRef":"#","url":"/docuhub-staging/docs/products/data-management/netwa/gettingstarted","content":"","keywords":"","version":"Next"},{"title":"Accessing the Testbed​","type":1,"pageTitle":"Getting Started","url":"/docuhub-staging/docs/products/data-management/netwa/gettingstarted#accessing-the-testbed","content":" ","version":"Next","tagName":"h2"},{"title":"Remote Desktop Software​","type":1,"pageTitle":"Getting Started","url":"/docuhub-staging/docs/products/data-management/netwa/gettingstarted#remote-desktop-software","content":" One of the easiest ways to access the NETWA is through the a remote desktop software, such as Remote Desktop Connection, which is a pre-installed softare on Windows. Microsoft also has a version available for MacOs as well, but any software that allows remote desktop access will do. Below are steps to access your own virtual desktop on the testbed:  Launch Remote Desktop Connection or similar software to connect to your virtual desktopIn the &quot;computer:&quot; field, enter &quot;ciroh-testbed.uvm.edu&quot;Click &quot;connect&quot; and enter your university username and password when promptedCongratulations! You've just logged on to your very own NETWA virtual desktop  ","version":"Next","tagName":"h3"},{"title":"SSH​","type":1,"pageTitle":"Getting Started","url":"/docuhub-staging/docs/products/data-management/netwa/gettingstarted#ssh","content":" Another way to access the testbed is through the Secure Shell Protocol, more comomnly known as SSH. This option is more appropriate for users who are familiar with using the Linux terminal or other command-line interfaces.  Open a terminal application on your machine (there are built-in terminals on MacOS and Windows, or you can use a third-party app such as PuTTY or MobaXterm)Type the command ssh your_netId@ciroh-testbed.uvm.edu, using your actual netIDEnter your password when promptedNow you should be logged in to the testbed and be able to navigate via the command-line  ","version":"Next","tagName":"h3"},{"title":"Data Storage​","type":1,"pageTitle":"Getting Started","url":"/docuhub-staging/docs/products/data-management/netwa/gettingstarted#data-storage","content":" The /netfiles/ciroh/ directory on the NETWA has over 40 terabytes of disk space that researchers can use to store data - that's equivalent to 40 million megabytes! Needless to say, there's plenty of space for data. If you are using any of the data modules in forecast-workflow that require a directory in which to store downloaded files, such as nwm_fc.py or gfs_fc_thredds.py, please use &quot;/netfiles/ciroh/downloadedData/&quot; as the download path. These modules will check the download path to see if the data you are requesting is already downloaded before attempting to get it again. Using a common shared path for large downloadable datasets like the NWM and GFS ensurse an effiecint use of disk space and also saves time by not having users redownload existing data! You can specify this download path in the module get_data() functions with the data_dir parameter. If you have other private data that doesn't make sense to share, please create your own directroy in /netfiles/ciroh titled after your netID. For example, &quot;John Smith&quot; would store their persoanl data under /netfiles/ciroh/jsmith.  ","version":"Next","tagName":"h2"},{"title":"Setting up Mamba / Conda​","type":1,"pageTitle":"Getting Started","url":"/docuhub-staging/docs/products/data-management/netwa/gettingstarted#setting-up-mamba--conda","content":" Mamba is a lightweight version of Conda, a popular package manager for a variety of programming languages. It is used on the testbed to set up virtual environments that contain all of the relevant packages and dependencies for a given software repository or workflow. There are a few existing mamba/conda Python environments on the testbed already, such as one that houses all of the packages necessary to run the forecast-workflow repo (more specific instructions for accessing that here). This section will demonstrate how to initalize mamba or conda for a new user and how to see what virtual enviornments are available.  On the testbed, open a terminal and run the following command: /usr/local/miniforge3/bin/mamba shell init to install mamba or '/usr/local/miniforge3/bin/conda init' to install conda or both to install both! Note: you only have to do this once, not every time you open a new terminal or want to use mamba or conda.If the command ran successfully, you should see something like this now at the command line: (base) [jsmith@ciroh-testbed ~]$ The (base) text indicates that you are in the base environment. To see the list of packages in said enviornment (or any environment you happen to be in), run mamba list or conda list.To see a list of available environments, run mamba env list or, if using conda, 'conda info --envs'. Most relevant environments will be located in /data/condaShared/envs.Note: This is one of the VERY few commands that differ between mamba and conda. Thus, you can mostly use mamba and conda at the command line interchangeably. To activate an environmnet, simply run mamba activate env_name replacing env_name with the actual name of the environment. ","version":"Next","tagName":"h2"},{"title":"Welcome to the Water Prediction Node","type":0,"sectionRef":"#","url":"/docuhub-staging/docs/products/data-management/waternode/","content":"","keywords":"","version":"Next"},{"title":"Data Catalog​","type":1,"pageTitle":"Welcome to the Water Prediction Node","url":"/docuhub-staging/docs/products/data-management/waternode/#data-catalog","content":" The WPN has a STAC data catalog. The catalog can be accessed via the graphical browser or programatically via R or Python. The WPN has a tutorial demonstrating how to download WPN data via python. More tutorials for working with WPN data will be created at the tutorials page.  The two main initial focuses of the data catalog will be:  Flood maps created using satellite data. Maps created by NESDIS STAR using VIIRS, Sentinel 1/2, landsat, and GOES ABI will be made available.ET related data. The WPN will catalog NESDIS STAR soil moisture products as well as remotely sensed vegetation indices that have the potential to improve hydrological model validation and evaluation efforts.Baseline inundated extents and river widths. Baseline inundated extents are already being produced by the National Water Center and have the potential to inform the flood masp in the catalog. Multi-year baselines can also be informative when evaluating drought stricken regions. River widths have the potential to be assimilated into the National Water Model to improve synthetic rating curves as well as model discharge estimates.  ","version":"Next","tagName":"h2"},{"title":"Current WPN projects​","type":1,"pageTitle":"Welcome to the Water Prediction Node","url":"/docuhub-staging/docs/products/data-management/waternode/#current-wpn-projects","content":" The Water Prediction Node first project is making it easier to perform qualitative comparisons between flood maps created from satellite imagery and flood maps created using the inundation models used by the National Water Center (NWC). The WPN will enable these comparisons by focusing on:  Exposing WPN data catalog assets as a web mapping service or web mapping tile service. This will allow stakeholders to easily import WPN satellite derived flood maps into their GIS viewer of choice. This satellite derived flood map layer can then be quickly compared to modeled inundation.Creating easy to access collections of satellite imagery of notable floods so that retrospective evaluation is easier.Creating a web processing service that allows for the creation of agreement maps in the style of gval. These agreement maps will highlight areas of agreement and disagreement between the modelled and remotely sensed flood maps and will allow the computation of agreement metrics.  ","version":"Next","tagName":"h2"},{"title":"Website repository​","type":1,"pageTitle":"Welcome to the Water Prediction Node","url":"/docuhub-staging/docs/products/data-management/waternode/#website-repository","content":" The source for the website implementation can be found here ","version":"Next","tagName":"h2"},{"title":"Evaluation Tools","type":0,"sectionRef":"#","url":"/docuhub-staging/docs/products/evaluation/","content":"Evaluation Tools 📄️ CSES Community Streamflow Evaluation System 📄️ TEEHR Tools for Exploratory Evaluation in Hydrologic Research","keywords":"","version":"Next"},{"title":"Forecast-Workflow","type":0,"sectionRef":"#","url":"/docuhub-staging/docs/products/data-management/netwa/","content":"","keywords":"","version":"Next"},{"title":"About​","type":1,"pageTitle":"Forecast-Workflow","url":"/docuhub-staging/docs/products/data-management/netwa/#about","content":" The forecast-workflow repository was initally created to implement the AEM3D model to generate 7-day forecasts of Harmful Algal Blooms (HABs) in Lake Champlain. However, we've created some handy data grabbers along the way that we've modified in order to make them more useful for other CIROH researchers. The scripts for these data grabbers can be found in the data/ folder within the repository. This page will include documentation on how to use some of these data grabber tools we've created.  ","version":"Next","tagName":"h2"},{"title":"Cloning the repository​","type":1,"pageTitle":"Forecast-Workflow","url":"/docuhub-staging/docs/products/data-management/netwa/#cloning-the-repository","content":" The first thing you need to do in order to be able to use the tools in forecast-workflow is clone the repository into your user space using git.  Once logged on the testbed, open a new terminal and navigate to the directroy in which you'd like to store the repoThen, run git clone https://github.com/CIROH-UVM/forecast-workflow.gitThat's it! You now have the repo on your own personal user space. Be sure to checkout the repo on GitHub every now and then to make sure you have the most recent version of the repo.If you need to pull any updates to your local repo, simply run git pull from your forecast-workflow/ directory  ","version":"Next","tagName":"h2"},{"title":"Using Jupyter Notebooks​","type":1,"pageTitle":"Forecast-Workflow","url":"/docuhub-staging/docs/products/data-management/netwa/#using-jupyter-notebooks","content":" We recommmend using Jupyter notebooks for interactive computing, but you can also import forecast-workflow tools in a plain old python script as well (see code blocks below).  Log onto the CIROH VM using Remote Desktop Viewer or similar softwareOpen a new terminal and cd to the top level project directory for your python scripts (it could be your home directory)Activate the standard mamba environment with mamba activate standardRun jupyter lab to launch JupyterIf starting a new notebook, click on a kernel underneath the &quot;Notebooks&quot; bannerOr if you have a notebook you're working on, simply open that one and pick up where you left off!  Add the repo to your python path variable​  Evertime you start or restart a jupyter kernel, you will need to add the forecast-workflow directory to your sys.path variable so that python knows where to look for forecast-workflow code. You can do that with the following code block:  import sys sys.path.append(&quot;/absolute/path/to/your/forecast-workflow&quot;)   This cell should be at the top of your notebook, but you only need to run it once; comment out these lines after runing so that you do not add the same path to you sys.path over and over again.  Now, you can import the data grabbers into your notebook like any other module!  import data.nwm_fc as nwm import data.gfs_fc as gfs   Etc.  ","version":"Next","tagName":"h2"},{"title":"Data Grabber Demo​","type":1,"pageTitle":"Forecast-Workflow","url":"/docuhub-staging/docs/products/data-management/netwa/#data-grabber-demo","content":" There is a neat demo notebook that includes more in-depth instructions on how to use our data grabber tools. You can find that notebook at forecast-workflow/examples/get_data_demo.ipynb ","version":"Next","tagName":"h2"},{"title":"CSES","type":0,"sectionRef":"#","url":"/docuhub-staging/docs/products/evaluation/cses/","content":"CSES Community Streamflow Evaluation System (CSES) is a Python-based, user friendly, fast, and model agnostic streamflow evaluator tool. This tool can be used to evaluate any hydrological model that uses NHDPlus dataset. It allows a user to evaluate the performance of a hydrological model at the collocated USGS gauges and NHDPlus stream reaches. This Python-based tool helps visualize the results and investigate the model performance interactively. The current version of the tool is available on GitHub and can be accessed using the following link.","keywords":"","version":"Next"},{"title":"TEEHR","type":0,"sectionRef":"#","url":"/docuhub-staging/docs/products/evaluation/rtiteehr/","content":"TEEHR TEEHR (pronounced &quot;tier&quot;) is a python tool set for loading, storing, processing and visualizing hydrologic data, particularly National Water Model data, for the purpose of exploring and evaluating the datasets to assess their skill and performance.","keywords":"","version":"Next"},{"title":"Hydrofabric: An Introduction","type":0,"sectionRef":"#","url":"/docuhub-staging/docs/products/Hydrofabric/","content":"","keywords":"","version":"Next"},{"title":"Getting Started with Hydrofabric​","type":1,"pageTitle":"Hydrofabric: An Introduction","url":"/docuhub-staging/docs/products/Hydrofabric/#getting-started-with-hydrofabric","content":" To get started with Hydrofabric, you can follow these steps:  Install hydrofabric from Github using remotes:remotes::install_github(&quot;NOAA-OWP/hydrofabric) Attach the package into your R script:library(hydrofabric) Explore the Hydrofabric documentation to learn more about its features and how to use them:  Hydrofabric Documentation  ","version":"Next","tagName":"h2"},{"title":"Hydrofabric Resources​","type":1,"pageTitle":"Hydrofabric: An Introduction","url":"/docuhub-staging/docs/products/Hydrofabric/#hydrofabric-resources","content":" Hydrofabric Code RepositoryHydrofabric Data ","version":"Next","tagName":"h2"},{"title":"Machine Learning and AI Tools","type":0,"sectionRef":"#","url":"/docuhub-staging/docs/products/ml-ai/","content":"Machine Learning and AI Tools 📄️ SWEML Snow Water Equivalent Machine Learning 📄️ NWM-ML National Water Model - Machine Learning","keywords":"","version":"Next"},{"title":"Products Overview","type":0,"sectionRef":"#","url":"/docuhub-staging/docs/products/intro","content":"Products Overview At CIROH, our team of researchers, hydrologists, and engineers is committed to advancing our understanding of hydrologic processes, improving operational hydrologic forecasting techniques and workflows, collaborating on community water modeling, converting forecasts into practical solutions, and utilizing water predictions to help guide decision-making processes.","keywords":"","version":"Next"},{"title":"NWM-ML","type":0,"sectionRef":"#","url":"/docuhub-staging/docs/products/ml-ai/nwm_ml/","content":"NWM-ML","keywords":"","version":"Next"},{"title":"Advancing Snow Modeling","type":0,"sectionRef":"#","url":"/docuhub-staging/docs/products/ml-ai/sweml/","content":"Advancing Snow Modeling The Snow Water Equivalent Machine Learning(SWEML) incorporates ground-based snow measuring sites, remotely-sensed snow cover information, and a Artificial Neural Network to provide point estimations of Snow Water Equivalent. The network was trained on historical data data from NASA’s ASO missions, divided into regions, and then a LightGradientBoost Model was used to preform recursive feature elimination to produce an efficient feature selection and region-specific model. The class contains the required functions for downloading data, pre-processing, running inference, and for producing visualizations.","keywords":"","version":"Next"},{"title":"Mobile Apps","type":0,"sectionRef":"#","url":"/docuhub-staging/docs/products/mobile-apps/","content":"Mobile Apps 📄️ CIROH RIVR App CIROH RIVR App","keywords":"","version":"Next"},{"title":"What is National Water Model (NWM)?","type":0,"sectionRef":"#","url":"/docuhub-staging/docs/products/national-water-model/","content":"","keywords":"","version":"Next"},{"title":"Features - NWM3.0​","type":1,"pageTitle":"What is National Water Model (NWM)?","url":"/docuhub-staging/docs/products/national-water-model/#features---nwm30","content":" First time provision of NWM Total Water Level guidance for coastal areas of the Continental United States (CONUS), Hawaii and Puerto Rico / U.S. Virgin Island domains. This is accomplished via use of the Semi-implicit Cross-scale Hydroscience Integrated System Model (SCHISM) integrated within the NWM, to couple NWM freshwater discharge estimates with oceanic forcing from the Surge and Tide Operational Forecast System (STOFS) and Probabilistic Tropical Storm Surge (P-SURGE) model. Output will be provided in both NetCDF as well as Standard Hydrometeorological Exchange Format (SHEF) format. Each NetCDF file contains full TWL domain output for one output time step, while each SHEF file contains timeseries station output for the full length of each simulation. NWM Domain expansion to south-central Alaska (Cook Inlet, Copper River Basin, and Prince William Sound regions), enabling provision of NWM operational hydrologic model forecast guidance to this region. Addition of the National Blend of Models (NBM) as a forcing source for NWM CONUS medium-range forecasts and Alaska short-range and medium-range forecasts. Use of Multi-Radar Multi-Sensor (MRMS) precipitation as forcing for the NWM Analysis and Assimilation configuration over the Puerto Rico / U.S. Virgin Island domain. Ingest of RFC-supplied reservoir outflow forecasts at 77 additional locations, bringing the total of such sites to 392. Enhancements to the treatment of reservoirs, land surface parameters and calibration/regionalization approach leading to improvements in model skill.  ","version":"Next","tagName":"h2"},{"title":"Features - NWM2.1​","type":1,"pageTitle":"What is National Water Model (NWM)?","url":"/docuhub-staging/docs/products/national-water-model/#features---nwm21","content":" The NWM currently runs in four configurations:  Analysis and assimilation provides a snapshotof current hydrologic conditionsShort-Range produces hourly forecasts of streamflow and hydrologic states out to 15 hoursMedium-Range produces 3-hourly forecasts out to 10 daysLong-Range generates 30-day ensemble forecasts.  Source : https://water.noaa.gov/about/nwm ","version":"Next","tagName":"h2"},{"title":"NextGen In A Box","type":0,"sectionRef":"#","url":"/docuhub-staging/docs/products/ngiab/","content":"","keywords":"","version":"Next"},{"title":"Deployment Options​","type":1,"pageTitle":"NextGen In A Box","url":"/docuhub-staging/docs/products/ngiab/#deployment-options","content":" NGIAB-CloudInfra: A Docker-based NGIAB distribution suitable for both local and cloud-based workflows. (Installation)NGIAB-HPCInfra: A Singularity-based NGIAB distribution suitable for high-performance computing (HPC) environments.NextGen on CIROH-2i2c JupyterHub: A dedicated image on CIROH-2i2c JupyterHub for running NextGen In A Box.  ","version":"Next","tagName":"h3"},{"title":"Core Components​","type":1,"pageTitle":"NextGen In A Box","url":"/docuhub-staging/docs/products/ngiab/#core-components","content":" Data Preprocess: Simplifies data preparation for the NGIAB workflow through an interactive map interface and straightforward command line tools.TEEHR Evaluation: Provides comprehensive model evaluation capabilities.Data Visualizer: Delivers sophisticated geospatial and time series visualization.NGIAB Calibration: Offers intuitive calibration for models in NGIAB workflows, including the CFE and NOAH-OWP modules.    ","version":"Next","tagName":"h3"},{"title":"Getting Started​","type":1,"pageTitle":"NextGen In A Box","url":"/docuhub-staging/docs/products/ngiab/#getting-started","content":" NGIAB 101 Training Module: The best place to learn how to start using NGIAB.NWM, NextGen, and NGIAB: A quick summary of the National Water Model, the NextGen Framework, and NGIAB's place in making them more accessible.NGIAB at a Glance: An overview of compatibility, capbilities, and access methods for tools within the NGIAB ecosystem.Glossary: A reference for tools, file formats, and terminology that may be unfamiliar.Directory Structure: The directory structure used for running simulations with NextGen.    ","version":"Next","tagName":"h3"},{"title":"Other Resources​","type":1,"pageTitle":"NextGen In A Box","url":"/docuhub-staging/docs/products/ngiab/#other-resources","content":" NGIAB Website: A high-level overview of everything NGIAB can do.Community NextGen News: Get the latest updates on NGIAB development and more.    🗃️ Intro to NGIAB 5 items 🗃️ Distributions 3 items 🗃️ Components 5 items 🗃️ Community NextGen Repos 3 items 📄️ Cyberinfrastructure and Community NextGen Office Hours Need help with NGIAB or CIROH Cyberinfrastructure. Join us for office hours! 📄️ GitHub Repository Dashboard A dashboard for viewing CIROH's managed repositories and workflows. ","version":"Next","tagName":"h3"},{"title":"CIROH RIVR App","type":0,"sectionRef":"#","url":"/docuhub-staging/docs/products/mobile-apps/RIVR/","content":"","keywords":"","version":"Next"},{"title":"Introduction to the CIROH RIVR App​","type":1,"pageTitle":"CIROH RIVR App","url":"/docuhub-staging/docs/products/mobile-apps/RIVR/#introduction-to-the-ciroh-rivr-app","content":" Welcome to the RIVR App, your go-to tool for real-time and forecast river flow information. Whether you're a kayaker, fisherman, or hydrologist, RIVR provides critical data to help you plan and understand river conditions. With the RIVR App, you can monitor river levels, access short-term and long-term forecasts, and manage a list of your favorite rivers for quick access.    ","version":"Next","tagName":"h2"},{"title":"Background of the US NWM Streamflow Forecasts​","type":1,"pageTitle":"CIROH RIVR App","url":"/docuhub-staging/docs/products/mobile-apps/RIVR/#background-of-the-us-nwm-streamflow-forecasts","content":" The RIVR App utilizes streamflow forecasts generated by the National Water Model (NWM). The NWM is a hydrologic model for the United States that simulates the water cycle, including streamflow, soil moisture, and evapotranspiration. By leveraging the NWM, RIVR provides users with up-to-date and scientifically sound river flow predictions.  ","version":"Next","tagName":"h2"},{"title":"Creating a User Account​","type":1,"pageTitle":"CIROH RIVR App","url":"/docuhub-staging/docs/products/mobile-apps/RIVR/#creating-a-user-account","content":" To personalize your experience and manage favorite rivers, you'll need to create an account:  On the registration screen, you'll be asked to enter your name, email address, and desired password.Ensure your password meets the strength requirements, including uppercase and lowercase letters, numbers, and special characters.Click the &quot;Register&quot; button to create your account.If you already have an account, select &quot;Login now&quot;.          ","version":"Next","tagName":"h2"},{"title":"Finding Rivers using the Map Interface​","type":1,"pageTitle":"CIROH RIVR App","url":"/docuhub-staging/docs/products/mobile-apps/RIVR/#finding-rivers-using-the-map-interface","content":" The RIVR App offers a map interface to find river monitoring stations:  Upon opening the &quot;Add River&quot; section, you'll see a map with location markers.Zoom in or out to explore different areas.Markers indicate river monitoring stations. Tap on a marker to see more details. You may be able to also search for a place in the search bar, such as &quot;Louisiana&quot;.Choose from different map views such as &quot;Streets,&quot; &quot;Outdoors,&quot; &quot;Light,&quot; &quot;Standard,&quot; and &quot;Satellite&quot;.You will also be able to search by distance or name.          ","version":"Next","tagName":"h2"},{"title":"Viewing River Stations and Details​","type":1,"pageTitle":"CIROH RIVR App","url":"/docuhub-staging/docs/products/mobile-apps/RIVR/#viewing-river-stations-and-details","content":" You will be able to see various stations that displays the Station ID, Type, and Coordinates.By clicking the station, you can see more details such as the Station ID, coordinates, and an &quot;Add to Favorites&quot; option.    ","version":"Next","tagName":"h2"},{"title":"Editing and Managing Your List of Favorite Rivers​","type":1,"pageTitle":"CIROH RIVR App","url":"/docuhub-staging/docs/products/mobile-apps/RIVR/#editing-and-managing-your-list-of-favorite-rivers","content":" Manage your list of rivers within the RIVR App:  Your list of favorite rivers will be shown in the &quot;My Rivers&quot; section.If you have no favorite rivers yet, you'll be notified. You can add rivers from the map interface (see &quot;Finding Rivers using the Map Interface&quot;).To add a river to favorites, navigate to the river details and click the &quot;Add to Favorites&quot; or equivalent button.      ","version":"Next","tagName":"h2"},{"title":"Viewing Forecasts for a Specific River​","type":1,"pageTitle":"CIROH RIVR App","url":"/docuhub-staging/docs/products/mobile-apps/RIVR/#viewing-forecasts-for-a-specific-river","content":" To access detailed forecasts for a river:  Select a river from your &quot;My Rivers&quot; list, or choose it from the map interface.You'll be taken to the river's detail page, where you can view current flow, flow status, and forecast information.The river's name and location (e.g., Louisville, Kentucky) will be displayed.Current flow is shown in ft³/s.Flow status (e.g., &quot;Low&quot;) indicates the current condition.Return period information is available showing 2-year, 5-year, 10-year, 25-year, 50-year, and 100-year flow benchmarks.Hydrographs (graphs of flow over time) are provided for different ranges.        ","version":"Next","tagName":"h2"},{"title":"Hourly (Short Range) Forecasts​","type":1,"pageTitle":"CIROH RIVR App","url":"/docuhub-staging/docs/products/mobile-apps/RIVR/#hourly-short-range-forecasts","content":" Hourly forecasts provide detailed short-range flow predictions:  Select the &quot;Hourly&quot; tab or option on the river's detail page.See the hourly flow forecast in ft³/s for the next few hours.View the hourly hydrograph, which shows predicted flow over time. The graph may be available in different view options such as &quot;Wave View&quot;.      ","version":"Next","tagName":"h2"},{"title":"Daily (Medium Range) Forecasts​","type":1,"pageTitle":"CIROH RIVR App","url":"/docuhub-staging/docs/products/mobile-apps/RIVR/#daily-medium-range-forecasts","content":" Daily forecasts give medium-range flow predictions:  Select the &quot;Daily&quot; tab or option on the river's detail page.View the daily flow forecast for the next several days.See the daily hydrograph showing predicted flow trends over the next few days.          ","version":"Next","tagName":"h2"},{"title":"Long Range Forecasts​","type":1,"pageTitle":"CIROH RIVR App","url":"/docuhub-staging/docs/products/mobile-apps/RIVR/#long-range-forecasts","content":" Long-range (Monthly) forecasts provide an overview of flow trends:  Select the &quot;Monthly&quot; tab or option on the river's detail page.See the monthly flow forecast for the upcoming month.View the monthly hydrograph depicting predicted flow trends over the month.        ","version":"Next","tagName":"h2"},{"title":"User Account Management​","type":1,"pageTitle":"CIROH RIVR App","url":"/docuhub-staging/docs/products/mobile-apps/RIVR/#user-account-management","content":" In the settings you can see Units &amp; Theme, Notifications &amp; Alerts, Data Management, Help &amp; Information, and Feedback &amp; Support.  ","version":"Next","tagName":"h2"},{"title":"About the Developers​","type":1,"pageTitle":"CIROH RIVR App","url":"/docuhub-staging/docs/products/mobile-apps/RIVR/#about-the-developers","content":" The RIVR App is developed by a dedicated team passionate about hydrology and technology. You can reach out for feedback and support through the app's settings.  ","version":"Next","tagName":"h2"},{"title":"About CIROH​","type":1,"pageTitle":"CIROH RIVR App","url":"/docuhub-staging/docs/products/mobile-apps/RIVR/#about-ciroh","content":" The RIVR App is developed in collaboration with CIROH. mentioned the CIROH Hydroinformatics Tools and Technologies and that CIROH has 27 universities collaborating together, over 90 research projects completed, more than 200 students supported, 75+ publications in top journals, and $35 million in research funding secured. It also mentioned some of the various resources of CIROH such as DocuHub, CIROH Portal, Google BigQuery NWM API, Tethys Platform, Pantarhei HPC, and NSF Access.  Note: Please check the app's settings or contact support for the most up-to-date information and detailed feature explanations. ","version":"Next","tagName":"h2"},{"title":"Community NextGen Repositories","type":0,"sectionRef":"#","url":"/docuhub-staging/docs/products/ngiab/community-nextgen-repos/","content":"Community NextGen Repositories This section surfaces open-source repositories that are integrated in NextGen In A Box (NGIAB). Several of these repositories are forks that haven't recieved significant attention to their READMEs. As such, unlike most projects, these READMEs are not currently mirrored to DocuHub. If you just want to use these projects' functionality at a high level, NGIAB-CloudInfra's documentation is likely a better starting point. 📄️ ngen The core engine of the NextGen framework. 📄️ t-route A Python package for generating realistic and hydrologically consistent channel networks. 📄️ lstm Cutting edge neural network-based modeling, adapted and optimized for the BMI standard.","keywords":"","version":"Next"},{"title":"lstm","type":0,"sectionRef":"#","url":"/docuhub-staging/docs/products/ngiab/community-nextgen-repos/lstm","content":"","keywords":"","version":"Next"},{"title":"Repository info​","type":1,"pageTitle":"lstm","url":"/docuhub-staging/docs/products/ngiab/community-nextgen-repos/lstm#repository-info","content":" CIROH-UA/lstm · main branch  Basic Model Interface (BMI) for streamflow prediction using Long Short-Term Memory (LSTM) networksmain is the active branch for NGIAB integrationForked from NOAA-OWP/lstm  Open Repo ","version":"Next","tagName":"h2"},{"title":"ngen","type":0,"sectionRef":"#","url":"/docuhub-staging/docs/products/ngiab/community-nextgen-repos/ngen","content":"","keywords":"","version":"Next"},{"title":"Repository info​","type":1,"pageTitle":"ngen","url":"/docuhub-staging/docs/products/ngiab/community-nextgen-repos/ngen#repository-info","content":" CIROH-UA/ngen · ngiab branch  Next Generation Water Modeling Engine and Framework Prototype (ngiab branch - development version of NextGen)Forked from NOAA-OWP/ngen  Open Repo  ","version":"Next","tagName":"h2"},{"title":"Summary​","type":1,"pageTitle":"ngen","url":"/docuhub-staging/docs/products/ngiab/community-nextgen-repos/ngen#summary","content":" ngen is a model agnostic framework designed for building and integrating models rather than being a model itself. It focuses on a data-centric process that abstracts the addition of processes and data behind a standard. This design allows for greater flexibility and standardization in model creation and integration.  ","version":"Next","tagName":"h2"},{"title":"Key Features of ngen​","type":1,"pageTitle":"ngen","url":"/docuhub-staging/docs/products/ngiab/community-nextgen-repos/ngen#key-features-of-ngen","content":" Model Agnostic: ngen is not a specific model but rather a framework for building and integrating different models.Data-Centric: ngen emphasizes a data-centric approach, ensuring that data is central to the modeling process.Flexible and Standardized: ngen provides a standardized approach to model creation and integration, allowing for greater flexibility and ease of use.  ","version":"Next","tagName":"h3"},{"title":"Documentation​","type":1,"pageTitle":"ngen","url":"/docuhub-staging/docs/products/ngiab/community-nextgen-repos/ngen#documentation","content":" Documentation for the mainline NOAA-OWP repository is available here.  Note that examples and instructions documentation may not be one-to-one with NGIAB-based installations. The high-level concepts should be the same, though.  ","version":"Next","tagName":"h3"},{"title":"Presentation ~ NextGen Framework: Building Novel and Mimic Model Formulations​","type":1,"pageTitle":"ngen","url":"/docuhub-staging/docs/products/ngiab/community-nextgen-repos/ngen#presentation--nextgen-framework-building-novel-and-mimic-model-formulations","content":" Speaker: Fred L. Ogden, Ph.D., P.E., Chief Scientist (ST), NOAA/NWS Office of Water Prediction    The Basic Model Interface (BMI) standard employed in the Next Generation Water Resources Modeling (NextGen) framework enables the construction of both mimic and novel model formulations. This involves the ordered execution of modules designed to simulate individual processes or fluxes and the integration of these over time on various control volumes to simulate the temporal evolution of model states. These are referred to as &quot;multi-BMI&quot; formulations.  Currently, the order of execution of modules within a multi-BMI model formulation is determined by the ordering of modules in the run realization file. This presentation showcases model/module developments undertaken to date in this regard and discusses knowledge gaps and needs. Additionally, it proposes potential coding standards for modules that allow for the monitoring of conservation law enforcement at the framework level for different model formulations using various internal discretizations. ","version":"Next","tagName":"h2"},{"title":"t-route","type":0,"sectionRef":"#","url":"/docuhub-staging/docs/products/ngiab/community-nextgen-repos/t-route","content":"","keywords":"","version":"Next"},{"title":"Repository info​","type":1,"pageTitle":"t-route","url":"/docuhub-staging/docs/products/ngiab/community-nextgen-repos/t-route#repository-info","content":" CIROH-UA/t-route · ngiab branch  Tree based hydrologic and hydraulic routing (ngiab branch - development version)Forked from NOAA-OWP/t-route  Open Repo  ","version":"Next","tagName":"h2"},{"title":"Summary​","type":1,"pageTitle":"t-route","url":"/docuhub-staging/docs/products/ngiab/community-nextgen-repos/t-route#summary","content":" T-Route is a Python package for generating channel networks from digital elevation models (DEMs). It uses a tree-based approach to identify and connect channels, resulting in realistic and hydrologically consistent channel networks.  ","version":"Next","tagName":"h2"},{"title":"Key Features of T-Route:​","type":1,"pageTitle":"t-route","url":"/docuhub-staging/docs/products/ngiab/community-nextgen-repos/t-route#key-features-of-t-route","content":" Tree-Based Approach: T-Route uses a tree-based algorithm to identify and connect channels, ensuring hydrologically consistent channel networks.High-Resolution DEMs: T-Route can generate channel networks from high-resolution DEMs, capturing detailed channel morphology.Flexible and Extensible: T-Route provides a flexible framework for customizing the channel generation process and extending its capabilities. ","version":"Next","tagName":"h3"},{"title":"Community Hydrofabric Patcher","type":0,"sectionRef":"#","url":"/docuhub-staging/docs/products/ngiab/components/community-hydrofabric/","content":"Community Hydrofabric Patcher","keywords":"","version":"Next"},{"title":"Components","type":0,"sectionRef":"#","url":"/docuhub-staging/docs/products/ngiab/components/","content":"Components This tab contains documentation on the many components that interface with NextGen In A Box. These tools offer powerful, streamlined options to get the most out of the NGIAB ecosystem. 📄️ NGIAB Data Preprocess Automatically process data for the NextGen directory structure. 📄️ NGIAB TEEHR Integration Use the TEEHR toolkit to evaluate your model runs. 📄️ NGIAB Tethys Visualization Integration Visualize your model output with both spatial navigation and temporal graphing. 📄️ NGIAB Calibration Callibrate your models to improve their output. 📄️ Community Hydrofabric Patcher Community Hydrofabric Patcher","keywords":"","version":"Next"},{"title":"NGIAB Calibration","type":0,"sectionRef":"#","url":"/docuhub-staging/docs/products/ngiab/components/ngiab-calibration/","content":"NGIAB Calibration","keywords":"","version":"Next"},{"title":"NGIAB TEEHR Integration","type":0,"sectionRef":"#","url":"/docuhub-staging/docs/products/ngiab/components/ngiab-teehr/","content":"NGIAB TEEHR Integration","keywords":"","version":"Next"},{"title":"NGIAB Data Preprocess","type":0,"sectionRef":"#","url":"/docuhub-staging/docs/products/ngiab/components/ngiab-preprocessor/","content":"NGIAB Data Preprocess","keywords":"","version":"Next"},{"title":"NGIAB Tethys Visualization Integration","type":0,"sectionRef":"#","url":"/docuhub-staging/docs/products/ngiab/components/ngiab-visualizer/","content":"NGIAB Tethys Visualization Integration","keywords":"","version":"Next"},{"title":"GitHub Repository Dashboard","type":0,"sectionRef":"#","url":"/docuhub-staging/docs/products/ngiab/dashboard/","content":"GitHub Repository Dashboard IntroductionCommunity NextGen related repositoriesCIROH Workflow Statuses Introduction This page contains the list of Community NextGen related repositories that CIROH is maintaining. Some repositories originate from the NOAA-OWP organization, but are now forks and maintained by CIROH. Loading...","keywords":"","version":"Next"},{"title":"Distributions","type":0,"sectionRef":"#","url":"/docuhub-staging/docs/products/ngiab/distributions/","content":"Distributions This tab contains documentation on the various distributions of NGIAB currently available. If you're unsure of which distribution to use, NGIAB-CloudInfra is the most appropriate option for most users. 🗃️ NGIAB-CloudInfra 6 items 📄️ NGIAB-HPCInfra A Singularity-based NGIAB distribution; suitable for high-performance computing (HPC) environments. 📄️ NextGen on CIROH JupyterHub A dedicated image on CIROH-2i2c JupyterHub for running NextGen In A Box.","keywords":"","version":"Next"},{"title":"Building the NGIAB Docker Container","type":0,"sectionRef":"#","url":"/docuhub-staging/docs/products/ngiab/distributions/ngiab-docker/building","content":"Building the NGIAB Docker Container","keywords":"","version":"Next"},{"title":"NGIAB-CloudInfra","type":0,"sectionRef":"#","url":"/docuhub-staging/docs/products/ngiab/distributions/ngiab-docker/","content":"NGIAB-CloudInfra","keywords":"","version":"Next"},{"title":"NextGen on CIROH JupyterHub","type":0,"sectionRef":"#","url":"/docuhub-staging/docs/products/ngiab/distributions/nextgen-2i2c/","content":"","keywords":"","version":"Next"},{"title":"Working with HydroShare, AORC data, HydroFabric and NextGen on CIROH JupyterHub Tutorial​","type":1,"pageTitle":"NextGen on CIROH JupyterHub","url":"/docuhub-staging/docs/products/ngiab/distributions/nextgen-2i2c/#working-with-hydroshare-aorc-data-hydrofabric-and-nextgen-on-ciroh-jupyterhub-tutorial","content":" This HydroShare resource provides a tutorial on the use of the Consortium of Universities for the Advancement of Hydrologic Science, Inc. (CUAHSI) HydroShare repository and linked CIROH JupyterHub computing platform on 2i2c in support of CIROH collaborative research and NextGen modeling. It introduces use of Jupyter Notebooks for retrieval of NOAA Analysis of Record for Calibration (AORC) datasets and setting up and executing NextGen for a small test watershed as a starting point for research with NextGen.  You can find the resource here: https://www.hydroshare.org/resource/fc8539358fe64ca6a47468728a0687a1/  To open the resource in CIROH JupyterHub, click the &quot;Open with...&quot; button on the HydroShare page and select &quot;CIROH JupyterHub&quot;:    ","version":"Next","tagName":"h2"},{"title":"Command Line Examples​","type":1,"pageTitle":"NextGen on CIROH JupyterHub","url":"/docuhub-staging/docs/products/ngiab/distributions/nextgen-2i2c/#command-line-examples","content":" Here are some command-line examples related to working with the data:  # Virtual environment source /ngen/.venv/bin/activate python -m ngiab_data_cli -i &quot;gage-10109001&quot; -s # Hydrofabric python -m ngiab_data_cli -i &quot;cat-2861446&quot; -s # Forcing python -m ngiab_data_cli -i &quot;cat-2861446&quot; -f --start &quot;2021-10-01&quot; --end &quot;2022-09-30&quot; # Configuration python -m ngiab_data_cli -i &quot;cat-2861446&quot; -r --start &quot;2021-10-01&quot; --end &quot;2022-09-30&quot; # Run /dmod/bin/ngen-serial config/cat-2861446_subset.gpkg all config/cat-2861446_subset.gpkg all config/realization.json     ","version":"Next","tagName":"h2"},{"title":"Visualizations​","type":1,"pageTitle":"NextGen on CIROH JupyterHub","url":"/docuhub-staging/docs/products/ngiab/distributions/nextgen-2i2c/#visualizations","content":" Shown below is a sample visualization based on the above command-line example:   ","version":"Next","tagName":"h2"},{"title":"Contact Us","type":0,"sectionRef":"#","url":"/docuhub-staging/docs/products/ngiab/distributions/ngiab-docker/contact","content":"Contact Us","keywords":"","version":"Next"},{"title":"Contribution Guidelines","type":0,"sectionRef":"#","url":"/docuhub-staging/docs/products/ngiab/distributions/ngiab-docker/contribute","content":"Contribution Guidelines","keywords":"","version":"Next"},{"title":"Installation","type":0,"sectionRef":"#","url":"/docuhub-staging/docs/products/ngiab/distributions/ngiab-docker/install","content":"Installation","keywords":"","version":"Next"},{"title":"Getting Started","type":0,"sectionRef":"#","url":"/docuhub-staging/docs/products/ngiab/distributions/ngiab-docker/getting-started","content":"Getting Started info See the Intro to NGIAB pages for a closer look at the topics summarized here.","keywords":"","version":"Next"},{"title":"Specifications","type":0,"sectionRef":"#","url":"/docuhub-staging/docs/products/ngiab/distributions/ngiab-docker/specifications/","content":"Specifications 📄️ Containers and Guide Scripts Guidance on NGIAB's associated containers and guide scripts. 📄️ Model Run Directories Guidance on NGIAB's model run directories. 📄️ Realization Files Guidance on NextGen realization files. 📄️ Included Models Guidance on models included within NGIAB.","keywords":"","version":"Next"},{"title":"Containers and Guide Scripts","type":0,"sectionRef":"#","url":"/docuhub-staging/docs/products/ngiab/distributions/ngiab-docker/specifications/containers","content":"Containers and Guide Scripts","keywords":"","version":"Next"},{"title":"Model Run Directories","type":0,"sectionRef":"#","url":"/docuhub-staging/docs/products/ngiab/distributions/ngiab-docker/specifications/run-directories","content":"Model Run Directories","keywords":"","version":"Next"},{"title":"Included Models","type":0,"sectionRef":"#","url":"/docuhub-staging/docs/products/ngiab/distributions/ngiab-docker/specifications/models","content":"Included Models","keywords":"","version":"Next"},{"title":"Realization Files","type":0,"sectionRef":"#","url":"/docuhub-staging/docs/products/ngiab/distributions/ngiab-docker/specifications/realizations","content":"Realization Files","keywords":"","version":"Next"},{"title":"Intro to NGIAB","type":0,"sectionRef":"#","url":"/docuhub-staging/docs/products/ngiab/intro/","content":"Intro to NGIAB This tab contains introductory and ecosystem-wide information for NextGen In A Box. If you're starting completely from scratch, the NGIAB 101 learning module offers the best way to get started. 📄️ NWM, NextGen, and NGIAB An introduction to the NextGen Framework and its related models. 📄️ NGIAB at a Glance A tour of the capabilities of the NGIAB ecosystem. 📄️ Installing NGIAB Locally A guide to install NGIAB locally. 📄️ Glossary A quick reference for terms and jargon pertaining to NGIAB. 📄️ Run Configuration Directory Structure The directory structure used by NGIAB to define model runs.","keywords":"","version":"Next"},{"title":"NGIAB-HPCInfra","type":0,"sectionRef":"#","url":"/docuhub-staging/docs/products/ngiab/distributions/ngiab-singularity/","content":"NGIAB-HPCInfra","keywords":"","version":"Next"},{"title":"NGIAB at a Glance","type":0,"sectionRef":"#","url":"/docuhub-staging/docs/products/ngiab/intro/at-a-glance","content":"NGIAB at a Glance Explore the NextGen In A Box (NGIAB) ecosystem through the interactive tabs below. Click on Key Features, Capabilities, or Access Methods to learn more. Key FeaturesCapabilitiesAccess Methods NGIAB and Extensions\tKey features\tNOAA-OWP Tools/Libraries UtilizedData Preprocess Specializes in initial data preparationHandles subsetting and forcing processingSupports basic data processing tasksHelps with running NGIAB t-routehydrotoolshydrofabric tools NGIAB Implementation (Cloud, HPC) Focused specifically on model executionCore engine for running simulationsDoes not handle pre/post-processing tasks TEEHR Evaluation Handles both input and output processingSupports full workflow, from data preparation to cloud deployment Built to evaluate OWP model outputs Data Visualizer Focused on analysis and validationSupports data processing and output analysis Designed for OWP hydrofabric visualization DataStreamCLI Complete workflow for creating inputs for and executing NGIAB and managing outputsBackend of the NextGen Research DataStreamDiscrete tooling for tasks like forcing processing and BMI file generation ngen-calt-routehydrofabric tools NGIAB-Cal Simplifies hydrologic model calibration for NGIAB workflowsCreates calibration directory and configurations within the NGIAB folder structureRuns calibration process using DockerCopies calibrated parameters to model configurations ngen-cal","keywords":"","version":"Next"},{"title":"Run Configuration Directory Structure","type":0,"sectionRef":"#","url":"/docuhub-staging/docs/products/ngiab/intro/directories","content":"","keywords":"","version":"Next"},{"title":"Root directory ngen-run/​","type":1,"pageTitle":"Run Configuration Directory Structure","url":"/docuhub-staging/docs/products/ngiab/intro/directories#root-directory-ngen-run","content":" A NextGen run directory ngen-run contains the following subfolders:  config: model configuration files and hydrofabric configuration files. (required)forcings: catchment-level forcing timeseries files. Forcing files contain variables like wind speed, temperature, precipitation, and solar radiation. (required)lakeout: for t-route (optional)metadata programmatically generated folder used within ngen. Do not edit this folder. (automatically generated)outputs: This is where ngen will place the output files. (required)restart: For restart files (optional)  ngen-run/ │ ├── config/ │ ├── forcings/ │ ├── lakeout/ | ├── metadata/ │ ├── outputs/ │ ├── restart/   ","version":"Next","tagName":"h3"},{"title":"Configuration directory ngen-run/config/​","type":1,"pageTitle":"Run Configuration Directory Structure","url":"/docuhub-staging/docs/products/ngiab/intro/directories#configuration-directory-ngen-runconfig","content":" This folder contains the NextGen realization file, which serves as the primary model configuration for the ngen framework. This file specifies which models to run (such as NoahOWP/CFE, LSTM, etc), run parameters like date and time, and hydrofabric specifications (like location, gage, catchment).  Based on the models defined in the realization file, BMI configuration files may be required. For those models that require per-catchment configuration files, a folder will hold these files for each model in ngen-run/config/cat-config. See the directory structure convention below.  ngen-run/ | ├── config/ | │ | ├── nextgen_09.gpkg | | | ├── realization.json | | | ├── ngen.yaml | | | ├── cat-config/ | │ | | | ├──PET/ | │ | | | ├──CFE/ | │ | | | ├──NOAH-OWP-M/   NextGen requires a single geopackage file. This file is the hydrofabric (Johnson, 2022) (spatial data). An example geopackage can be found on Lynker-Spatial's website. Tools to subset a geopackage into a smaller domain can be found at Lynker's hfsubset. ","version":"Next","tagName":"h3"},{"title":"Glossary","type":0,"sectionRef":"#","url":"/docuhub-staging/docs/products/ngiab/intro/glossary","content":"","keywords":"","version":"Next"},{"title":"Concepts and Terms​","type":1,"pageTitle":"Glossary","url":"/docuhub-staging/docs/products/ngiab/intro/glossary#concepts-and-terms","content":" ","version":"Next","tagName":"h2"},{"title":"Catchment​","type":1,"pageTitle":"Glossary","url":"/docuhub-staging/docs/products/ngiab/intro/glossary#catchment","content":" A specific, contiguous geographic area. In hydrologic modeling, larger regions are divided into catchments to allow for localized modeling and outputs.  Note that catchment IDs may not be persistent across hydrofabrics. Instead, points of interest should be used as persistent identifiers.  ","version":"Next","tagName":"h3"},{"title":"Calibration​","type":1,"pageTitle":"Glossary","url":"/docuhub-staging/docs/products/ngiab/intro/glossary#calibration","content":" The process of tuning a hydrologic model's parameters to more accurately match observed outcomes.  The NGIAB ecosystem offers the NGIAB Calibration utility to streamline this process.  ","version":"Next","tagName":"h3"},{"title":"Evaluation​","type":1,"pageTitle":"Glossary","url":"/docuhub-staging/docs/products/ngiab/intro/glossary#evaluation","content":" The process of assessing the quality and performance of a hydrologic model.  The NGIAB ecosystem offers integration with the TEEHR toolkit to streamline this process.  ","version":"Next","tagName":"h3"},{"title":"Forcing​","type":1,"pageTitle":"Glossary","url":"/docuhub-staging/docs/products/ngiab/intro/glossary#forcing","content":" Values provided as inputs for a hydrologic model. The term comes from the practice of experimentally &quot;forcing&quot; certain input values by overriding the usual flow of data into a model.  ","version":"Next","tagName":"h3"},{"title":"Formulation​","type":1,"pageTitle":"Glossary","url":"/docuhub-staging/docs/products/ngiab/intro/glossary#formulation","content":" Some set of defined links between modules within the NextGen Framework.  ","version":"Next","tagName":"h3"},{"title":"Gage​","type":1,"pageTitle":"Glossary","url":"/docuhub-staging/docs/products/ngiab/intro/glossary#gage","content":" Short for a stream gauge or streamgage. Fixed locations where streamflow properties such as water level and streamflow are measured and recorded.  ","version":"Next","tagName":"h3"},{"title":"Hydrofabric​","type":1,"pageTitle":"Glossary","url":"/docuhub-staging/docs/products/ngiab/intro/glossary#hydrofabric","content":" Broadly speaking, a hydrofabric defines the physical hydrology of a region, allowing for the region to be modeled. These hydrofabrics are defined by physical attributes and the links between those attributes.  For the R library, see &quot;Hydrofabric (R library)&quot;. For the hydrofabric used as a baseline by most NextGen models, see &quot;USGS-NOAA Reference Fabric&quot;.  ","version":"Next","tagName":"h3"},{"title":"Nexus​","type":1,"pageTitle":"Glossary","url":"/docuhub-staging/docs/products/ngiab/intro/glossary#nexus","content":" A point of data exchange within a NextGen network. Nexuses are placed at endpoint locations where water flows to within a catchment, such as along its borders.  Note that nexus IDs may not be persistent across hydrofabrics. Instead, points of interest should be used as persistent identifiers.  ","version":"Next","tagName":"h3"},{"title":"Point of Interest​","type":1,"pageTitle":"Glossary","url":"/docuhub-staging/docs/products/ngiab/intro/glossary#point-of-interest","content":" A physical position that is being used in a model or hydrofabric, such as a gage location. Points of interest are tracked by a wide variety of sources, including the Army Corp National Inventory of Dams and the USGS Gages III database.  Within the USGS-NOAA reference fabric, points of interest are given a persistent POI identifier to allow for tracking across hydrofabric versions.  ","version":"Next","tagName":"h3"},{"title":"Realization​","type":1,"pageTitle":"Glossary","url":"/docuhub-staging/docs/products/ngiab/intro/glossary#realization","content":" The instructions that tell the NextGen Framework how to run a simulation.  Realizations are stored in model realization files. These files contain formulations of modules, along with the order in which those modules should be run.  ","version":"Next","tagName":"h3"},{"title":"Subsetting​","type":1,"pageTitle":"Glossary","url":"/docuhub-staging/docs/products/ngiab/intro/glossary#subsetting","content":" The practice of taking a subset of a larger dataset. Subsetting is both common and highly recommended for model runs, as it can greatly reduce the computational load required.  Both GUI and command-line utilities for subsetting the NextGen reference fabric are available within the Data Preprocess utility.  ","version":"Next","tagName":"h3"},{"title":"Vector Processing Unit (VPU)​","type":1,"pageTitle":"Glossary","url":"/docuhub-staging/docs/products/ngiab/intro/glossary#vector-processing-unit-vpu","content":" Regional discretizations used by the USGS-NOAA reference fabric. These VPUs designate fixed regions for model runs and data collections.  Somewhat related to catchments, though VPUs are significantly larger and act as collections of contiguous catchments.  ","version":"Next","tagName":"h3"},{"title":"File Formats​","type":1,"pageTitle":"Glossary","url":"/docuhub-staging/docs/products/ngiab/intro/glossary#file-formats","content":" ","version":"Next","tagName":"h2"},{"title":"Geopackage (.gpkg)​","type":1,"pageTitle":"Glossary","url":"/docuhub-staging/docs/products/ngiab/intro/glossary#geopackage-gpkg","content":" An open, non-proprietary data format for geographic inforamtion systems. NextGen uses geopackages to contain hydrofabrics.  ","version":"Next","tagName":"h3"},{"title":"JSON (.json)​","type":1,"pageTitle":"Glossary","url":"/docuhub-staging/docs/products/ngiab/intro/glossary#json-json","content":" Short for &quot;JavaScript Object Notation&quot;, JSON files are used to store objects consisting of name-value pairs. Despite the name, JSON sees wide use in non-JavaScript contexts.  NextGen uses JSON to store its model realization files (ngen-run/config/realization.json), which tell the NextGen engine how to execute a model run.  ","version":"Next","tagName":"h3"},{"title":"NetCDF (.nc)​","type":1,"pageTitle":"Glossary","url":"/docuhub-staging/docs/products/ngiab/intro/glossary#netcdf-nc","content":" An open standard for arrays of scientific data. NextGen uses the netCDF-4 standard to contain model variable metadata.  ","version":"Next","tagName":"h3"},{"title":"Tarball (.tar.gz)​","type":1,"pageTitle":"Glossary","url":"/docuhub-staging/docs/products/ngiab/intro/glossary#tarball-targz","content":" A common archive format among Linux users. Analogously to formats like .zip and .rar, it allows for folders of content to be shared as a downloadable file.  ","version":"Next","tagName":"h3"},{"title":"YAML (.yaml/.yml)​","type":1,"pageTitle":"Glossary","url":"/docuhub-staging/docs/products/ngiab/intro/glossary#yaml-yamlyml","content":" Short for &quot;Yet Another Markup Language&quot;, YAML files are typically used to store metadata. NextGen uses a YAML file to store run configurations (ngen-run/config/ngen.yaml).  ","version":"Next","tagName":"h3"},{"title":"Tools and Software​","type":1,"pageTitle":"Glossary","url":"/docuhub-staging/docs/products/ngiab/intro/glossary#tools-and-software","content":" ","version":"Next","tagName":"h2"},{"title":"Basic Modeling Interface (BMI)​","type":1,"pageTitle":"Glossary","url":"/docuhub-staging/docs/products/ngiab/intro/glossary#basic-modeling-interface-bmi","content":" A standardized set of function and parameter bindings that allow models to interact with external components in a predictable, modular way.  BMI has been adopted as the standard for models compatible with the NextGen framework. Documentation for BMI is available here.  ","version":"Next","tagName":"h3"},{"title":"Hydrofabric (R library)​","type":1,"pageTitle":"Glossary","url":"/docuhub-staging/docs/products/ngiab/intro/glossary#hydrofabric-r-library","content":" A standard library maintained by NOAA-OWP for processing and updating hydrofabrics in R.  Information on Hydrofabric is available here.  ","version":"Next","tagName":"h3"},{"title":"National Hydrography Dataset Plus (NHDPlus)​","type":1,"pageTitle":"Glossary","url":"/docuhub-staging/docs/products/ngiab/intro/glossary#national-hydrography-dataset-plus-nhdplus","content":" A comprehensive reference hydrofabric for the United States, maintained by the EPA and USGS. NHDPlusV2 is the most current and relevant version.  Relevant to NextGen due to its use as a base for the USGS-NOAA reference fabric. As such, many tools and utilities designed for the NHD and its descendents will also work for NextGen hydrofabrics.  ","version":"Next","tagName":"h3"},{"title":"TEEHR​","type":1,"pageTitle":"Glossary","url":"/docuhub-staging/docs/products/ngiab/intro/glossary#teehr","content":" Short for &quot;Tools for Exploratory Evolution in Hydrologic Research&quot;, TEEHR is a Python library that provides comprehensive tooling for model evaluation. TEEHR is included as a part of the NGIAB ecosystem.  Information on TEEHR is available here.  ","version":"Next","tagName":"h3"},{"title":"Tethys Platform​","type":1,"pageTitle":"Glossary","url":"/docuhub-staging/docs/products/ngiab/intro/glossary#tethys-platform","content":" Tethys is a Python-based platform for building geospatial web apps. It forms the core of NGIAB's Data Visualizer, alongside many other CIROH applications.  Information on the Tethys Platform is available here.  ","version":"Next","tagName":"h3"},{"title":"t-route​","type":1,"pageTitle":"Glossary","url":"/docuhub-staging/docs/products/ngiab/intro/glossary#t-route","content":" A dynamic channel routing model using in determining routing for river networks. T-route is BMI-compliant and is used to prepare hydrofabrics for both the National Water Model and NGIAB.  Information on t-route is available here.  ","version":"Next","tagName":"h3"},{"title":"USGS-NOAA Reference Fabric​","type":1,"pageTitle":"Glossary","url":"/docuhub-staging/docs/products/ngiab/intro/glossary#usgs-noaa-reference-fabric","content":" A hydrofabric jointly developed by the NOAA, USGS, and Lynker Spatial, which provides a shared baseline for NextGen models.  Implementation and usage information for this fabric is available here. ","version":"Next","tagName":"h3"},{"title":"Installing NGIAB Locally","type":0,"sectionRef":"#","url":"/docuhub-staging/docs/products/ngiab/intro/install","content":"","keywords":"","version":"Next"},{"title":"Installing Prerequisites​","type":1,"pageTitle":"Installing NGIAB Locally","url":"/docuhub-staging/docs/products/ngiab/intro/install#installing-prerequisites","content":" ","version":"Next","tagName":"h2"},{"title":"Windows​","type":1,"pageTitle":"Installing NGIAB Locally","url":"/docuhub-staging/docs/products/ngiab/intro/install#windows","content":" Install WSL: Head over to Microsoft's official documentation and follow their comprehensive guide on installing WSL: https://learn.microsoft.com/en-us/windows/wsl/installInstall Docker Desktop: Begin by downloading and installing Docker Desktop from the official website: https://docs.docker.com/desktop/install/windows-install/#install-docker-desktop-on-windowsStart Docker Desktop: After installation, launch the Docker Desktop application.Open WSL as Admin: Right-click on the WSL icon and select &quot;Run as Administrator&quot;.Verify Installation: In the WSL window, type the command docker ps -a to check if Docker is running correctly. This command should display a list of Docker containers.  warning If you've installed WSL before as a part of Docker, be sure to create a second WSL distribution that isn't tied to Docker. NextGen In A Box shell commands can't be run from within Docker's dedicated WSL environment.  ","version":"Next","tagName":"h3"},{"title":"Mac​","type":1,"pageTitle":"Installing NGIAB Locally","url":"/docuhub-staging/docs/products/ngiab/intro/install#mac","content":" Install Docker Desktop: Download and install Docker Desktop for Mac from: https://docs.docker.com/desktop/install/mac-install/Start Docker Desktop: Launch the Docker Desktop application once the installation is complete.Open Terminal: Open the Terminal application on your Mac.Verify Installation: Similar to Windows, use the command docker ps -a in the Terminal to verify Docker is functioning as expected.  ","version":"Next","tagName":"h3"},{"title":"Linux​","type":1,"pageTitle":"Installing NGIAB Locally","url":"/docuhub-staging/docs/products/ngiab/intro/install#linux","content":" Install Docker: The installation process for Linux varies depending on your distribution. Refer to the official documentation for detailed instructions: https://docs.docker.com/desktop/install/linux-install/Start Docker and Verify: Follow the same steps as described for Mac to start Docker and verify its installation using the docker ps -a command in the terminal.  ","version":"Next","tagName":"h3"},{"title":"Installing NGIAB-CloudInfra​","type":1,"pageTitle":"Installing NGIAB Locally","url":"/docuhub-staging/docs/products/ngiab/intro/install#installing-ngiab-cloudinfra","content":" Input Data: Download Sample Data: Use the provided commands to download sample data for the Sipsey Fork case study.To generate your own data: Refer to the NGIAB-datapreprocessor for instructions on generating custom input data.To generate your own data and run using NGIAB: Refer to the ngen-datastream repository for instructions on generating custom input data.  This section guides you through downloading and preparing the sample input data for the NextGen In A Box project.  ","version":"Next","tagName":"h2"},{"title":"Step 1: Create Project Directory​","type":1,"pageTitle":"Installing NGIAB Locally","url":"/docuhub-staging/docs/products/ngiab/intro/install#step-1-create-project-directory","content":" Windows users: WSL (Right click and run as Admin): For ease of access, you may want to store NGIAB's files in your Windows directories. To move there, run the following in your WSL CLI:  cd /mnt/c/Users/&lt;Folder&gt;   From there, navigate to the directory where you'd like to store NGIAB and its associated data.  mkdir -p NextGen cd NextGen   ","version":"Next","tagName":"h3"},{"title":"Step 2: Download Sample Data​","type":1,"pageTitle":"Installing NGIAB Locally","url":"/docuhub-staging/docs/products/ngiab/intro/install#step-2-download-sample-data","content":" info While this step isn't strictly necessary, it'll be useful for verifying that NGIAB is working properly on your system.  Within your project directory, create the ngen-data folder to hold the sample data.  mkdir -p ngen-data cd ngen-data   Use wget to download the compressed data file. Then, extract it.  wget https://ciroh-ua-ngen-data.s3.us-east-2.amazonaws.com/AWI-009/AWI_16_10154200_009.tar.gz tar -xf AWI_16_10154200_009.tar.gz   Then, return to the root of the project directory.  cd ..   ","version":"Next","tagName":"h3"},{"title":"Step 3: Clone and Run NGIAB​","type":1,"pageTitle":"Installing NGIAB Locally","url":"/docuhub-staging/docs/products/ngiab/intro/install#step-3-clone-and-run-ngiab","content":" warning For WSL users: Before pulling NGIAB, ensure that Git is configured to pull with LF line breaks instead of CRLF line breaks. Failing to do so will prevent NGIAB's shell scripts from correctly running. Information on triaging this issue is available in the NGIAB 101 training module.  Clone the NGIAB-CloudInfra repository.  git clone https://github.com/CIROH-UA/NGIAB-CloudInfra.git cd NGIAB-CloudInfra   At this point, everything you need to install NGIAB has been installed!To test your installation, try running the interactive guide script, which will help you navigate your first model run:  ./guide.sh   info For a broader introduction to using the NGIAB ecosystem, please see the NGIAB 101 training module. ","version":"Next","tagName":"h3"},{"title":"CIROH Cyberinfrastructure & Community NextGen Monthly Office Hours","type":0,"sectionRef":"#","url":"/docuhub-staging/docs/products/ngiab/office-hours","content":"","keywords":"","version":"Next"},{"title":"How to Join​","type":1,"pageTitle":"CIROH Cyberinfrastructure & Community NextGen Monthly Office Hours","url":"/docuhub-staging/docs/products/ngiab/office-hours#how-to-join","content":" Email us at ciroh-it-admin@ua.edu to receive the Teams Meeting link and calendar invitation.  ","version":"Next","tagName":"h2"},{"title":"What We Cover​","type":1,"pageTitle":"CIROH Cyberinfrastructure & Community NextGen Monthly Office Hours","url":"/docuhub-staging/docs/products/ngiab/office-hours#what-we-cover","content":" CIROH Research Cyberinfrastructure: Cloud computing resources, on-premise computing, and external computing accessCommunity NextGen: Advanced hydrologic modeling tools  ","version":"Next","tagName":"h2"},{"title":"Resources​","type":1,"pageTitle":"CIROH Cyberinfrastructure & Community NextGen Monthly Office Hours","url":"/docuhub-staging/docs/products/ngiab/office-hours#resources","content":" CIROH Cyberinfrastructure DocumentationNGIAB Ecosystem Documentation ","version":"Next","tagName":"h2"},{"title":"An Introduction to the National Water Model, The NextGen Framework, and NextGen In A Box","type":0,"sectionRef":"#","url":"/docuhub-staging/docs/products/ngiab/intro/what-is","content":"","keywords":"","version":"Next"},{"title":"What is the National Water Model?​","type":1,"pageTitle":"An Introduction to the National Water Model, The NextGen Framework, and NextGen In A Box","url":"/docuhub-staging/docs/products/ngiab/intro/what-is#what-is-the-national-water-model","content":" The National Water Model is the United States' core water prediction framework, offering predictions for the Continental United States (CONUS), Alaska, Hawaii, Puerto Rico, and the U.S. Virgin Islands.  While the model's core implementation is administered by the NOAA's Office of Water Prediction (NOAA-OWP), it is built on free and open-source components, allowing the broader hydrology community to run it locally, make adjustments, and propose changes.  ","version":"Next","tagName":"h2"},{"title":"What is the NextGen Framework?​","type":1,"pageTitle":"An Introduction to the National Water Model, The NextGen Framework, and NextGen In A Box","url":"/docuhub-staging/docs/products/ngiab/intro/what-is#what-is-the-nextgen-framework","content":" The Next Generation Water Resources Modeling Framework, most frequently referred to as NextGen, is a hydrologic modeling framework that forms the core for modern versions of the National Water Model.  The NextGen Framework aims to resolve current limitations of the National Water Model by transitioning to a highly modular, model-agnostic framework that improves regional interoperability and by making it easier for the hydrology community to append their own models. To enable these tenets, NextGen uses the Basic Model Interface (BMI) to standardize the format of each model's input, output, and properties, thus allowing for models written in any language to quickly be coupled together to create new outputs. These models are combined according to external configuration files, which ensures that all model runs are highly portable and reproducible.  The NextGen Framework is slated for adoption within the National Water Model in its forthcoming Version 4.  Source: The National Water Model, NOAA-OWP  ","version":"Next","tagName":"h2"},{"title":"What is NGen?​","type":1,"pageTitle":"An Introduction to the National Water Model, The NextGen Framework, and NextGen In A Box","url":"/docuhub-staging/docs/products/ngiab/intro/what-is#what-is-ngen","content":" NGen is the core implementation of the NextGen framework, responsible for administering connections between encapsulated models. While NGen is still under development, it's become an incredibly powerful tool for novel approaches to hydrology.  Unfortunately, these advantages are stymied somewhat by the difficulty of installing NGen. No standard installation process is currently available, instead requiring users to manage a lengthy chainof dependencies, compilers, and environmental conditions. This can already be a time-consuming task for experienced programmers, so for scientists wanting to apply it in their own research, installing and configuring NGen is severely unapproachable. These challenges fundamentally undermine the NextGen framework's core objectives regarding community accessibility and contribution.  ","version":"Next","tagName":"h2"},{"title":"What is NextGen In A Box?​","type":1,"pageTitle":"An Introduction to the National Water Model, The NextGen Framework, and NextGen In A Box","url":"/docuhub-staging/docs/products/ngiab/intro/what-is#what-is-nextgen-in-a-box","content":" Fortunately, there's a solution to this problem!NextGen In A Box is a containerized distribution of the NextGen framework. Containerization is an approach to software distribution and deployment that directly defines the host operating system, dependencies, and run conditions for software, which allows the software to be run in effectively identical conditions regardless of the host system. This means that with NextGen In A Box, anyone can get NextGen up and running in as few as 30 minutes!  Additionally, NextGen In A Box acts as the host platform for a full ecosystem of free, open-source utilities that allow users to customize, evaluate, and visualize their model runs. These tools lower the barrier to entry even further, allowing scientists to seamlessly integrate the NextGen framework into their workflows and produce exciting new discoveries.    If you'd like to get started with NextGen In A Box, check out the NGIAB 101 module, which contains everything you'll need to know to get started.  NGIAB 101 Module ","version":"Next","tagName":"h2"},{"title":"NextGen Research Datastream","type":0,"sectionRef":"#","url":"/docuhub-staging/docs/products/research-datastream/","content":"NextGen Research Datastream info The underlying forcing files and associated metadata the NextGen Research DataStream (NRDS) are also available through AWS S3 Explorer. You can browse and access these regularly updated resources at datastream.ciroh.org/index.html. In addition to NextGen forcings, daily NextGen simulation outputs via Datastream will soon be available.","keywords":"","version":"Next"},{"title":"CIROH Research Portal 🌐","type":0,"sectionRef":"#","url":"/docuhub-staging/docs/products/portal/","content":"","keywords":"","version":"Next"},{"title":"Core Technologies ⚙️​","type":1,"pageTitle":"CIROH Research Portal 🌐","url":"/docuhub-staging/docs/products/portal/#core-technologies-️","content":" Our technology stack combines powerful open-source tools with cloud infrastructure:  🚀 Docusaurus - Modern documentation framework🌐 Tethys Platform - Geospatial web app framework💧 HydroShare - Water data collaboration environment🎓 HydroLearn - Hydrologic education platform☁️ CIROH AWS Account - Cloud hosting infrastructure  🌍 Live Portal: Explore the production environment at https://portal.ciroh.org  ","version":"Next","tagName":"h2"},{"title":"DatastreamCLI","type":0,"sectionRef":"#","url":"/docuhub-staging/docs/products/research-datastream/cli/","content":"DatastreamCLI 📄️ Installation Learn what you'll need to install DataStreamCLI. 📄️ Usage Guide How to get the most out of DataStreamCLI. 📄️ Available Models The available models in DataStreamCLI. 📄️ Directory Structure The directory structure in DataStreamCLI. 📄️ CLI Options The available options in DataStreamCLI. 📄️ Internal Breakdown An end-to-end tour of the internal workings of DataStreamCLI.","keywords":"","version":"Next"},{"title":"Research Applications 📲​","type":1,"pageTitle":"CIROH Research Portal 🌐","url":"/docuhub-staging/docs/products/portal/#research-applications-","content":"   The portal includes applications developed using the Tethys Platform framework, and external applications.  Please use the following information to learn more about the applications hosted in the portal.  ","version":"Next","tagName":"h2"},{"title":"Native Applications 🏠​","type":1,"pageTitle":"CIROH Research Portal 🌐","url":"/docuhub-staging/docs/products/portal/#native-applications-","content":" Built using Tethys Portal  Application\tDescription📊 TethysDash\tInteractive dashboard for hydrological data visualization 🔍 Water Data Explorer\tMulti-source water data analysis tool 🧪 HydroCompute Demo\tUniversity of Iowa's statistical analysis showcase ❄️ SWEML\tSnow Water Equivalent visualization platform 📡 Grace Groundwater Tool\tGRACE satellite data analysis for groundwater 🌨️ Snow Inspector\tMODIS satellite snow cover analysis 🌊 CSES\tNational Water Model evaluation system  📍 Access all native apps: Tethys Portal  ","version":"Next","tagName":"h3"},{"title":"Proxy Applications 🔗​","type":1,"pageTitle":"CIROH Research Portal 🌐","url":"/docuhub-staging/docs/products/portal/#proxy-applications-","content":" External apps integrated into the Tethys Platform ecosystem  Application\tPurpose🛰️ FIM Visualization Deck\tFlood inundation mapping analysis 🗺️ OWP NWM Map Viewer\tNational Water Model visualization 💻 CIROH JupyterHub\tCloud-based computational environment 📚 HydroShare\tCollaborative data repository 🚨 NFFA APP\tReal-time flood alert system  You can develop an application using the Tethys platform, which can later be installed in the portal for greater visibility. Similarly, if you already have an application developed using another framework already deployed it can be added to the portal as a proxy applications  ","version":"Next","tagName":"h3"},{"title":"Develop Your Own Apps Using the Tethys Platform 🎮​","type":1,"pageTitle":"CIROH Research Portal 🌐","url":"/docuhub-staging/docs/products/portal/#develop-your-own-apps-using-the-tethys-platform-","content":" Besides Adding your application to the portal, you can also use the Tethys Platform to boost your applications and datasets.    Tethys Platform has been designed to lower the barrier to geospatial web app development. Convey your models and data as interactive web apps.  🐍 Python Powered💾 Data Oriented🌨️ Cloud Analysis🔬 Model-Centric  🔗 Useful links  ⚡ Getting started with Tethys Platform Documentation🎓 Follow some Tutorials📲 Explore the source code  ","version":"Next","tagName":"h3"},{"title":"Research Datasets 📊​","type":1,"pageTitle":"CIROH Research Portal 🌐","url":"/docuhub-staging/docs/products/portal/#research-datasets-","content":"   🔍 Explore curated hydrological datasets supporting:  Advanced forecasting models ⚡Water resource management strategies 💧Climate impact studies 🌍  ","version":"Next","tagName":"h2"},{"title":"Research Publications 📚​","type":1,"pageTitle":"CIROH Research Portal 🌐","url":"/docuhub-staging/docs/products/portal/#research-publications-","content":"   🧠 Access our growing library of research outputs:  9+ specialized collections 📂 Artificial Intelligence 🤖Ciroh CyberInfrastructure 🖥️Community Resources 🤝Hydrologic Modeling 💧Flood Inundation Modeling 🌊Model and Forecast Evaluation 📈Hydroinformatics 🌐Decision Support 🎯Early Career 🌱 Search filters by searching by title, author, or year 🔍Multi-format support (Journals, Conferences, Books) 📖  🔗 Connect with our Zotero Group Library  Note Filtering by title, author, or year only works if the exact text you type occurs anywhere in the citation’s title, author, or year.  ","version":"Next","tagName":"h2"},{"title":"Research Courses 🎓​","type":1,"pageTitle":"CIROH Research Portal 🌐","url":"/docuhub-staging/docs/products/portal/#research-courses-","content":"   📚 Discover open educational resources featuring:  CIROH-developed courses 🧩NOAA research integration 🌊Multi-level hydrology courses 📈  ","version":"Next","tagName":"h2"},{"title":"Contribute 🤝​","type":1,"pageTitle":"CIROH Research Portal 🌐","url":"/docuhub-staging/docs/products/portal/#contribute-","content":"   The Portal allows the users to contribute applications, datasets, publications, and courses through different forms at the contribute page.  ","version":"Next","tagName":"h2"},{"title":"Applications 📱​","type":1,"pageTitle":"CIROH Research Portal 🌐","url":"/docuhub-staging/docs/products/portal/#applications-","content":"   If you would like any current application that you have to appear on the portal.  You can use the following form and submit your application through the portal page.  Submission Requirements:  Field\tRequirementHydroShare Account\tRequired ✅ Application Title\tRequired ✅ Application URL (where your app is hosted)\tOptional ⚪️ Documentation URL\tOptional ⚪️ Thumbnail Image\tOptional ⚪️ Application Description (at least 150 words)\tRequired ✅ Keywords (e.g., hydrology, water data, etc.)\tOptional ⚪️  Under the hood the form creates a HydroShare resource for your application. If you would like to add your application using HydroShare directly, you can do the following:  🔑 Log in to HydroShare or create an account.➕ Create a HydroShare Resource.🏷️ Add the keyword: nwm_portal_app.📁 Upload any files related to your application (optional).🌍 Add any spatial or temporal coverages associated with your application (optional).📝 Fill in the following additional metadata: page_urlthumbnail_urldocs_url ✅ Make the resource public.  After following these steps, you will be able to see your app on the apps page of the portal  ","version":"Next","tagName":"h3"},{"title":"Datasets 💾​","type":1,"pageTitle":"CIROH Research Portal 🌐","url":"/docuhub-staging/docs/products/portal/#datasets-","content":"   To have your dataset appear on the portal, use our form to submit it through the portal page.  Submission Requirements:  Field\tRequirementHydroShare Account\tRequired ✅ Dataset Title\tRequired ✅ Dataset URL (where your app is hosted)\tOptional ⚪️ Documentation URL\tOptional ⚪️ Thumbnail Image\tOptional ⚪️ Dataset Description (at least 150 words)\tRequired ✅ Keywords (e.g., hydrology, water data, etc.)\tOptional ⚪️ Dataset Files\tOptional ⚪️  Similarly to the applications, the dataset form creates a HydroShare resource under the hood for you datasets If you would like to add your dataset using HydroShare directly, you can do the following:  🔑 Log in to HydroShare or create an account.➕ Create a HydroShare Resource.🏷️ Add the keyword: nwm_portal_data.📁 Upload any files related to your dataset (optional).🌍 Add any spatial or temporal coverages associated with your dataset (optional).📝 Fill in the following additional metadata: page_urlthumbnail_urldocs_url ✅ Make the resource public.  After following these steps, you will be able to see your dataset  ","version":"Next","tagName":"h3"},{"title":"Publications 📄​","type":1,"pageTitle":"CIROH Research Portal 🌐","url":"/docuhub-staging/docs/products/portal/#publications-","content":"   If you would like any current or past publication to appear on the portal publications page, you can use our submission form to submit your publication.  Submission Requirements:  🔖 DOI of your publication — use the format 10.1234/abcd.efgh (do not use a URL).📚 Select the collection for your publication: Artificial Intelligence 🤖CIROH CyberInfrastructure 🖥️Community Resources 🤝Hydrologic Modeling 💧Flood Inundation Modeling 🌊Model and Forecast Evaluation 📈Hydroinformatics 🌐Decision Support 🎯Early Career 🌱  Under the hood, the publications form imports the submitted publication into the selected collection within the Zotero CIROH Group Library. If you would like to add your publication to Zotero, follow these steps:  Request access to the Zotero CIROH Library Group.Return to your Zotero library.Sync your Zotero account to display the CIROH group folder.Click on a collection within the CIROH group folder, then add new citations using the Zotero icon in the URL bar or drag and drop items into the desired collection.  After following these steps, your publication will appear on the CIROH portal publications page.  ","version":"Next","tagName":"h3"},{"title":"Courses 🎓​","type":1,"pageTitle":"CIROH Research Portal 🌐","url":"/docuhub-staging/docs/products/portal/#courses-","content":" If you would like your courses to appear on the portal courses page, use our submission form.    Submission Requirements:  Field\tRequirementHydroShare Account\tRequired ✅ Course Title\tRequired ✅ Course URL (where your course is hosted)\tOptional ⚪️ Thumbnail Image\tOptional ⚪️ Course Description (at least 150 words)\tRequired ✅ Keywords (e.g., hydrology, water data, etc.)\tOptional ⚪️  Under the hood, the form creates a HydroShare resource for your course. To add a course directly via HydroShare:  🔑 Log in to HydroShare or create an account.➕ Create a HydroShare Resource.🏷️ Add the keyword: nwm_portal_module.📁 Upload any related files (optional).🌍 Add any spatial or temporal coverages (optional).📝 Fill in the additional metadata: page_urlthumbnail_url ✅ Make the resource public.  After completing these steps, your course will appear on the portal courses page.    ","version":"Next","tagName":"h3"},{"title":"🔗 Useful Resources​","type":1,"pageTitle":"CIROH Research Portal 🌐","url":"/docuhub-staging/docs/products/portal/#-useful-resources","content":" HydroShare Making Resources PublicHydroShare Resource TypesHydroShare Upload &amp; Publish DataZotero Quick-Start Guide    ","version":"Next","tagName":"h2"},{"title":"🛠️ Source Code​","type":1,"pageTitle":"CIROH Research Portal 🌐","url":"/docuhub-staging/docs/products/portal/#️-source-code","content":" Repository\tDescriptionCIROH-UA/ciroh-portal\tDocusaurus site and general portal configuration CIROH-UA/tethysportal-ciroh\tTethys deployment with all native apps  Found a bug? File an issue 👉    ","version":"Next","tagName":"h2"},{"title":"👥 Team​","type":1,"pageTitle":"CIROH Research Portal 🌐","url":"/docuhub-staging/docs/products/portal/#-team","content":" Brigham Young University – Dan Ames, Jim NelsonUniversity of Iowa – Ibrahim DemirAquaveo – Gio Romero, Michael Souffront, Nathan Swain    ","version":"Next","tagName":"h2"},{"title":"🚧 Ready to Develop?​","type":1,"pageTitle":"CIROH Research Portal 🌐","url":"/docuhub-staging/docs/products/portal/#-ready-to-develop","content":" Interested in adding a native Tethys app or integrating an existing tool? Reach out to the Aquaveo dev team:  Nathan SwainMichael SouffrontJacob JohnsonGiovanni Romero  We’re happy to help you make a splash! 🌊 ","version":"Next","tagName":"h2"},{"title":"DataStreamCLI Directory Structure","type":0,"sectionRef":"#","url":"/docuhub-staging/docs/products/research-datastream/cli/directories","content":"DataStreamCLI Directory Structure","keywords":"","version":"Next"},{"title":"DataStreamCLI Breakdown","type":0,"sectionRef":"#","url":"/docuhub-staging/docs/products/research-datastream/cli/breakdown","content":"DataStreamCLI Breakdown warning This document is primarily intended to explain what steps DataStreamCLI performs internally. Performing these steps manually is discouraged, as a typical call to scripts/datastream will handle all of this for you. If you're looking for a guide for how to run the script, run scripts/datastream-guide after installation for an interactive tutorial.","keywords":"","version":"Next"},{"title":"Installing DataStreamCLI","type":0,"sectionRef":"#","url":"/docuhub-staging/docs/products/research-datastream/cli/install","content":"Installing DataStreamCLI","keywords":"","version":"Next"},{"title":"DataStreamCLI Options","type":0,"sectionRef":"#","url":"/docuhub-staging/docs/products/research-datastream/cli/options","content":"DataStreamCLI Options","keywords":"","version":"Next"},{"title":"Available Models","type":0,"sectionRef":"#","url":"/docuhub-staging/docs/products/research-datastream/cli/models","content":"Available Models","keywords":"","version":"Next"},{"title":"DataStreamCLI Usage Guide","type":0,"sectionRef":"#","url":"/docuhub-staging/docs/products/research-datastream/cli/usage","content":"DataStreamCLI Usage Guide","keywords":"","version":"Next"},{"title":"NextGen Datastream Components","type":0,"sectionRef":"#","url":"/docuhub-staging/docs/products/research-datastream/components/","content":"NextGen Datastream Components 📄️ Datastream Python Tools Scripts to create ngen bmi module configuration files and validate ngen-run packages. 🗃️ Forcing Processor 1 item 📄️ Datastream Configuration NextGen Framework Research Datastream","keywords":"","version":"Next"},{"title":"Forcing Processor","type":0,"sectionRef":"#","url":"/docuhub-staging/docs/products/research-datastream/components/forcingprocessor/","content":"Forcing Processor","keywords":"","version":"Next"},{"title":"Forcing Processor Sources","type":0,"sectionRef":"#","url":"/docuhub-staging/docs/products/research-datastream/components/forcingprocessor/forcing_sources","content":"Forcing Processor Sources","keywords":"","version":"Next"},{"title":"NextGen Datastream Python Tools","type":0,"sectionRef":"#","url":"/docuhub-staging/docs/products/research-datastream/components/python_tools/","content":"NextGen Datastream Python Tools","keywords":"","version":"Next"},{"title":"Datastream Configuration","type":0,"sectionRef":"#","url":"/docuhub-staging/docs/products/research-datastream/components/research_datastream/","content":"Datastream Configuration","keywords":"","version":"Next"},{"title":"NextGen Datastream Status","type":0,"sectionRef":"#","url":"/docuhub-staging/docs/products/research-datastream/status","content":"NextGen Datastream Status","keywords":"","version":"Next"},{"title":"Snow Sensing and Modeling Tools","type":0,"sectionRef":"#","url":"/docuhub-staging/docs/products/snow-tools/","content":"Snow Sensing and Modeling Tools","keywords":"","version":"Next"},{"title":"Optimized Snow Sensor Location","type":0,"sectionRef":"#","url":"/docuhub-staging/docs/products/snow-tools/optimize-sensors/","content":"Optimized Snow Sensor Location","keywords":"","version":"Next"},{"title":"Intro to Snow Observations Modeling Analysis","type":0,"sectionRef":"#","url":"/docuhub-staging/docs/products/snow-tools/snow-intro/","content":"Intro to Snow Observations Modeling Analysis","keywords":"","version":"Next"},{"title":"Snow Sensing","type":0,"sectionRef":"#","url":"/docuhub-staging/docs/products/snow-tools/snow-sensing/","content":"Snow Sensing","keywords":"","version":"Next"},{"title":"SWEMLv2.0","type":0,"sectionRef":"#","url":"/docuhub-staging/docs/products/snow-tools/sweml-v2-0/","content":"SWEMLv2.0","keywords":"","version":"Next"},{"title":"Visualization and Analysis Tools","type":0,"sectionRef":"#","url":"/docuhub-staging/docs/products/visualization/","content":"Visualization and Analysis Tools 📄️ Tethys-CSES Community Streamflow Evaluation System (CSES) - Tethys Web Application","keywords":"","version":"Next"},{"title":"Tethys-CSES","type":0,"sectionRef":"#","url":"/docuhub-staging/docs/products/visualization/tethys-cses/","content":"Tethys-CSES","keywords":"","version":"Next"},{"title":"Public Cloud Services","type":0,"sectionRef":"#","url":"/docuhub-staging/docs/services/cloudservices/","content":"Public Cloud Services In tandem with the power of the public cloud, our team of researchers, hydrologists, and engineers at CIROH is committed to advancing our understanding of hydrologic processes, improving operational hydrologic forecasting techniques and workflows, collaborating on community water modeling, converting forecasts into practical solutions, and utilizing water predictions to help guide decision-making processes. By leveraging the scalability and flexibility of public cloud platforms like AWS and Google Cloud, CIROH Cloud empowers our team to conduct groundbreaking research in hydrology. This translates into a robust and efficient computing environment that accelerates discovery and innovation. 🗃️ CIROH AWS Account 2 items 📄️ CIROH Google Account Google Research Cloud 🗃️ CIROH JupyterHub 9 items 📄️ HydroShare A web-based hydrologic information system for data and model sharing. 📄️ CUAHSI JupyterHub cuahsi jupyterhub 📄️ Cloud Services Blogs Want to keep up with the latest updates in the cloud? If so, consider checking out the block pages for CIROH's cloud services partners.","keywords":"","version":"Next"},{"title":"Server Image Configurations","type":0,"sectionRef":"#","url":"/docuhub-staging/docs/services/cloudservices/2i2c/available-images","content":"","keywords":"","version":"Next"},{"title":"Pangeo Notebook base image​","type":1,"pageTitle":"Server Image Configurations","url":"/docuhub-staging/docs/services/cloudservices/2i2c/available-images#pangeo-notebook-base-image","content":" This configuration has a general set of pre-installed libraries and tools commonly used in geoscience for tasks such as data analysis and mapping. It allows users to run workflows directly from JupyterHub without the need to install software or manage dependencies. This base image serves as a starting point; users can run it as-is for general analysis or build on it with additional packages tailored to hydrology, climate science, and beyond.  ","version":"Next","tagName":"h2"},{"title":"NextGen National Water Model​","type":1,"pageTitle":"Server Image Configurations","url":"/docuhub-staging/docs/services/cloudservices/2i2c/available-images#nextgen-national-water-model","content":" This configuration has the NextGen Framework and its preprocessor tools pre-installed, allowing for them to be run from JupyterHub without users needing to install software or configure dependencies.  The below HydroShare resource contains a tutorial for using this image:  Tarboton, D., H. Salehabadi, A. Nassar, F. Baig, A. M. Castronova, I. Garousi-Nejad, A. Patel, P. Dash (2025). Working with HydroShare, AORC data, HydroFabric and NextGen on CIROH JupyterHub Tutorial, HydroShare, https://doi.org/10.4211/hs.fc8539358fe64ca6a47468728a0687a1 ","version":"Next","tagName":"h2"},{"title":"Infrastructure Access Guide","type":0,"sectionRef":"#","url":"/docuhub-staging/docs/services/access","content":"","keywords":"","version":"Next"},{"title":"Selecting a platform​","type":1,"pageTitle":"Infrastructure Access Guide","url":"/docuhub-staging/docs/services/access#selecting-a-platform","content":" Click the buttons below to open/close their panels.  What is your primary requirement?  High Performance Computing Which HPC platform are you planning to use? Pantarhei HPC Pantarhei offers CPU, GPU, and FPGA nodes. Get started with Pantarhei Wukong HPC Wukong offers CPU and GPU nodes. Get started with Wukong  Cloud Computing What kind of computing environment are you looking for? General Purpose CIROH provides researchers with access to enterprise-level AWS cloud services. CIROH AWS Account info Google-Specific Services CIROH Google VM Link CIROH provides researchers with access to enterprise-level Google Cloud services. CIROH Google Account info Interactive Computing Environment In collaboration with 2i2c, CIROH offers a dedicated JupyterHub environment on Google Cloud specifically designed for hydrological research. Both CPU and GPU options are available. CIROH-2i2c JupyterHub info  Data Storage and Archival Hydroshare is a collaborative, general-purpose repository for data, models, and other research products. HydroShare info Additionally, CIROH provides researchers with access to data buckets for use with AWS and Google Cloud services. CIROH AWS Account infoCIROH Google Account info  Data-Intensive Computing CIROH's AWS instance and Google VMs both offer solid options for data-intensive processes. CIROH AWS Account infoCIROH Google Account info    ","version":"Next","tagName":"h2"},{"title":"Accessing Public Cloud Services​","type":1,"pageTitle":"Infrastructure Access Guide","url":"/docuhub-staging/docs/services/access#accessing-public-cloud-services","content":" CIROH has partnered with AWS and Google Cloud to provide access to their cloud computing services.  note If using CIROH-2i2c services, please see the &quot;Accessing CIROH JupyterHub&quot; section below for additional steps.  ","version":"Next","tagName":"h2"},{"title":"Requesting Project Access​","type":1,"pageTitle":"Infrastructure Access Guide","url":"/docuhub-staging/docs/services/access#requesting-project-access","content":" PIs or Workshops Lead leading CIROH projects or workshops may use this form to request cloud computing resources on AWS or Google Cloud. Access is available to all consortium members and partners.  Submit a GitHub template request detailing your project requirements and specifications.Our team will review your request and assist you in obtaining the necessary access.  Cloud Infrastructure Request Form  note Please refer to this link for references to submitted forms.  CIROH Consortium members or partners are responsible for:  Management of CIROH subaccounts assigned to them.Project-specific software and environment configuration.Handling account creation and/or access for project contacts.  ","version":"Next","tagName":"h3"},{"title":"Cost of Use​","type":1,"pageTitle":"Infrastructure Access Guide","url":"/docuhub-staging/docs/services/access#cost-of-use","content":" Use of CIROH JupyterHub is free for all consortium projects.Individual projects are allotted $500 monthly for use of AWS and Google Cloud services.CIROH projects that anticipate exceeding the monthly budget for cloud services may request additional funds via the form below.  Exceeding Budget Request Form    ","version":"Next","tagName":"h3"},{"title":"Accessing CIROH-2i2c JupyterHub​","type":1,"pageTitle":"Infrastructure Access Guide","url":"/docuhub-staging/docs/services/access#accessing-ciroh-2i2c-jupyterhub","content":" In partnership with 2i2c, CIROH provides JupyterHub with both CPU and GPU capabilities.  ","version":"Next","tagName":"h2"},{"title":"Requesting Project Access​","type":1,"pageTitle":"Infrastructure Access Guide","url":"/docuhub-staging/docs/services/access#requesting-project-access-1","content":" PIs or Workshop Leads leading CIROH projects or workshops may use this form to request cloud computing resources (CIROH-2i2c JupyterHub). Access is available to all consortium members and partners.  Submit a GitHub template request detailing your project requirements and specifications.Our team will review your request and assist you in obtaining the necessary access.  Cloud Infrastructure Request Form  ","version":"Next","tagName":"h3"},{"title":"Requesting Individual Access​","type":1,"pageTitle":"Infrastructure Access Guide","url":"/docuhub-staging/docs/services/access#requesting-individual-access","content":" Submit one of the following forms to get access to CIROH JupyterHub environments:  CIROH-2i2c JupyterHub CPU Access Request Form  CIROH-2i2c JupyterHub GPU Access Request Form  note You will need to submit your GitHub username for this request. If you do not currently have a GitHub account, follow the instructions at GitHub.  ","version":"Next","tagName":"h3"},{"title":"Requesting Custom Images​","type":1,"pageTitle":"Infrastructure Access Guide","url":"/docuhub-staging/docs/services/access#requesting-custom-images","content":" To request custom images:  Create an environment.yml file by exporting your conda environment.Fill out the CIROH-2i2c JupyterHub Software Install form.  CIROH-2i2c JupyterHub Software Install Form    ","version":"Next","tagName":"h3"},{"title":"Accessing On-Premises Infrastructure​","type":1,"pageTitle":"Infrastructure Access Guide","url":"/docuhub-staging/docs/services/access#accessing-on-premises-infrastructure","content":" CIROH operates an on-premises infrastructure that includes high-performance computing (HPC) resources and specialized software via the Pantarhei and Wukong systems.  ","version":"Next","tagName":"h2"},{"title":"Requesting Project Access​","type":1,"pageTitle":"Infrastructure Access Guide","url":"/docuhub-staging/docs/services/access#requesting-project-access-2","content":" Principle Investigators (PIs) leading CIROH projects may use this form to request CIROH on-premise resources for their teams, including Pantarhei and Wukong. Access is available to all consortium members and partners.  Submit a GitHub template request detailing your project requirements and specifications.Our team will review your request and assist you in obtaining the necessary access.  Note: The On-Premises Infrastructure Request Form must be submitted by the PI of the project.  On-premises Infrastructure Request Form  ","version":"Next","tagName":"h3"},{"title":"Requesting Individual Access​","type":1,"pageTitle":"Infrastructure Access Guide","url":"/docuhub-staging/docs/services/access#requesting-individual-access-1","content":" Note: Before requesting individual access, the On-Premises Infrastructure Request Form above must be completed by your PI.  Non-UA users should complete the VPN Access Request section of the form below before proceeding.  From there, please complete the On-Premise Access Request section of the form below to request individual access to Pantarhei or Wukong.  On-Premise Access Request Form    ","version":"Next","tagName":"h3"},{"title":"Accessing JetStream2 through CIROH​","type":1,"pageTitle":"Infrastructure Access Guide","url":"/docuhub-staging/docs/services/access#accessing-jetstream2-through-ciroh","content":" In collaboration with NSF Access, CIROH offers access to an allocation on the JetStream2 computing platform.  Step 1: The PI for your project must submit the Infrastructure Request Form below to request team-wide access to JetStream2.  Infrastructure Request Form      Step 2: If you don't already have an NSF Access account, register for one using the link below.   NSF Access New User Registration      Step 3: Using your NSF Access ID, submit the JetStream2 Access Request form for individual user accounts on JetStream2.   JetStream2 Access Request Form      info If you are unable to access the JetStream2 forms, please contact the CIROH team at ciroh-it-admin@ua.edu for assistance.  Once you're granted access, you're ready to begin using JetStream2! Visit the logging in to JetStream2 page to get started.    ","version":"Next","tagName":"h2"},{"title":"Accessing NWM BigQuery API​","type":1,"pageTitle":"Infrastructure Access Guide","url":"/docuhub-staging/docs/services/access#accessing-nwm-bigquery-api","content":" To access CIROH's NWM BigQuery API, please submit the form below.  NWM BigQuery API Access Request Form    ","version":"Next","tagName":"h2"},{"title":"Requesting Infrastructure Support for Conferences and Programs​","type":1,"pageTitle":"Infrastructure Access Guide","url":"/docuhub-staging/docs/services/access#requesting-infrastructure-support-for-conferences-and-programs","content":" CIROH Project Leads can request computing resources for workshops using this process. Available infrastructure includes NSF Access VMs, CIROH-2i2c JupyterHub, AWS, and Google Cloud - accessible to all consortium members and partners.  Complete our GitHub template with your workshop's technical requirements.Our team will process your request and ensure participants have necessary access before your workshop begins.  Workshop IT Request Form ","version":"Next","tagName":"h2"},{"title":"CIROH-2i2c JupyterHub","type":0,"sectionRef":"#","url":"/docuhub-staging/docs/services/cloudservices/2i2c/","content":"","keywords":"","version":"Next"},{"title":"Powered by 2i2c JupyterHub on Google Cloud​","type":1,"pageTitle":"CIROH-2i2c JupyterHub","url":"/docuhub-staging/docs/services/cloudservices/2i2c/#powered-by-2i2c-jupyterhub-on-google-cloud","content":" CIROH, in collaboration with 2i2c, offers a dedicated JupyterHub environment on Google Cloud specifically designed for hydrological researchers. 2i2c is a cloud service provider specializing in open-source infrastructure for research and development.    ","version":"Next","tagName":"h2"},{"title":"Video Tutorial​","type":1,"pageTitle":"CIROH-2i2c JupyterHub","url":"/docuhub-staging/docs/services/cloudservices/2i2c/#video-tutorial","content":" Watch this video to find out how to access CIROH-2i2c Jupyterhub and how to launch and use CIROH-2i2c Jupyterhub with HydroShare:    ","version":"Next","tagName":"h2"},{"title":"Benefits of CIROH-2i2c JupyterHub:​","type":1,"pageTitle":"CIROH-2i2c JupyterHub","url":"/docuhub-staging/docs/services/cloudservices/2i2c/#benefits-of-ciroh-2i2c-jupyterhub","content":" Managed JupyterHub as a Service: CIROH Cloud takes care of the entire JupyterHub infrastructure, allowing researchers to focus on their scientific endeavors. Open Source Powerhouse: Built on open-source tools, 2i2c JupyterHub offers flexibility, scalability, and a collaborative environment that fosters research advancement. Leveraging Google Cloud: 2i2c utilizes Google Cloud's robust infrastructure to deliver a powerful and reliable platform for your computational needs.    ","version":"Next","tagName":"h2"},{"title":"Requesting Access to CIROH-2i2c JupyterHub​","type":1,"pageTitle":"CIROH-2i2c JupyterHub","url":"/docuhub-staging/docs/services/cloudservices/2i2c/#requesting-access-to-ciroh-2i2c-jupyterhub","content":" CIROH JupyterHub provides both CPU and GPU capabilities. To get started, please head to the Infrastructure Access page.  Infrastructure Access      note If you are participating in an event that uses the workshop environment, ask your workshop coordinator for information on logging in.    ","version":"Next","tagName":"h2"},{"title":"Requesting Software Installation on CIROH-2i2c JupyterHub​","type":1,"pageTitle":"CIROH-2i2c JupyterHub","url":"/docuhub-staging/docs/services/cloudservices/2i2c/#requesting-software-installation-on-ciroh-2i2c-jupyterhub","content":" Before making a request, please refer to the Dockerfile for the list of software currently deployed on CIROH JupyterHub.  If your software in not listed in this file, please submit the form below to request new software installation on 2i2c JupyterHub.   JupyterHub (2i2c) Software Install Form    ","version":"Next","tagName":"h2"},{"title":"CIROH-2i2c JupyterHub Environments​","type":1,"pageTitle":"CIROH-2i2c JupyterHub","url":"/docuhub-staging/docs/services/cloudservices/2i2c/#ciroh-2i2c-jupyterhub-environments","content":" Click on the buttons below to access the CIROH-2i2c JupyterHub environments.  Note that the workshop environment is only active for the duration of the conferences and programs it supports.     CIROH Production JupyterHub   CIROH Staging JupyterHub   CIROH Workshop JupyterHub      Please remember to stop the server when you're not actively using it.      ","version":"Next","tagName":"h2"},{"title":"Server Options​","type":1,"pageTitle":"CIROH-2i2c JupyterHub","url":"/docuhub-staging/docs/services/cloudservices/2i2c/#server-options","content":" Small - 5GB RAM, 2 CPUs Medium - 11GB RAM, 4 CPUs Large - 24GB RAM, 8 CPUs Huge - 52GB RAM, 16 CPUs  note The per user storage quota is currently set to 250 GB.  ","version":"Next","tagName":"h3"},{"title":"Software currently deployed on CIROH JupyterHub​","type":1,"pageTitle":"CIROH-2i2c JupyterHub","url":"/docuhub-staging/docs/services/cloudservices/2i2c/#software-currently-deployed-on-ciroh-jupyterhub","content":" Please refer to the Dockerfile for the list of software currently deployed on CIROH JupyterHub.  ","version":"Next","tagName":"h3"},{"title":"Cost of Use​","type":1,"pageTitle":"CIROH-2i2c JupyterHub","url":"/docuhub-staging/docs/services/cloudservices/2i2c/#cost-of-use","content":" CIROH 2i2c JupyterHub is free to use for consortium members. Its cost is covered by CIROH Infrastructure project funds.    ","version":"Next","tagName":"h3"},{"title":"Documentation and Resources​","type":1,"pageTitle":"CIROH-2i2c JupyterHub","url":"/docuhub-staging/docs/services/cloudservices/2i2c/#documentation-and-resources","content":" These external websites provide general guidance for getting the most out of 2i2c.  2i2c JupyterHub Documentation (If you're not sure what you're looking for, start here!)2i2c Infrastructure Documentation2i2c Homepage2i2c BlogGitHub template for CIROH 2i2c images  Alternatively, check out DocuHub's specialized documentation below for information specific to CIROH's 2i2c deployment!    📄️ JupyterHub User Directory Learn to navigate the 2i2c JupyterHub file system. 📄️ HydroShare Integration Easily launch and execute notebooks from HydroShare. 📄️ Manage files in GCP bucket Take advantage of Google Cloud Bucket storage. 📄️ Push and Pull to GitHub Push to your GitHub repositories from the cloud. 📄️ Server Image Configurations Explore CIROH's available image configurations. 📄️ Request Custom Images Request JupyterHub server images tailored to your needs. 📄️ Persistent Conda Environment Maintain your Conda environments between sessions. 📄️ Prevent Server Timeout Keep your server active to sustain long-running tasks. 📄️ Python Package Version Conflicts Debug version conflicts between your Python packages.    ","version":"Next","tagName":"h2"},{"title":"Help and Support​","type":1,"pageTitle":"CIROH-2i2c JupyterHub","url":"/docuhub-staging/docs/services/cloudservices/2i2c/#help-and-support","content":" Email UA's CIROH Cloud Team: ciroh-it-admin@ua.eduMessage the CIROH Cloud Slack Channel: #ciroh-ua-it-admin ","version":"Next","tagName":"h2"},{"title":"Persistent Conda Environments on CIROH 2i2c Server","type":0,"sectionRef":"#","url":"/docuhub-staging/docs/services/cloudservices/2i2c/conda","content":"","keywords":"","version":"Next"},{"title":"1. Create a directory for Conda environments:​","type":1,"pageTitle":"Persistent Conda Environments on CIROH 2i2c Server","url":"/docuhub-staging/docs/services/cloudservices/2i2c/conda#1-create-a-directory-for-conda-environments","content":" You can set up a directory within your home folder to store all your Conda environments. This prevents them from being removed when the server is restarted. For example:  mkdir -p ~/conda_envs   ","version":"Next","tagName":"h3"},{"title":"2. Create a new environment in that directory:​","type":1,"pageTitle":"Persistent Conda Environments on CIROH 2i2c Server","url":"/docuhub-staging/docs/services/cloudservices/2i2c/conda#2-create-a-new-environment-in-that-directory","content":" Use the --prefix option with conda create to specify the location where you want to create your environment. For example, to create an environment called my_env in ~/conda_envs:  conda create --prefix ~/conda_envs/my_env python=3.9   ","version":"Next","tagName":"h3"},{"title":"3. Activate the environment:​","type":1,"pageTitle":"Persistent Conda Environments on CIROH 2i2c Server","url":"/docuhub-staging/docs/services/cloudservices/2i2c/conda#3-activate-the-environment","content":" You can activate the environment as usual, using the path to where you created it:  conda activate ~/conda_envs/my_env   ","version":"Next","tagName":"h3"},{"title":"4. Automatically activate the environment on restart:​","type":1,"pageTitle":"Persistent Conda Environments on CIROH 2i2c Server","url":"/docuhub-staging/docs/services/cloudservices/2i2c/conda#4-automatically-activate-the-environment-on-restart","content":" If you want this environment to be activated every time you log in or the server restarts, you can add the following to your .bashrc or .bash_profile file:  conda activate ~/conda_envs/my_env   By creating your environments in your home folder (e.g., ~/conda_envs/), they will persist across server restarts, ensuring that you don't have to recreate them every time. ","version":"Next","tagName":"h3"},{"title":"A Step-by-Step Guide: Requesting custom images","type":0,"sectionRef":"#","url":"/docuhub-staging/docs/services/cloudservices/2i2c/custom-images","content":"","keywords":"","version":"Next"},{"title":"1. Create an environment.yml file:​","type":1,"pageTitle":"A Step-by-Step Guide: Requesting custom images","url":"/docuhub-staging/docs/services/cloudservices/2i2c/custom-images#1-create-an-environmentyml-file","content":" Open your terminal or command prompt. Make sure you have conda installed and activated in the environment that contains the packages you want to use for creating custom images. Learn more here. Run the following command, replacing ENVNAME with the actual name of your environment.  conda env export -n ENVNAME &gt; environment.yml   ","version":"Next","tagName":"h3"},{"title":"2. Submit a Request Form:​","type":1,"pageTitle":"A Step-by-Step Guide: Requesting custom images","url":"/docuhub-staging/docs/services/cloudservices/2i2c/custom-images#2-submit-a-request-form","content":" Click on the link below to access the Jupyterhub (2i2c) Software Install form.Select Install Software on CIROH 2i2c JupyterHub as a reason for request.Fill out remaining sections of the form and submit it.   JupyterHub (2i2c) Software Install Form  ","version":"Next","tagName":"h3"},{"title":"3. Share your environment.yml file with CIROH-IT support​","type":1,"pageTitle":"A Step-by-Step Guide: Requesting custom images","url":"/docuhub-staging/docs/services/cloudservices/2i2c/custom-images#3-share-your-environmentyml-file-with-ciroh-it-support","content":" After submitting the request form, attach the environment.yml file you created in step 1 to an email and send it to ciroh-it-support@ua.edu ","version":"Next","tagName":"h3"},{"title":"JupyterHub User Directory","type":0,"sectionRef":"#","url":"/docuhub-staging/docs/services/cloudservices/2i2c/directory","content":"","keywords":"","version":"Next"},{"title":"1. /home/jovyan​","type":1,"pageTitle":"JupyterHub User Directory","url":"/docuhub-staging/docs/services/cloudservices/2i2c/directory#1-homejovyan","content":" This is your home directory and is same for all JupyterHub users. Only you can access files in your home directory. Any files you place in your home directory persists between sessions. It is recommended to use only for notebooks and code since it is not suitable for large datasets.  ","version":"Next","tagName":"h3"},{"title":"2. /home/jovyan/shared​","type":1,"pageTitle":"JupyterHub User Directory","url":"/docuhub-staging/docs/services/cloudservices/2i2c/directory#2-homejovyanshared","content":" This is the shared readonly directory. All users can access and read from the shared directory. Only the hub admins can add and delete data from this directory.  ","version":"Next","tagName":"h3"},{"title":"3. /tmp​","type":1,"pageTitle":"JupyterHub User Directory","url":"/docuhub-staging/docs/services/cloudservices/2i2c/directory#3-tmp","content":" This is a non persistient directory. This means any files you add under /tmp direcotry will be deleted once you log out. This directory can be used to store data temporary data. ","version":"Next","tagName":"h3"},{"title":"A Step-by-Step Guide: Push Code to GitHub from 2i2c JupyterHub","type":0,"sectionRef":"#","url":"/docuhub-staging/docs/services/cloudservices/2i2c/github-push","content":"","keywords":"","version":"Next"},{"title":"You can also follow along with this video tutorial that walks you through the same process visually.​","type":1,"pageTitle":"A Step-by-Step Guide: Push Code to GitHub from 2i2c JupyterHub","url":"/docuhub-staging/docs/services/cloudservices/2i2c/github-push#you-can-also-follow-along-with-this-video-tutorial-that-walks-you-through-the-same-process-visually","content":"    ","version":"Next","tagName":"h3"},{"title":"Manage files in GCP bucket","type":0,"sectionRef":"#","url":"/docuhub-staging/docs/services/cloudservices/2i2c/gcp-object-storage","content":"","keywords":"","version":"Next"},{"title":"1. Overview​","type":1,"pageTitle":"Manage files in GCP bucket","url":"/docuhub-staging/docs/services/cloudservices/2i2c/gcp-object-storage#1-overview","content":" CIROH JupyterHub uses object Google Cloud Storage to store data in buckets (containers for objects). Currently, there are two buckets available to use on CIROH JupyterHub.  Scratch Bucket: It is intended for storing temporary files since any files in scratch bucket get deleted after seven days. Open a terminal in CIROH JupyterHub and run this command to display your scratch bucket name:  echo $SCRATCH_BUCKET gs://awi-ciroh-scratch/&lt;username&gt;   Note: In the above command output, the name of the bucket is 'awi-ciroh-scratch' and &lt;username&gt; is the folder in the bucket.  Persistent Bucket: It is recommended to use for storing files that you will be using for a longer period of time. Open a terminal in CIROH JupyterHub and run this command to display your persistent bucket name:  echo $PERSISTENT_BUCKET gs://awi-ciroh-persistent/&lt;username&gt;   ","version":"Next","tagName":"h3"},{"title":"2. Copying file to a bucket​","type":1,"pageTitle":"Manage files in GCP bucket","url":"/docuhub-staging/docs/services/cloudservices/2i2c/gcp-object-storage#2-copying-file-to-a-bucket","content":" You can copy files on your CIROH JupyterHub to an available bucket using the following command.  gcloud storage cp &lt;filepath&gt; $PERSISTENT_BUCKET/&lt;filepath&gt;   ","version":"Next","tagName":"h3"},{"title":"3. Copying file from a bucket to CIROH JupyterHub​","type":1,"pageTitle":"Manage files in GCP bucket","url":"/docuhub-staging/docs/services/cloudservices/2i2c/gcp-object-storage#3-copying-file-from-a-bucket-to-ciroh-jupyterhub","content":" You can copy files from an accessible bucket to your CIROH JupyterHub using the following command.  gcloud storage cp $PERSISTENT_BUCKET/&lt;filepath&gt; &lt;destination-filepath&gt;   ","version":"Next","tagName":"h3"},{"title":"4. Listing files in a bucket​","type":1,"pageTitle":"Manage files in GCP bucket","url":"/docuhub-staging/docs/services/cloudservices/2i2c/gcp-object-storage#4-listing-files-in-a-bucket","content":" You can list all files/folder in a bucket using the following command.  gcloud storage ls $PERSISTENT_BUCKET   Note: The above command will list all files/folders in the folder &lt;username&gt;. It won't list files in the sub-folders of folder &lt;username&gt;. To list all files including the files in the sub-folders of the root folder &lt;username&gt;, use the following command.  gcloud storage ls --recursive $PERSISTENT_BUCKET   ","version":"Next","tagName":"h3"},{"title":"5. Deleting file from a bucket​","type":1,"pageTitle":"Manage files in GCP bucket","url":"/docuhub-staging/docs/services/cloudservices/2i2c/gcp-object-storage#5-deleting-file-from-a-bucket","content":" You can delete a file in a bucket with the following command:  gcloud storage rm $PERSISTENT_BUCKET/&lt;filepath&gt;   ","version":"Next","tagName":"h3"},{"title":"6. User permssions on buckets​","type":1,"pageTitle":"Manage files in GCP bucket","url":"/docuhub-staging/docs/services/cloudservices/2i2c/gcp-object-storage#6-user-permssions-on-buckets","content":" All users have read/write permissions on both the scratch and persistent buckets.  note Anyone can access each other's files in buckets on the hub. Please be careful not to delete other user's files. Using the enviornment variables ($SCRATCH_BUCKET &amp; $PERSISTENT_BUCKET) to access buckets in commands would prevent accidententally deleting any other user's files. Your actions impact the entire organization's storage. If unsure, consult with the team lead or ciroh IT support.  ","version":"Next","tagName":"h3"},{"title":"7. Accessing buckets in Python​","type":1,"pageTitle":"Manage files in GCP bucket","url":"/docuhub-staging/docs/services/cloudservices/2i2c/gcp-object-storage#7-accessing-buckets-in-python","content":" You can find information on how to access buckets in Python code, here.  ","version":"Next","tagName":"h3"},{"title":"Where to go for help:​","type":1,"pageTitle":"Manage files in GCP bucket","url":"/docuhub-staging/docs/services/cloudservices/2i2c/gcp-object-storage#where-to-go-for-help","content":" Email ciroh-it-admin@ua.edu UA CIROH Cloud TeamCIROH Cloud Slack Channel - #ciroh-ua-it-adminCIROH Infrastructure Support Slack Channel - #ciroh-infrastructure-support ","version":"Next","tagName":"h2"},{"title":"CIROH-2i2c JupyterHub: HydroShare Integration","type":0,"sectionRef":"#","url":"/docuhub-staging/docs/services/cloudservices/2i2c/hydroshare-integration","content":"","keywords":"","version":"Next"},{"title":"Getting started​","type":1,"pageTitle":"CIROH-2i2c JupyterHub: HydroShare Integration","url":"/docuhub-staging/docs/services/cloudservices/2i2c/hydroshare-integration#getting-started","content":"   If you have not already, request CPU access to the CIROH-2i2c JupyterHub. CIROH Jupyterhub is a CUAHSI approved app. As such, it appears on the &quot;Open with&quot; list on any HydroShare resource that you have access to. In CIROH JupyterHub, click on the &quot;Login to continue&quot; button. You may be prompted to sign in to your GitHub account and authorize access if this is your first time using CIROH JupyterHub.    You will then be directed to the Server Options page. Select one of the server options that is appropriate for the analysis you need to run (small, medium, large, or huge), as well as the &quot;New Pangeo Notebook&quot; base image or NextGen National Water Model (NWM). Then, click the “Start” button at the bottom to launch the JupyterHub. You will now be inside the CIROH JupyterHub. All of the files from your HydroShare resource will appear in the file browser on the left, including any notebooks that were in your resource. Double click on a notebook to open it and then run it.  ","version":"Next","tagName":"h2"},{"title":"Using HydroShare resources in JupyterHub​","type":1,"pageTitle":"CIROH-2i2c JupyterHub: HydroShare Integration","url":"/docuhub-staging/docs/services/cloudservices/2i2c/hydroshare-integration#using-hydroshare-resources-in-jupyterhub","content":" The Open With command will copy files from the HydroShare resource into the Downloads directory in your CIROH JupyterHub user account in a folder named with the HydroShare resource unique identifier (GUID). Your work will use and add to these local versions of files and will not affect the original files in the HydroShare resource. Once your work is complete, you should save results you want to retain, either by downloading them or saving them to the HydroShare repository.  info For information about how to save files back to HydroShare, see the sections below on the hsfiles_jupyter integration, which is available by right clicking on files in the JupyterHub table of contents, or the hsclient Python library.  There are a number of HydroShare resources developed specifically to use this CIROH HydroShare integration:  Working with HydroShare, AORC data, HydroFabric and NextGen on CIROH JupyterHub Tutorial: This HydroShare resource provides a tutorial on the use of the CIROH JupyterHub linked from HydroShare. It introduces use of Jupyter Notebooks for retrieval of NOAA Analysis of Record for Calibration (AORC) datasets and setting up and executing NextGen for a small test watershed as a starting point for research with NextGen. Citation info Tarboton, D., H. Salehabadi, A. Nassar, F. Baig, A. M. Castronova, I. Garousi-Nejad, A. Patel, P. Dash (2025). Working with HydroShare, AORC data, HydroFabric and NextGen on CIROH JupyterHub Tutorial, HydroShare, https://doi.org/10.4211/hs.fc8539358fe64ca6a47468728a0687a1 Jupyter Notebooks for the Retrieval of AORC Data for Hydrologic Analysis: This HydroShare resource contains Jupyter Notebooks with instructions and code for accessing and subsetting the NOAA Analysis of Record for Calibration (AORC) dataset. Citation info Salehabadi, H., D. Tarboton, A. Nassar, A. M. Castronova, P. Dash, A. Patel, F. Baig (2025). Jupyter Notebooks for the Retrieval of AORC Data for Hydrologic Analysis, HydroShare, http://www.hydroshare.org/resource/72ea9726187e43d7b50a624f2acf591f  info NextGen and its preprocessor tools are pre-installed in the NextGen National Water Model (NWM) image in CIROH JupyterHub on 2i2c. This provides the capability for them to be run from JupyterHub without users needing to install software or configure dependencies. For more information, see the Server Image Configurations page.  ","version":"Next","tagName":"h2"},{"title":"hsfiles_jupyter: HydroShare File Manager for Jupyter​","type":1,"pageTitle":"CIROH-2i2c JupyterHub: HydroShare Integration","url":"/docuhub-staging/docs/services/cloudservices/2i2c/hydroshare-integration#hsfiles_jupyter-hydroshare-file-manager-for-jupyter","content":" hsfiles_jupyter (HydroShare File Manager for Jupyter) is an extension that enables seamless integration between HydroShare and Jupyter environments that are accessible through HydroShare’s “Open with” functionality, such as the CIROH 2i2c JupyterHub. It allows users to easily manage files between the HydroShare repository and Jupyter environments directly through right-click context menu options in the JupyterLab file browser, eliminating the need to manually transfer files between platforms.    The following are five file management options provided by the hsfiles_jupyter extension. The HydroShare-specific file menu options are only available when you are inside a HydroShare resource folder in the JupyterLab file browser, where the resource was downloaded using HydroShare’s “Open with” functionality.  ","version":"Next","tagName":"h2"},{"title":"Upload File to HydroShare​","type":1,"pageTitle":"CIROH-2i2c JupyterHub: HydroShare Integration","url":"/docuhub-staging/docs/services/cloudservices/2i2c/hydroshare-integration#upload-file-to-hydroshare","content":" You can use this option to upload local files from your JupyterLab workplace to your HydroShare resource. This allows you to add new files or update existing files in your HydroShare resource without leaving the JupyterHub environment. If that file already exists in HydroShare it would prompt you to overwrite. If the file upload is successful, you should see a message accordingly.  ","version":"Next","tagName":"h3"},{"title":"Replace with File from HydroShare​","type":1,"pageTitle":"CIROH-2i2c JupyterHub: HydroShare Integration","url":"/docuhub-staging/docs/services/cloudservices/2i2c/hydroshare-integration#replace-with-file-from-hydroshare","content":" You can use this option to overwrite a local version of the resource file by fetching the same file from HydroShare. This works when a matching file is in HydroShare resource, otherwise, you would see an error message accordingly.  ","version":"Next","tagName":"h3"},{"title":"Delete File in HydroShare​","type":1,"pageTitle":"CIROH-2i2c JupyterHub: HydroShare Integration","url":"/docuhub-staging/docs/services/cloudservices/2i2c/hydroshare-integration#delete-file-in-hydroshare","content":" Use this option to delete files from your HydroShare resource directly from the JupyterHub environment. It deletes the files on HydroShare only and not the local copy. This provides a convenient way to manage and clean up files in your HydroShare resource.  ","version":"Next","tagName":"h3"},{"title":"Check Status of File in HydroShare​","type":1,"pageTitle":"CIROH-2i2c JupyterHub: HydroShare Integration","url":"/docuhub-staging/docs/services/cloudservices/2i2c/hydroshare-integration#check-status-of-file-in-hydroshare","content":" Use this option to check if a matching file is in your HydroShare resource and if they are identical or not. This helps you identify which files have been modified locally and need synchronization.  ","version":"Next","tagName":"h3"},{"title":"Download from HydroShare​","type":1,"pageTitle":"CIROH-2i2c JupyterHub: HydroShare Integration","url":"/docuhub-staging/docs/services/cloudservices/2i2c/hydroshare-integration#download-from-hydroshare","content":" Use this option to download a specific file from your current hydroshare resource (the one you are browsing in the Jupyter file browser) into your local JupyterLab workplace. Once you navigate to a resource’s /data/contents/ path in the Jupyter file browser, right-click anywhere in the file listing area to see the option “Download from HydroShare”. Selecting this option opens a dialog box where you can select from a list of files that exist in HydroShare but are not available in JupyterLab. If all files for the resource already exist locally on JupyterLab, a message will be displayed confirming this when you select “Download from HydroShare”. If you see a list of files, select one file and then click Download. The selected file will be downloaded. Currently, you can only download one file at a time.  ","version":"Next","tagName":"h3"},{"title":"hsclient​","type":1,"pageTitle":"CIROH-2i2c JupyterHub: HydroShare Integration","url":"/docuhub-staging/docs/services/cloudservices/2i2c/hydroshare-integration#hsclient","content":" Another effective way to transfer files to and from the HydroShare repository is by using the HydroShare Python Client Library (hsclient). This Python-based interface facilitates communication between HydroShare and external programs or systems. It enables users to automate nearly all functions available through HydroShare’s web interface, making it a powerful tool for integrating HydroShare into data workflows and research applications. To learn more and get started, visit the resource here: https://www.hydroshare.org/resource/7561aa12fd824ebb8edbee05af19b910/  ","version":"Next","tagName":"h2"},{"title":"Video Tutorial​","type":1,"pageTitle":"CIROH-2i2c JupyterHub: HydroShare Integration","url":"/docuhub-staging/docs/services/cloudservices/2i2c/hydroshare-integration#video-tutorial","content":" Watch this video to find out how to access CIROH Jupyterhub and how to launch and use CIROH Jupyterhub with HydroShare:    info To join a group and gain access to the CIROH HydroShare community, go to HydroShare Communities.::: ","version":"Next","tagName":"h2"},{"title":"Preventing JupyterHub Server Timeout for Long-Running Jobs","type":0,"sectionRef":"#","url":"/docuhub-staging/docs/services/cloudservices/2i2c/server-timeout","content":"","keywords":"","version":"Next"},{"title":"1. Jupyter Keepalive Extension​","type":1,"pageTitle":"Preventing JupyterHub Server Timeout for Long-Running Jobs","url":"/docuhub-staging/docs/services/cloudservices/2i2c/server-timeout#1-jupyter-keepalive-extension","content":" The Jupyter Keepalive extension provides an easy way to control your server's active time.  To install the extension, open a terminal and run:  pip install jupyter-keepalive   Use the JupyterLab Command Palette (Command+Shift+C on Mac or Control+Shift+C on Linux/Windows) to select the &quot;Keep server alive while idle&quot; option.Once your task is complete, it's crucial that you then use the Command Palette to select the &quot;Stop keeping server alive&quot; option. This will ensure that the server is no longer being kept active unnecessarily.    ","version":"Next","tagName":"h3"},{"title":"2. Using time.sleep() (Alternate Method)​","type":1,"pageTitle":"Preventing JupyterHub Server Timeout for Long-Running Jobs","url":"/docuhub-staging/docs/services/cloudservices/2i2c/server-timeout#2-using-timesleep-alternate-method","content":" As an alternative, you can create a separate notebook with the following code and run the cell before starting your job.  import time time.sleep(24 * 60 * 60) # Sleeps for 24 hours  ","version":"Next","tagName":"h3"},{"title":"Python Package Version Conflicts","type":0,"sectionRef":"#","url":"/docuhub-staging/docs/services/cloudservices/2i2c/python-package-conflicts","content":"","keywords":"","version":"Next"},{"title":"Overview:​","type":1,"pageTitle":"Python Package Version Conflicts","url":"/docuhub-staging/docs/services/cloudservices/2i2c/python-package-conflicts#overview","content":" When different versions of Python packages are installed in your user home directory than the version that was already installed in the JupyterHub image, these local installations take precedence, potentially causing version mismatches and unexpected behavior.  ","version":"Next","tagName":"h3"},{"title":"Troubleshooting guide:​","type":1,"pageTitle":"Python Package Version Conflicts","url":"/docuhub-staging/docs/services/cloudservices/2i2c/python-package-conflicts#troubleshooting-guide","content":" Step 1: Identify Package Conflicts​  ls ~/.local/lib/pythonX.Y/site-packages/   Note: Replace X.Y with your Python version (e.g., 3.10). Find your version with python --version.  Step 2: Remove conflicting packages from user home directory​  Clear all locally installed Python packages:  rm -rf ~/.local/lib/pythonX.Y   Note: This will remove ALL Python packages installed in your home directory and ensure that only system or environment packages are used.  Step 3: Verify Your Environment​  Verify that you are using the correct JupyterHub image by checking the JUPYTER_IMAGE environment variable:  echo $JUPYTER_IMAGE   Reinstall needed packages properly  pip install &lt;package-name&gt;  ","version":"Next","tagName":"h3"},{"title":"CIROH AWS Account","type":0,"sectionRef":"#","url":"/docuhub-staging/docs/services/cloudservices/aws/","content":"","keywords":"","version":"Next"},{"title":"Unleashing Research Potential with AWS Cloud Services​","type":1,"pageTitle":"CIROH AWS Account","url":"/docuhub-staging/docs/services/cloudservices/aws/#unleashing-research-potential-with-aws-cloud-services","content":" Here's how AWS empowers your research:  Enhanced Data Accessibility and Analysis: AWS provides scalable storage and computing resources, allowing researchers to readily access, analyze, and manipulate vast datasets efficiently. Specialized Solutions at Your Fingertips: The AWS Marketplace offers a wealth of pre-built solutions and tools specifically designed for hydrological research. This eliminates the need for time-consuming development and allows researchers to focus on scientific discovery. World-Class IT Infrastructure for Research Excellence: AWS offers a robust and secure cloud infrastructure that delivers the best possible IT foundation for your research projects. This translates to increased efficiency, reduced costs, and faster time-to-results. Accelerated Research Timelines: By leveraging the on-demand scalability and elasticity of AWS, researchers can dynamically scale their computing resources to meet the specific needs of their projects. This translates to faster analysis and completion of research endeavors.    ","version":"Next","tagName":"h2"},{"title":"Requesting CIROH AWS Accounts​","type":1,"pageTitle":"CIROH AWS Account","url":"/docuhub-staging/docs/services/cloudservices/aws/#requesting-ciroh-aws-accounts","content":" CIROH Cloud Hosting services include:  Creation of AWS subaccounts for CIROH consortium members and partners.Project PI contact identity creation and access (AWS IAM)  CIROH Consortium members or partners are responsible for:  Management of CIROH subaccounts assigned to them.Project-specific software and environment configuration.Handling account creation and/or access for project contacts To get started, please head to the &quot;Accessing Public Cloud Services&quot; section of the Infrastructure Access page.  Infrastructure Access  note Please refer to this link for references to submitted forms.    ","version":"Next","tagName":"h2"},{"title":"Cost of Use​","type":1,"pageTitle":"CIROH AWS Account","url":"/docuhub-staging/docs/services/cloudservices/aws/#cost-of-use","content":" Individual projects are allotted $500 monthly for use of AWS and Google Cloud services.CIROH projects that anticipate exceeding the monthly budget for cloud services may request additional funds via the form below.  Exceeding Budget Request Form    ","version":"Next","tagName":"h2"},{"title":"AWS News Blog​","type":1,"pageTitle":"CIROH AWS Account","url":"/docuhub-staging/docs/services/cloudservices/aws/#aws-news-blog","content":" Stay up-to-date on the latest AWS news and announcements by visiting the official AWS News Blog:   AWS News Blog    ","version":"Next","tagName":"h2"},{"title":"Help and Support​","type":1,"pageTitle":"CIROH AWS Account","url":"/docuhub-staging/docs/services/cloudservices/aws/#help-and-support","content":" Email UA's CIROH Cloud Team: ciroh-it-admin@ua.eduMessage the CIROH Cloud Slack Channel: #ciroh-ua-it-adminMessage the CIROH AWS Support Slack Channel: #aws-ciroh-support      📄️ CIROH AWS Office Hours CIROH AWS Office Hours 🗃️ Documentation and Tutorial 4 items ","version":"Next","tagName":"h2"},{"title":"Documentation and Tutorial","type":0,"sectionRef":"#","url":"/docuhub-staging/docs/services/cloudservices/aws/documentation/","content":"Documentation and Tutorial 📄️ AWS Best Practices As the main account administrator for CIROH subaccount, here are some best practices to follow within your subaccount: 📄️ Setting Up OIDC for GitHub Actions with AWS Secure GitHub Actions authentication using OpenID Connect 📄️ Tag Resources on AWS AWS tags for cost tracking 📄️ AWS Data Science Tools AWS Data Science Tools","keywords":"","version":"Next"},{"title":"AWS Data Science Tools","type":0,"sectionRef":"#","url":"/docuhub-staging/docs/services/cloudservices/aws/documentation/data-science-tools/","content":"AWS Data Science Tools A collection of Amazon Web Services (AWS) scripts supporting Water Data Science. This repository can serve as a resoruces for those looking to connect and leverage the power of AWS products, specifically AWS S3 storage. info AWS Data Science Tools","keywords":"","version":"Next"},{"title":"How to Tag Resources on AWS","type":0,"sectionRef":"#","url":"/docuhub-staging/docs/services/cloudservices/aws/documentation/tagging/","content":"Tags in AWS are essential for organizing resources based on their purpose, owner, or environment, and can also aid in cost tracking when unique key-value pairs are assigned. How to Tag Resources on AWS Using AWS Console: Navigate to the desired resource, such as an EC2 instance, and follow these steps: Select the instance from the list view.Go to the Tags tab and click on the Manage tags button.Add a new tag with a unique Key and Value. Save the changes. Using AWS CLI: Use the following command-line example to create a tag for an EC2 instance: aws ec2 create-tags \\ --resources i-1234567890abcdef0 \\ --tags Key=webserver,Value=dev For each project, tag all its resources with: Project=project_name (e.g., ciroh-hydroshare, ciroh-fim) Double-check the tag name with the AWS main account admin to make sure it fits well with our naming scheme.","keywords":"","version":"Next"},{"title":"AWS Best Practices for CIROH AWS Users","type":0,"sectionRef":"#","url":"/docuhub-staging/docs/services/cloudservices/aws/documentation/aws-best-practice/","content":"","keywords":"","version":"Next"},{"title":"Security:​","type":1,"pageTitle":"AWS Best Practices for CIROH AWS Users","url":"/docuhub-staging/docs/services/cloudservices/aws/documentation/aws-best-practice/#security","content":" MFA: Require Multi-Factor Authentication (MFA) for all subaccount users and admins to enhance account security. IAM roles for resources: Instead of individual access keys, utilize IAM roles for accessing resources within subaccounts. This simplifies access management and eliminates the need for storing long-lived credentials. Regularly review and update permissions: Regularly review and update user and role permissions within subaccounts to ensure they remain aligned with their current needs. Utilize git-secrets: git-secrets is a client tool that prohibits unwanted commits containing secret data such as API keys, passwords, and tokens. You can integrate it into your CI/CD pipelines to prevent sensitive information from being added to your GitHub repositories. For more information, refer to the AWS documentation and the git-secrets GitHub repository. Use AWS Secrets Manager: Use AWS Secrets Manager, or other secrets management solution, so you don’t have to hardcode keys in plaintext. The application or client can then retrieve secrets when needed. For more information, see What is AWS Secrets Manager?  ","version":"Next","tagName":"h2"},{"title":"Access Key Management :​","type":1,"pageTitle":"AWS Best Practices for CIROH AWS Users","url":"/docuhub-staging/docs/services/cloudservices/aws/documentation/aws-best-practice/#access-key-management-","content":" Never store your access key in plain text, in a code repository, or in code.Never check in the access key in the public repository.Disable or delete access key when no longer needed.Enable least-privilege permissions.Rotate access keys regularly, preferably every 90 days.  ","version":"Next","tagName":"h2"},{"title":"Resource Management:​","type":1,"pageTitle":"AWS Best Practices for CIROH AWS Users","url":"/docuhub-staging/docs/services/cloudservices/aws/documentation/aws-best-practice/#resource-management","content":" Tagging: Implement a consistent tagging strategy for resources in all linked accounts. This allows for better cost allocation, resource identification, and easier filtering when managing resources across multiple accounts. Follow How to tag resources on AWS. Cost allocation: Allowed limit for new subaccount is $500/project per month. Monitor the usage throughout the month and if it reaches above $500/project, notify admin of the subaccount to take necessary actions. For projects expecting more than $500 per month usage, please email ciroh-it-admin@ua.edu in advance to get the approval from higher management. Effective Sept 2024, we transitioned to a new budgeting model (for existing users) that provides your CIROH AWS subaccount with a $10,000 budget for every 6-month period and monthly max limit of $3000. This change will give you more flexibility to plan and execute your research workloads without the constraints of a monthly cap. Resource quotas: Set resource quotas for subaccounts to limit their spending and resource usage. This helps prevent accidental overspending and ensures efficient resource allocation. Monitor resource usage: Encourage subaccount admins to monitor their resource usage regularly to identify potential cost optimization opportunities. Data Locality: Always consider the location of your data when selecting a region for deploying resources. Deploying resources in the same region as your data minimizes data transfer costs and latency, leading to improved performance and cost-efficiency. Region Selection: Carefully evaluate the available AWS regions and select the one that best aligns with your data residency requirements, compliance needs, and desired performance characteristics.  EBS:  EBS Volume Management: Avoiding Unnecessary Costs: Terminate EBS Volumes with Terminated Instances: When terminating an EC2 instance, ensure that you also delete any associated EBS volumes that are no longer needed. EBS volumes incur charges even if they are not attached to a running instance. Regularly Review EBS Volume Usage: Periodically review your EBS volumes using the EC2 Management Console or AWS CLI. Identify any unattached volumes that are no longer required and delete them to avoid ongoing charges.  EFS:  Data Lifecycle Management: Evaluate your data access patterns. For infrequently accessed files, consider migrating data from Amazon EFS to Amazon S3 to leverage its cost-efficient storage classes, such as S3 Standard-IA or S3 Glacier. Tiered Storage Strategy: Implement a tiered storage strategy where frequently accessed data resides on EFS for high performance, while infrequently accessed or archival data is moved to S3 for cost-effective long-term storage.  ","version":"Next","tagName":"h2"},{"title":"Governance and Compliance:​","type":1,"pageTitle":"AWS Best Practices for CIROH AWS Users","url":"/docuhub-staging/docs/services/cloudservices/aws/documentation/aws-best-practice/#governance-and-compliance","content":" Standardized configurations: Establish and enforce standardized configurations for resources across linked accounts. This ensures consistency and simplifies management. Compliance policies: Implement compliance policies for subaccounts to ensure they adhere to relevant regulations and internal standards. Logging and auditing: Enable logging and auditing for all activities within linked accounts to track resource usage, identify potential security threats, and maintain compliance. Regular security audits: Conduct regular security audits of linked accounts to identify and address any vulnerabilities.  ","version":"Next","tagName":"h2"},{"title":"Additional Recommendations:​","type":1,"pageTitle":"AWS Best Practices for CIROH AWS Users","url":"/docuhub-staging/docs/services/cloudservices/aws/documentation/aws-best-practice/#additional-recommendations","content":" Centralized documentation: Use CIROH DocuHub (docs.ciroh.org) as a central location for documenting procedures, best practices, and resource usage guidelines for linked accounts. Training and awareness: Offer training and awareness programs to subaccount admins on secure practices, compliance requirements, and resource management best practices through CIROH AWS Office hours. Regular communication: Maintain regular communication with subaccount admins to address their concerns, answer questions, and share updates regarding policies and procedures via Slack Channel and also available through CIROH AWS Office hours.  ","version":"Next","tagName":"h2"},{"title":"Application Deployment:​","type":1,"pageTitle":"AWS Best Practices for CIROH AWS Users","url":"/docuhub-staging/docs/services/cloudservices/aws/documentation/aws-best-practice/#application-deployment","content":" Use terraforms or any Infrastructure as Code if possible for your application deployment. ","version":"Next","tagName":"h2"},{"title":"CIROH AWS Office Hours","type":0,"sectionRef":"#","url":"/docuhub-staging/docs/services/cloudservices/aws/officehours","content":"CIROH AWS Office Hours Scheduled the monthly CIROH AWS Office Hour sessions, an opportunity for discussing AWS-related inquiries with direct response from AWS experts. These sessions cover various topics, AWS services, projects, and other topics of interest. AWS technical staff is available to address the community's AWS questions and have discussions around best practices. We encourage CIROH members to participate to learn more about how to effectively leverage AWS tools and resources for their projects and share knowledge and experience with fellow CIROH members. info Email: ciroh-it-admin@ua.edu to subscribe to monthly CIROH Office Hour Sessions.","keywords":"","version":"Next"},{"title":"Cloud Services Blogs","type":0,"sectionRef":"#","url":"/docuhub-staging/docs/services/cloudservices/cloudserviceblogs","content":"","keywords":"","version":"Next"},{"title":"AWS News Blog​","type":1,"pageTitle":"Cloud Services Blogs","url":"/docuhub-staging/docs/services/cloudservices/cloudserviceblogs#aws-news-blog","content":" Stay up-to-date on the latest AWS news and announcements by visiting the official AWS News Blog: AWS News Blog    ","version":"Next","tagName":"h2"},{"title":"Google Cloud Blog​","type":1,"pageTitle":"Cloud Services Blogs","url":"/docuhub-staging/docs/services/cloudservices/cloudserviceblogs#google-cloud-blog","content":" Stay up-to-date on the latest Google Cloud news and updates by visiting the official Google Cloud Blog: Google Cloud Blog    ","version":"Next","tagName":"h2"},{"title":"2i2c Blog​","type":1,"pageTitle":"Cloud Services Blogs","url":"/docuhub-staging/docs/services/cloudservices/cloudserviceblogs#2i2c-blog","content":" Stay up-to-date on the latest 2i2c news and announcements by visiting the official 2i2c Blog: 2i2c Blog ","version":"Next","tagName":"h2"},{"title":"CUAHSI JupyterHub","type":0,"sectionRef":"#","url":"/docuhub-staging/docs/services/cloudservices/cuahsi/","content":"","keywords":"","version":"Next"},{"title":"An Introduction​","type":1,"pageTitle":"CUAHSI JupyterHub","url":"/docuhub-staging/docs/services/cloudservices/cuahsi/#an-introduction","content":" The CUAHSI JupyterHub is a free cloud computing environment that enables researchers to execute scientific code and explore, modify, and interact with data inside a remote execution environment using Python and/or R programming languages. It is integrated with HydroShare and the Hydrologic Information System data repositories, making it easy to leverage community datasets, collaborate, and disseminate research workflows.  NOTE: Below content is taken from https://github.com/CUAHSI/jupyterhub/blob/main/docs/getting-started.md  Getting Started    ","version":"Next","tagName":"h2"},{"title":"Access​","type":1,"pageTitle":"CUAHSI JupyterHub","url":"/docuhub-staging/docs/services/cloudservices/cuahsi/#access","content":" To access the CUAHSI JupyterHub platform, you must be a member of the CUAHSI JupyterHub Group. Group membership limits system interruptions and ensures that resources are effectively curated and managed. When first accessing the application, you will be directed to the CUAHSI JupyterHub Group landing page. Request to join the group, and after admission has been granted you will be able to access the computational environment. To expedite the approval process, please ensure that your HydroShare user profile is complete and up-to-date. Contact help@cuahsi.org if you have any questions regarding this process.  ","version":"Next","tagName":"h2"},{"title":"Launching JupyterHub​","type":1,"pageTitle":"CUAHSI JupyterHub","url":"/docuhub-staging/docs/services/cloudservices/cuahsi/#launching-jupyterhub","content":" There are multiple ways to access the CUAHSI JupyterHub platform which are listed below. All of these methods require that you register your HydroShare account with the CUAHSI JupyterHub Group (see :Access&quot; above).  ","version":"Next","tagName":"h2"},{"title":"HydroShare Web Application​","type":1,"pageTitle":"CUAHSI JupyterHub","url":"/docuhub-staging/docs/services/cloudservices/cuahsi/#hydroshare-web-application","content":" The simplest way to get started with the CUAHSI JupyterHub is by launching it directly from the HydroShare Apps library (hydroshare.org/apps) by clicking on the CUAHSI JupyterHub icon. This redirect you to the CUAHSI JupyterHub server where you will be asked to login using your HydroShare credentials. Once authenticated, you will be asked to choose a Profile Environment which will be used to launch an isolated cloud computing environment for you to work in.    In this space, you can create files and execute code from within your web browser. Any data you upload, download, and create is associated with your HydroShare account and will persist between sessions, meaning that it will be there next time you log in. Prior to gaining access, you will be asked join the CUAHSI JupyterHub HydroShare group (see the Access and Authentication section for details).  ","version":"Next","tagName":"h3"},{"title":"HydroShare Open-With Menu​","type":1,"pageTitle":"CUAHSI JupyterHub","url":"/docuhub-staging/docs/services/cloudservices/cuahsi/#hydroshare-open-with-menu","content":" Another common way of accessing the CUAHSI JupyterHub environment is by using the HydroShare Open with functionality. This button can be found in the top right corner of any HydroShare resource landing page. After selecting &quot;CUAHSI JupyterHub&quot;, a computing environment will be prepared and the content of the current HydroShare resource will be placed inside of it. This is a convenient method for executing code, data, and workflows that have been published in the HydroShare repository.    ","version":"Next","tagName":"h3"},{"title":"Direct URL​","type":1,"pageTitle":"CUAHSI JupyterHub","url":"/docuhub-staging/docs/services/cloudservices/cuahsi/#direct-url","content":" Once you are familiar with the this environment, it's often useful to access it directly rather than navigating through HydroShare. This can be done by simply navigating to https://jupyterhub.cuahsi.org. ","version":"Next","tagName":"h3"},{"title":"CIROH Google Account","type":0,"sectionRef":"#","url":"/docuhub-staging/docs/services/cloudservices/google-cloud/","content":"","keywords":"","version":"Next"},{"title":"Unleashing Research Potential with Google Cloud Services​","type":1,"pageTitle":"CIROH Google Account","url":"/docuhub-staging/docs/services/cloudservices/google-cloud/#unleashing-research-potential-with-google-cloud-services","content":" Here's some services and tools offered by Google Cloud:  Compute Engine: It is a highly scalable computing and hosting service that provides on-demand, high-performance computing resources. It lets you create and run virtual machines on Google infrastructure. Cloud Storage: Google Cloud provides fast, low-cost, highly durable archive and backup storage, allowing researchers to readily access, analyze, and manipulate vast datasets efficiently. BigQuery: Using its serverless architecture, researchers can use SQL queries to analyze huge datasets. It lets you manage all data types across clouds with fine-grained access controls. Google Earth Engine: It is a cloud-based geospatial analysis platform to analyze earth observation data. Google Cloud offers Earth Engine for remote sensing research, predicting desease outbreaks, natural resource management, and more.    ","version":"Next","tagName":"h2"},{"title":"Requesting CIROH Google Cloud Accounts​","type":1,"pageTitle":"CIROH Google Account","url":"/docuhub-staging/docs/services/cloudservices/google-cloud/#requesting-ciroh-google-cloud-accounts","content":" CIROH Cloud Hosting services include:  Creation of Google Cloud subaccounts for CIROH consortium members and partners.Project PI contact identity creation and access (Google IAM)  To get started, please head to the &quot;Accessing Public Cloud Services&quot; section of the Infrastructure Access page.  Infrastructure Access  note Please refer to this link for references to submitted forms.    ","version":"Next","tagName":"h2"},{"title":"Cost of Use​","type":1,"pageTitle":"CIROH Google Account","url":"/docuhub-staging/docs/services/cloudservices/google-cloud/#cost-of-use","content":" Individual projects are allotted $500 monthly for use of AWS and Google Cloud services.CIROH projects that anticipate exceeding the monthly budget for cloud services may request additional funds via the form below.  Exceeding Budget Request Form    ","version":"Next","tagName":"h2"},{"title":"Google Cloud Blog​","type":1,"pageTitle":"CIROH Google Account","url":"/docuhub-staging/docs/services/cloudservices/google-cloud/#google-cloud-blog","content":" Stay up-to-date on the latest Google Cloud news and announcements by visiting the official Google Cloud Blog:   Google Cloud Blog    ","version":"Next","tagName":"h2"},{"title":"Help and Support​","type":1,"pageTitle":"CIROH Google Account","url":"/docuhub-staging/docs/services/cloudservices/google-cloud/#help-and-support","content":" Email UA's CIROH Cloud Team: ciroh-it-admin@ua.eduMessage the CIROH Cloud Slack Channel: #ciroh-ua-it-admin ","version":"Next","tagName":"h2"},{"title":"Setting Up OIDC for GitHub Actions with AWS","type":0,"sectionRef":"#","url":"/docuhub-staging/docs/services/cloudservices/aws/documentation/OIDC-IAM-Integration/","content":"","keywords":"","version":"Next"},{"title":"Benefits of OIDC​","type":1,"pageTitle":"Setting Up OIDC for GitHub Actions with AWS","url":"/docuhub-staging/docs/services/cloudservices/aws/documentation/OIDC-IAM-Integration/#benefits-of-oidc","content":" No long-lived credentials: Eliminates the need to store AWS access keys in GitHub secretsAutomatic credential rotation: Temporary credentials are generated for each workflow runEnhanced security: Fine-grained control over which repositories can access AWS resourcesAudit trail: Clear CloudTrail logs showing which workflows assumed roles  ","version":"Next","tagName":"h2"},{"title":"Step 1: Create the OIDC Identity Provider​","type":1,"pageTitle":"Setting Up OIDC for GitHub Actions with AWS","url":"/docuhub-staging/docs/services/cloudservices/aws/documentation/OIDC-IAM-Integration/#step-1-create-the-oidc-identity-provider","content":" Navigate to IAM and select Identity Providers to add a new OpenID Connect provider. Enter the GitHub Actions token URL (https://token.actions.githubusercontent.com) as the provider URL and specify sts.amazonaws.com as the audience. This establishes the trust relationship between AWS and GitHub Actions for secure authentication.    After creating the identity provider, you'll see it listed in your IAM console with the provider URL token.actions.githubusercontent.com. The provider details show the ARN, creation time, and configured audience (sts.amazonaws.com) that will be used for authentication.    ","version":"Next","tagName":"h2"},{"title":"Step 2: Create IAM Role with Custom Trust Policy​","type":1,"pageTitle":"Setting Up OIDC for GitHub Actions with AWS","url":"/docuhub-staging/docs/services/cloudservices/aws/documentation/OIDC-IAM-Integration/#step-2-create-iam-role-with-custom-trust-policy","content":" When creating a new IAM role, select &quot;Custom trust policy&quot; as the trusted entity type. The trust policy defines the conditions under which GitHub Actions can assume this role, including the Federated principal (your OIDC provider ARN) and the repository filter using StringLike condition.    ","version":"Next","tagName":"h2"},{"title":"Custom Trust Policy​","type":1,"pageTitle":"Setting Up OIDC for GitHub Actions with AWS","url":"/docuhub-staging/docs/services/cloudservices/aws/documentation/OIDC-IAM-Integration/#custom-trust-policy","content":" Replace ACCOUNT_ID with your AWS account ID and ORG_NAME/REPO_NAME with your GitHub organization and repository name:  { &quot;Version&quot;: &quot;2012-10-17&quot;, &quot;Statement&quot;: [ { &quot;Effect&quot;: &quot;Allow&quot;, &quot;Principal&quot;: { &quot;Federated&quot;: &quot;arn:aws:iam::ACCOUNT_ID:oidc-provider/token.actions.githubusercontent.com&quot; }, &quot;Action&quot;: &quot;sts:AssumeRoleWithWebIdentity&quot;, &quot;Condition&quot;: { &quot;StringEquals&quot;: { &quot;token.actions.githubusercontent.com:aud&quot;: &quot;sts.amazonaws.com&quot; }, &quot;StringLike&quot;: { &quot;token.actions.githubusercontent.com:sub&quot;: &quot;repo:ORG_NAME/REPO_NAME:*&quot; } } } ] }   ","version":"Next","tagName":"h3"},{"title":"Step 3: Create Permissions Policy​","type":1,"pageTitle":"Setting Up OIDC for GitHub Actions with AWS","url":"/docuhub-staging/docs/services/cloudservices/aws/documentation/OIDC-IAM-Integration/#step-3-create-permissions-policy","content":" Create a custom IAM policy that defines the specific AWS permissions your GitHub Actions workflow needs. This policy includes permissions for Terraform state management (S3, DynamoDB), infrastructure deployment (Lambda, Step Functions, EC2), and resource monitoring (CloudWatch Logs).    The JSON policy editor allows you to specify granular permissions for each AWS service your workflow will interact with. Refer to your infrastructure requirements to determine the necessary permissions.  ","version":"Next","tagName":"h2"},{"title":"Step 4: Attach Policy to Role​","type":1,"pageTitle":"Setting Up OIDC for GitHub Actions with AWS","url":"/docuhub-staging/docs/services/cloudservices/aws/documentation/OIDC-IAM-Integration/#step-4-attach-policy-to-role","content":" In the role creation wizard, search for and select your custom policy (e.g., GitHubActions-NgenDatastream-Policy) to attach it to the role. This step links the permissions you defined earlier with the trust relationship configured in the trust policy.    Multiple policies can be attached to a single role if your workflow requires permissions from different policy documents.  ","version":"Next","tagName":"h2"},{"title":"Step 5: Name and Create the Role​","type":1,"pageTitle":"Setting Up OIDC for GitHub Actions with AWS","url":"/docuhub-staging/docs/services/cloudservices/aws/documentation/OIDC-IAM-Integration/#step-5-name-and-create-the-role","content":" Provide a descriptive name for your IAM role (e.g., github-actions-ngen-datastream-role) and review the complete trust policy configuration. The trust policy shows the OIDC provider ARN, the required audience (sts.amazonaws.com), and the repository condition that restricts access to your specific GitHub repository.    After verification, create the role to finalize the OIDC setup.  ","version":"Next","tagName":"h2"},{"title":"Step 6: Add Role ARN to GitHub Secrets​","type":1,"pageTitle":"Setting Up OIDC for GitHub Actions with AWS","url":"/docuhub-staging/docs/services/cloudservices/aws/documentation/OIDC-IAM-Integration/#step-6-add-role-arn-to-github-secrets","content":" In your GitHub repository settings, navigate to Settings → Secrets and variables → Actions and add a new repository secret. Create a secret named AWS_ROLE_ARN with the complete ARN of the IAM role you just created.    This secret will be referenced in your workflow file to authenticate with AWS using OIDC instead of long-lived access keys.  ","version":"Next","tagName":"h2"},{"title":"Step 7: Use OIDC in Your Workflow​","type":1,"pageTitle":"Setting Up OIDC for GitHub Actions with AWS","url":"/docuhub-staging/docs/services/cloudservices/aws/documentation/OIDC-IAM-Integration/#step-7-use-oidc-in-your-workflow","content":" Add the following step to your GitHub Actions workflow to authenticate with AWS using OIDC:  - name: Configure AWS Credentials (OIDC) uses: aws-actions/configure-aws-credentials@v4 with: role-to-assume: ${{ secrets.AWS_ROLE_ARN }} aws-region: ${{ env.AWS_REGION }} role-session-name: GitHubActions-TerraformCheck   ","version":"Next","tagName":"h2"},{"title":"How It Works​","type":1,"pageTitle":"Setting Up OIDC for GitHub Actions with AWS","url":"/docuhub-staging/docs/services/cloudservices/aws/documentation/OIDC-IAM-Integration/#how-it-works","content":" Initiates OIDC Authentication  This step uses the official AWS GitHub Action to authenticate with AWS using OpenID Connect instead of traditional access keys. GitHub automatically generates a short-lived JSON Web Token (JWT) that contains information about your workflow, repository, and branch.  Assumes the IAM Role  The action sends the JWT to AWS STS (Security Token Service) and requests to assume the IAM role specified in role-to-assume. AWS validates the token against your OIDC provider's trust policy, checking that the request comes from your authorized repository.  Provides Temporary Credentials  Once validated, AWS returns temporary security credentials (access key, secret key, and session token) that are valid for the duration of the workflow job. These credentials are automatically configured in the GitHub Actions environment, making them available to all subsequent steps that use the AWS CLI or SDKs.  ","version":"Next","tagName":"h3"},{"title":"Key Parameters​","type":1,"pageTitle":"Setting Up OIDC for GitHub Actions with AWS","url":"/docuhub-staging/docs/services/cloudservices/aws/documentation/OIDC-IAM-Integration/#key-parameters","content":" role-to-assume: The ARN of your IAM role stored as a GitHub secretaws-region: The AWS region for the sessionrole-session-name: A descriptive identifier for CloudTrail logging and auditing (helps you track which job assumed the role)  ","version":"Next","tagName":"h3"},{"title":"Required Workflow Permissions​","type":1,"pageTitle":"Setting Up OIDC for GitHub Actions with AWS","url":"/docuhub-staging/docs/services/cloudservices/aws/documentation/OIDC-IAM-Integration/#required-workflow-permissions","content":" Ensure your workflow job has the necessary permissions to request OIDC tokens:  permissions: id-token: write contents: read   ","version":"Next","tagName":"h2"},{"title":"Complete Workflow Example​","type":1,"pageTitle":"Setting Up OIDC for GitHub Actions with AWS","url":"/docuhub-staging/docs/services/cloudservices/aws/documentation/OIDC-IAM-Integration/#complete-workflow-example","content":" Here's a complete example of a GitHub Actions workflow using OIDC authentication:  name: Deploy Infrastructure on: workflow_dispatch: env: AWS_REGION: us-east-2 jobs: deploy: runs-on: ubuntu-latest permissions: id-token: write contents: read steps: - name: Checkout code uses: actions/checkout@v4 - name: Configure AWS Credentials (OIDC) uses: aws-actions/configure-aws-credentials@v4 with: role-to-assume: ${{ secrets.AWS_ROLE_ARN }} aws-region: ${{ env.AWS_REGION }} role-session-name: GitHubActions-Deploy - name: Verify AWS Identity run: aws sts get-caller-identity   ","version":"Next","tagName":"h2"},{"title":"Security Best Practices​","type":1,"pageTitle":"Setting Up OIDC for GitHub Actions with AWS","url":"/docuhub-staging/docs/services/cloudservices/aws/documentation/OIDC-IAM-Integration/#security-best-practices","content":" Limit repository access: Use specific repository names in the trust policy rather than wildcardsUse least privilege: Only grant the permissions your workflow actually needsSet session names: Use descriptive role-session-name values for better audit trailsMonitor role usage: Regularly review CloudTrail logs to detect unauthorized access attemptsRotate OIDC thumbprints: Keep the OIDC provider thumbprints updated if GitHub rotates their signing keys  ","version":"Next","tagName":"h2"},{"title":"Troubleshooting​","type":1,"pageTitle":"Setting Up OIDC for GitHub Actions with AWS","url":"/docuhub-staging/docs/services/cloudservices/aws/documentation/OIDC-IAM-Integration/#troubleshooting","content":" Error: &quot;Not authorized to perform sts:AssumeRoleWithWebIdentity&quot;  Verify the trust policy includes your correct repository nameEnsure the OIDC provider ARN matches in both the trust policy and your AWS account  Error: &quot;Token audience validation failed&quot;  Check that the audience in the trust policy is exactly sts.amazonaws.comVerify the OIDC provider configuration has the correct audience  Permissions denied during workflow execution  Review the IAM policy attached to the role and ensure it includes all necessary permissionsCheck for resource-specific ARN restrictions that might be blocking access ","version":"Next","tagName":"h2"},{"title":"HydroShare","type":0,"sectionRef":"#","url":"/docuhub-staging/docs/services/cloudservices/HydroShare/","content":"","keywords":"","version":"Next"},{"title":"HydroShare communities​","type":1,"pageTitle":"HydroShare","url":"/docuhub-staging/docs/services/cloudservices/HydroShare/#hydroshare-communities","content":" HydroShare communities are designed to allow broader collectives of researchers to share resources more seamlessly, fostering public data sharing and open access. A community is a set of HydroShare groups, which allows several differently administered groups to be combined to collaborate toward a common goal.  The Cooperative Institute for Research to Operations in Hydrology (CIROH) community brings together the CIROH groups in HydroShare for easier data access and sharing. Group members can share content with a group. Content shared with a group in the CIROH community, are listed on both the CIROH community page and the CIROH Portal website.  ","version":"Next","tagName":"h2"},{"title":"Hydroshare groups​","type":1,"pageTitle":"HydroShare","url":"/docuhub-staging/docs/services/cloudservices/HydroShare/#hydroshare-groups","content":" A HydroShare group is a collaborative space where users can share, organize, and discover resources related to a particular project, research group, institution, or area of interest. Any HydroShare user can create a group.  ","version":"Next","tagName":"h2"},{"title":"Join a group​","type":1,"pageTitle":"HydroShare","url":"/docuhub-staging/docs/services/cloudservices/HydroShare/#join-a-group","content":" To join a group, navigate to Collaborate → Groups, then search for the name of group and request to join. To create a group, click the green &quot;Create group&quot; button.    ","version":"Next","tagName":"h3"},{"title":"Share resources with a group​","type":1,"pageTitle":"HydroShare","url":"/docuhub-staging/docs/services/cloudservices/HydroShare/#share-resources-with-a-group","content":" When you share a resource, you can share it with specific groups so that resources appear in both group pages and in any upstream community pages, such as the CIROH Community page.  To share a resource with a group, go to the &quot;manage who has access&quot; panel on your resource and select groups, then type the name of the group to which you want to share and designate an access level. Groups cannot be owners of resources (resource ownership remains with individuals), but they serve as shared access/visibility points.    ","version":"Next","tagName":"h3"},{"title":"Mirroring resources onto CIROH Portal​","type":1,"pageTitle":"HydroShare","url":"/docuhub-staging/docs/services/cloudservices/HydroShare/#mirroring-resources-onto-ciroh-portal","content":" CIROH Portal mirrors the majority of its content from HydroShare resources. While Portal's Contribute page makes it easy to submit new Portal-compatible HydroShare resources, already-existing resources can also be added to Portal through the addition of subject keywords and additional metadata.  ","version":"Next","tagName":"h2"},{"title":"Subject keywords​","type":1,"pageTitle":"HydroShare","url":"/docuhub-staging/docs/services/cloudservices/HydroShare/#subject-keywords","content":" By adding them to the &quot;Subject Keywords&quot; field, the following subject keywords can be used to surface HydroShare resources on Portal:  nwm_portal_app: Surfaces resources on the Applications page. The page_url additional metadata field is strongly recommended, see below.ciroh_portal_data: Surfaces resources on the Datasets page.ciroh_portal_presentation: Surfaces resources on the Presentations page. The pres_path additional metadata field is strongly recommended; see below.ciroh_portal_pres_collections: Surfaces resources on the &quot;Collections&quot; tab of the Presentations page. Recommended as a way to surface HydroShare collections that contain many presentations from a single conference or event.nwm_portal_module: Surfaces resources on the Courses page. The page_url additional metadata field is strongly recommended, see below.  ","version":"Next","tagName":"h3"},{"title":"Additional metadata fields​","type":1,"pageTitle":"HydroShare","url":"/docuhub-staging/docs/services/cloudservices/HydroShare/#additional-metadata-fields","content":" By adding these to a resource's &quot;Additional Metadata&quot; section, you can manage how the resource is displayed on Portal:  thumbnail_url: A URL for the thumbnail used as a resource preview on Portal. These can include permalinks to files within the resource or to external images. If not present, the thumbnail will simply be the CIROH logo. page_url: The primary URL that Portal will link to for the resource. Strongly recommended for web apps and courses. If not present, the primary link will be to the HydroShare resource. pres_path: A path to a PDF file that will be embedded as a presentation. This path should be relative to the root of the HydroShare resource. ","version":"Next","tagName":"h3"},{"title":"External Resources","type":0,"sectionRef":"#","url":"/docuhub-staging/docs/services/external-resources/","content":"External Resources CIROH recognizes the importance of leveraging external resources to enhance our research capabilities. This section provides information on external computing resources available to CIROH consortium members. Explore options beyond our on-premise and cloud-based services to expand your research horizons. Learn about NSF Access ACCESS is a program established and funded by the National Science Foundation to help researchers and educators, with or without supporting grants, to utilize the nation’s advanced computing systems and services – at no cost. ACCESS Resources ACCESS offers computing and storage resources free of charge to researchers and educators. About ACCESS Resources","keywords":"","version":"Next"},{"title":"NSF Access","type":0,"sectionRef":"#","url":"/docuhub-staging/docs/services/external-resources/nsf-access/","content":"NSF Access ACCESS is an advanced computing and data resource program supported by the U.S. National Science Foundation (NSF). Please refer to https://allocations.access-ci.org/ for more information on how to get access to NSF ACCESS resources. note Allocations are absolutely free of cost and you do not need NSF, or funding from any agency to receive one. Here are some useful links to get started: About Access AllocationsGet your first projectSupported Resources Recommended Resources for CIROH projects: JetStream2 Anvil NSF NCAR Derecho","keywords":"","version":"Next"},{"title":"Anvil","type":0,"sectionRef":"#","url":"/docuhub-staging/docs/services/external-resources/nsf-access/anvil","content":"","keywords":"","version":"Next"},{"title":"Use cases:​","type":1,"pageTitle":"Anvil","url":"/docuhub-staging/docs/services/external-resources/nsf-access/anvil#use-cases","content":" General-Purpose CPU Power: Anvil's powerful CPUs (with 128 cores per node) are ideal for computationally intensive tasks suitable for modeling and simulation across scientific and engineering fields.Memory-Intensive Workloads: The dedicated large memory nodes (with 1TB of DDR4-3200 memory per node) works bestfor research that demands significant memory resources.Composable Subsystem: It is a private cloud built on Kubernetes and consists of bothe CPU and GPU nodes and S3 data storage. It is suitable for applications such as model inference service (via NVIDIA Triton), Specialized LLMs, dataset hosting, science gateways and web application hosting, and classroom and training applications via interactive access interfaces.  Anvil Documentation  info For a more detailed information on Anvil, visit the official NSF ACCESS website here. ","version":"Next","tagName":"h3"},{"title":"NSF NCAR Derecho","type":0,"sectionRef":"#","url":"/docuhub-staging/docs/services/external-resources/nsf-access/derecho","content":"","keywords":"","version":"Next"},{"title":"Use Cases:​","type":1,"pageTitle":"NSF NCAR Derecho","url":"/docuhub-staging/docs/services/external-resources/nsf-access/derecho#use-cases","content":" Earth System Science Research: Designed specifically for tasks related to climate modeling, weather prediction, and environmental studies.Large-Scale Simulations: Optimal for computationally demanding research in Earth sciences that requires extensive core-hours.Educational Use: Available for classroom and instructional purposes, supporting students, postdocs, and new faculty without external funding.  To learn more, visit the NCAR HPC Documentation page.  NSF NCAR HPC Documentation  info For additional information on NSF NCAR Derecho and its allocation process, refer to the NSF ACCESS Resources page. ","version":"Next","tagName":"h3"},{"title":"JetStream2","type":0,"sectionRef":"#","url":"/docuhub-staging/docs/services/external-resources/nsf-access/jetstream","content":"","keywords":"","version":"Next"},{"title":"Use cases:​","type":1,"pageTitle":"JetStream2","url":"/docuhub-staging/docs/services/external-resources/nsf-access/jetstream#use-cases","content":" Jetstream2 is ideal for researchers with diverse needs:  On-demand virtual machines: It is ideal for research that requires on demand virtual machine services. It is also best for researchers needing to create their own customized virtual machine environment for specific software needs.Always-on research infrastructure: It can host research-supporting infrastructure services that require continuous operation.Educational support: It can be used to provide virtual machines for student use in research or coursework.  For information about available instance sizes, visit the JetStream2 VM Sizes page.    ","version":"Next","tagName":"h3"},{"title":"Gaining Access to JetStream2​","type":1,"pageTitle":"JetStream2","url":"/docuhub-staging/docs/services/external-resources/nsf-access/jetstream#gaining-access-to-jetstream2","content":" CIROH offers its researchers access to an allocation on JetStream2. To get started, visit the Infrastructure Access page below.  Infrastructure Access    info For a more detailed information on JetStream2, visit the official NSF ACCESS Jetstream2 website here. ","version":"Next","tagName":"h3"},{"title":"CIROH CyberInfrastructure: Unleashing Potential in Hydrological Research","type":0,"sectionRef":"#","url":"/docuhub-staging/docs/services/intro","content":"","keywords":"","version":"Next"},{"title":"Computing infrastructure access for consortium members and partners​","type":1,"pageTitle":"CIROH CyberInfrastructure: Unleashing Potential in Hydrological Research","url":"/docuhub-staging/docs/services/intro#computing-infrastructure-access-for-consortium-members-and-partners","content":" CIROH CyberInfrastructure empowers CIROH consortium members by providing a scalable, efficient, and user-friendly computing platform. We understand the challenges researchers face in managing computational resources, and CIROH CyberInfrastructure alleviates these burdens by offering a suite of pre-configured environments and resources. Our team of engineers and developers meticulously optimizes both cloud-based (AWS and Google Cloud) and on-premise infrastructure (Pantarhei HPC cluster) to ensure unparalleled flexibility and scalability.  This translates into a powerful platform that includes:  Simplified Access: CIROH CyberInfrastructure streamlines access to computational environments, eliminating the need for time-consuming installations and maintenance. Unmatched Flexibility: Our multi-cloud and on-premise infrastructure provides a diverse range of options to suit your specific research needs. Scalable Resources: CIROH CyberInfrastrucure readily scales to accommodate your growing data analysis and computational demands. Pre-Installed Software: Leverage pre-installed hydrological software packages to jumpstart your research endeavors. Streamlined Development: Benefit from the secure and rapid application development and deployment capabilities offered by Cloud.  CIROH CyberInfrastructure is meticulously designed to empower CIROH researchers and innovators to achieve groundbreaking results in hydrology. Join us and unlock the full potential of your research today!  Get started        ","version":"Next","tagName":"h2"},{"title":"Definition and Scope of CyberInfrastructure​","type":1,"pageTitle":"CIROH CyberInfrastructure: Unleashing Potential in Hydrological Research","url":"/docuhub-staging/docs/services/intro#definition-and-scope-of-cyberinfrastructure","content":" The CIROH CyberInfrastructure supports a broad ecosystem of research, development, and integration activities. Its primary scopes are to:  Support community development, system integration, and research cyberinfrastructure for a large consortium of members and partners.Assist in integrating research products from consortium members into a unified presentation for sponsors to evaluate for potential operational applications.Conduct independent research, particularly in data science, machine learning, computational and algorithmic optimization, and visualization.    ","version":"Next","tagName":"h2"},{"title":"CIROH CyberInfrastructure Goals​","type":1,"pageTitle":"CIROH CyberInfrastructure: Unleashing Potential in Hydrological Research","url":"/docuhub-staging/docs/services/intro#ciroh-cyberinfrastructure-goals","content":" Promote reproducible hydrologic computing experiments with the NextGen Water Resource Modeling Framework.Provide support and reduce the barrier to entry for performing NextGen-related experiments at various scales.Accelerate the interconnection and integration of research products and hydroinformatics innovations from the various ongoing CIROH experiments.Provide a flexible, scalable, and secure infrastructure for researchers to perform computational tasks, store, and analyze data.Enable researchers to run simulations, analyze large datasets, and collaborate across different locations through cloud and on-premise systems.Advance hydrological modeling through modern DevOps practices, Cloud–HPC integration, and open-source development of NextGen and related tools.Enhance research impact and community engagement for CIROH projects, while addressing computational challenges and facilitating experiments across scales.Strengthen cyberinfrastructure to support R2O2R (Research to Operations to Research) activities in hydrology.    ","version":"Next","tagName":"h2"},{"title":"Importance and Benefits for CIROH CyberInfrastructure​","type":1,"pageTitle":"CIROH CyberInfrastructure: Unleashing Potential in Hydrological Research","url":"/docuhub-staging/docs/services/intro#importance-and-benefits-for-ciroh-cyberinfrastructure","content":" CIROH CyberInfrastructure provides significant benefits to consortium members and partners in terms of research infrastructure:  Accessibility: Easy access for researchers regardless of their location or device.Scalability: Cloud-based resources dynamically adjust to meet changing research demands.Security: Protects data through robust confidentiality, integrity, and availability measures.Performance: Delivers high computational power and efficient data analysis capabilities.Collaboration: Enables seamless and secure sharing of data and resources among researchers.       CIROH JupyterHub Service CIROH AWS Services Pantarhei HPC CIROH Google Cloud Services ","version":"Next","tagName":"h2"},{"title":"Overview","type":0,"sectionRef":"#","url":"/docuhub-staging/docs/services/on-prem/","content":"","keywords":"","version":"Next"},{"title":"What is On-Premises services?​","type":1,"pageTitle":"Overview","url":"/docuhub-staging/docs/services/on-prem/#what-is-on-premises-services","content":" At CIROH, On-Premises services establish a comprehensive platform that facilitates the exchange of research data and access to computational resources and enables collaborative partnerships with academic peers locally and globally. A team of engineers and developers at the University of Alabama operates these services. Our overarching objective is to construct a dynamic &quot;network of services&quot; tailored to enable efficient organization, analysis, and dissemination of research data. Within On-Premises Research Computing, we structure our services around the following fundamental domains:  Hydroinformatics: This domain integrates hydrology with information technology, focusing on the development and application of computational tools and techniques for data management, analysis, modeling, and decision support in hydrological studies.Data Science and Big Data: Utilizing data-driven approaches and big data analytics to process and analyze large volumes of hydrological data from various sources, including remote sensing, sensors, and numerical models, to extract meaningful insights and patterns.Numerical Modeling and Simulation: Developing and implementing computational models and simulation techniques to simulate hydrological processes and phenomena, such as rainfall-runoff modeling, groundwater flow, and water quality modeling, to support scientific research and water resources management.Geographic Information Systems (GIS): Applying GIS technology to hydrological research by integrating spatial data with hydrological models, analyzing spatial patterns, and visualizing hydrological processes to understand spatial relationships and make informed decisions in water management.Machine Learning and Artificial Intelligence: Employing machine learning algorithms and AI techniques to enhance hydrological modeling, prediction, and decision-making by learning from data patterns, optimizing model parameters, and improving the accuracy of hydrological forecasts and simulations.  These domains reflect the interdisciplinary nature of hydrology and highlight the critical role of On-Premises High-performance computing (HPC) in advancing our understanding of water systems and addressing complex hydrological challenges.  In essence, CIROH's Research Computing services are a cornerstone for fostering interdisciplinary collaboration, enabling data-driven research, and advancing scientific discovery in hydrology and related domains.  ","version":"Next","tagName":"h2"},{"title":"Available clusters​","type":1,"pageTitle":"Overview","url":"/docuhub-staging/docs/services/on-prem/#available-clusters","content":"     🗃️ Pantarhei 4 items 🗃️ Wukong 3 items ","version":"Next","tagName":"h2"},{"title":"Pantarhei","type":0,"sectionRef":"#","url":"/docuhub-staging/docs/services/on-prem/Pantarhei/","content":"","keywords":"","version":"Next"},{"title":"Navigational Resources​","type":1,"pageTitle":"Pantarhei","url":"/docuhub-staging/docs/services/on-prem/Pantarhei/#navigational-resources","content":"   📄️ System Architecture System Architecture of Pantarhei 📄️ Obtaining an Account Obtain an account on Pantarhei 📄️ Accessing the System Access of On-Premises Cluster Pantarhei 🗃️ Running Jobs 1 item ","version":"Next","tagName":"h3"},{"title":"Accessing the System","type":0,"sectionRef":"#","url":"/docuhub-staging/docs/services/on-prem/Pantarhei/access","content":"","keywords":"","version":"Next"},{"title":"General overview​","type":1,"pageTitle":"Accessing the System","url":"/docuhub-staging/docs/services/on-prem/Pantarhei/access#general-overview","content":" To connect to Pantarhei using SSH, you must follow two high-level steps:  Connect to the University of Alabama (UA) NetworkConnect to the Secure Shell (SSH)  Obtain Pantarhei Access In the case that access to the Pantarhei system is unavailable to you, please follow the instructions on Obtaining an Account.  ","version":"Next","tagName":"h3"},{"title":"Connect to the Network​","type":1,"pageTitle":"Accessing the System","url":"/docuhub-staging/docs/services/on-prem/Pantarhei/access#connect-to-the-network","content":" University of Alabama (UA) requires users to use the Virtual private network (VPN) to connect to the UA campus network in order to connect to the Pantarhei cluster.  tip For more information on setting up a VPN, please visit the Office of Information Technology (OIT) website.  ","version":"Next","tagName":"h3"},{"title":"Connect to the SSH​","type":1,"pageTitle":"Accessing the System","url":"/docuhub-staging/docs/services/on-prem/Pantarhei/access#connect-to-the-ssh","content":"   MacOS and LinuxWindows Once you are connected to the VPN, follow these steps to access Pantarhei: Open a Terminal: Find Terminal in your local machine and open it. tip In MacOS, use Spotlight search (Command + Spacebar) and type Terminal to open a new terminal window. Connect via SSH: In the terminal, Use the SSH command to connect to Pantarhei. ssh &lt;USERNAME&gt;@pantarhei.ua.edu note Replace &lt;USERNAME&gt; with your actual Pantarhei username. Enter your Pantarhei password  We hope this guide helps you efficiently utilize the Pantarhei HPC system for your research needs. Happy computing! ","version":"Next","tagName":"h3"},{"title":"Obtaining an Account","type":0,"sectionRef":"#","url":"/docuhub-staging/docs/services/on-prem/Pantarhei/obtain","content":"","keywords":"","version":"Next"},{"title":"How to get access to Pantarhei?​","type":1,"pageTitle":"Obtaining an Account","url":"/docuhub-staging/docs/services/on-prem/Pantarhei/obtain#how-to-get-access-to-pantarhei","content":" Submit the On-Premise Infrastructure Request Form below to get access to Pantarhei:  We encourage PI of the project to start here: (select On-Premises Infrastructure Request Form and fill out details)   Infrastructure Request Form    Submit the On-premise Access Request form for individual user accounts on Pantarhei:   On-Premise Access Request Form    note For UA users, please submit On-Premises Access Request Form. For outside UA users, please start with VPN Access Request Form followed by On-Premises Access Request Form    Note: If you are unable to access the On-Premise forms, please contact the CIROH team at ciroh-it-admin@ua.edu for assistance. ","version":"Next","tagName":"h3"},{"title":"Accessing the Compute Nodes","type":0,"sectionRef":"#","url":"/docuhub-staging/docs/services/on-prem/Pantarhei/RunningJobs/computenode","content":"Accessing the Compute Nodes Pantarhei employs the Slurm Workload Manager for the purpose of job scheduling and management. Utilizing Slurm, a user initiates a request for resources and submits a job to a designated queue. Subsequently, the system undertakes the task of extracting jobs from the queues, assigning the requisite compute nodes, and executing the submitted tasks. Although users typically access the Slurm job scheduler by SSH-ing to a Pantarhei login node, it is imperative to emphasize that the recommended practice entails utilizing Slurm to submit work as a job, as opposed to executing computationally intensive tasks directly on a login node. Given that all users share the login nodes, running anything beyond minimal test jobs can adversely affect the collective ability of users to effectively utilize Pantarhei resources. Pantarhei's framework is tailored to accommodate the moderate-scale computational and data requirements of the majority of CIROH users. Users with allocations possess the capability to submit tasks to a diverse array of queues, each featuring distinct job size and walltime constraints. Dedicated sets of queues are allocated for CPU, GPU, and FPGA nodes, with typically shorter walltime and smaller job size limits translating to expedited turnaround times. Several additional considerations regarding Pantarhei queues merit attention: Pantarhei facilitates shared jobs, whereby multiple tasks can be executed on a single node. This approach enhances job throughput, maximizes overall system utilization, and fosters increased user accessibility to Pantarhei resources.Pantarhei accommodates long-running jobs, with run times extendable up to seven days for tasks utilizing up to 6 full nodes.The maximum permissible job size on Pantarhei is 240 cores. For tasks exceeding this threshold, users are advised to initiate a consulting ticket to engage in further discussion with Pantarhei support personnel.","keywords":"","version":"Next"},{"title":"Running Jobs","type":0,"sectionRef":"#","url":"/docuhub-staging/docs/services/on-prem/Pantarhei/RunningJobs/","content":"","keywords":"","version":"Next"},{"title":"Navigational Resources​","type":1,"pageTitle":"Running Jobs","url":"/docuhub-staging/docs/services/on-prem/Pantarhei/RunningJobs/#navigational-resources","content":"   📄️ Accessing the Compute Nodes Accessing compute notes in Pantarhei ","version":"Next","tagName":"h3"},{"title":"Wukong","type":0,"sectionRef":"#","url":"/docuhub-staging/docs/services/on-prem/Wukong/","content":"","keywords":"","version":"Next"},{"title":"Navigational Resources​","type":1,"pageTitle":"Wukong","url":"/docuhub-staging/docs/services/on-prem/Wukong/#navigational-resources","content":"   📄️ System Architecture System Architecture of Wukong 📄️ Obtaining an Account Obtain an account on Wukong 📄️ Accessing the System Access of On-Premises Cluster Wukong ","version":"Next","tagName":"h3"},{"title":"Hardware Specifications","type":0,"sectionRef":"#","url":"/docuhub-staging/docs/services/on-prem/Pantarhei/sysinfo","content":"","keywords":"","version":"Next"},{"title":"Network​","type":1,"pageTitle":"Hardware Specifications","url":"/docuhub-staging/docs/services/on-prem/Pantarhei/sysinfo#network","content":" All nodes are interconnected by a Mellanox InfiniBand switch with FDR 56 Gb/s networks. ","version":"Next","tagName":"h3"},{"title":"Obtaining an Account","type":0,"sectionRef":"#","url":"/docuhub-staging/docs/services/on-prem/Wukong/obtain","content":"","keywords":"","version":"Next"},{"title":"General overview​","type":1,"pageTitle":"Obtaining an Account","url":"/docuhub-staging/docs/services/on-prem/Wukong/obtain#general-overview","content":" To obtain an account, users will need to follow these step:  Submit On-premises Infrastructure Request Form and describe full project information and resouce requirements. warning This GitHub issue must be submitted by Principal Investigator (PI) of the project. Hint In the GitHub issue PI should mention following details: PI's Full NamePI's Affiliated InstitutePI's Affiliated Email AddressList of students who will need access, also please provide following details of each student Full Name of studentGitHub User Name of studentAffiliated Email Address of student Submit On-premise Access Request form in CIROH On-Premise Access Form. tip The administration of the Wukong cluster falls under the purview of the CIROH IT Computing group at the University of Alabama (UA). Consequently, individuals lacking UA credentials (MyBama ID, VPN User Name, and CWID) are obliged to complete the VPN Access Request form, followed by the On-premise Access Request form within CIROH On-Premise Access Form. Account Creation Creation of accounts on the Wukong system necessitates submission of individual forms by each respective user. For instance, a Principal Investigator (PI) submits a GitHub issue pertaining to a project, and three students require access to the Wukong system for project-related work, three separate account creation forms must be duly submitted during this process. ","version":"Next","tagName":"h3"},{"title":"Accessing the System","type":0,"sectionRef":"#","url":"/docuhub-staging/docs/services/on-prem/Wukong/access","content":"","keywords":"","version":"Next"},{"title":"General overview​","type":1,"pageTitle":"Accessing the System","url":"/docuhub-staging/docs/services/on-prem/Wukong/access#general-overview","content":" To connect to Wukong using SSH, you must follow two high-level steps:  Connect to the University of Alabama (UA) NetworkConnect to the Secure Shell (SSH)  Obtain Wukong Access In the case that access to the Wukong system is unavailable to you, please follow the instructions on Obtaining an Account.  ","version":"Next","tagName":"h3"},{"title":"Connect to the Network​","type":1,"pageTitle":"Accessing the System","url":"/docuhub-staging/docs/services/on-prem/Wukong/access#connect-to-the-network","content":" University of Alabama (UA) requires users to use the Virtual private network (VPN) to connect to the UA campus network in order to connect to the Wukong cluster.  tip For more information on setting up a VPN, please visit the Office of Information Technology (OIT) website.  ","version":"Next","tagName":"h3"},{"title":"Connect to the SSH​","type":1,"pageTitle":"Accessing the System","url":"/docuhub-staging/docs/services/on-prem/Wukong/access#connect-to-the-ssh","content":"   MacOS and LinuxWindows Once you are connected to the VPN, follow these steps to access Wukong: Open a Terminal: Find Terminal in your local machine and open it. tip In MacOS, use Spotlight search (Command + Spacebar) and type Terminal to open a new terminal window. Connect via SSH: In the terminal, Use the SSH command to connect to Wukong. ssh &lt;USERNAME&gt;@Wukong.ua.edu note Replace &lt;USERNAME&gt; with your actual Wukong username. Enter your Wukong password  We hope this guide helps you efficiently utilize the Wukong HPC system for your research needs. Happy computing! ","version":"Next","tagName":"h3"},{"title":"System Architecture","type":0,"sectionRef":"#","url":"/docuhub-staging/docs/services/on-prem/Wukong/sysinfo","content":"","keywords":"","version":"Next"},{"title":"Comupute Node​","type":1,"pageTitle":"System Architecture","url":"/docuhub-staging/docs/services/on-prem/Wukong/sysinfo#comupute-node","content":"  Compute Node Specifications Model\tIntel(R) Xeon(R) Platinum 8470 Number of nodes\t1 Sockets per node\t2 Cores per socket\t52 Cores per node\t208 Hardware threads per core\t2 Hardware threads per node\t416 Clock rate\t2.00GHz (3.80GHz max boost) RAM\t1024 GB DDR5-4800 Cache L1d cache: 4.9 MiB (104 instances) L1i cache: 3.3 MiB (104 instances) L2 cache: 208 MiB (104 instances) L3 cache: 210 MiB (2 instances) Local storage per node\t56 TB Number GPUs per node\t8 GPU model\tNVIDIA A100 SXM4 Memory per GPU\t80 GB  info Presently, the Wukong operates as a stand-alone, self-contained server, implying that the compute node is the login node.  ","version":"Next","tagName":"h3"},{"title":"Network​","type":1,"pageTitle":"System Architecture","url":"/docuhub-staging/docs/services/on-prem/Wukong/sysinfo#network","content":" The Wukong's all GPUs are fully interconnected with NVIDIA NVLink technology. ","version":"Next","tagName":"h3"},{"title":"CIROH Subdomain Request Form","type":0,"sectionRef":"#","url":"/docuhub-staging/docs/services/subdomain","content":"CIROH Subdomain Request Form To request a ciroh.org subdomain, please fill out the following form. CIROH Subdomain Request Form note Your request will be handled by DevOps staff at CIROH and if permitted will be given access to the research cloud or on-premise infrastructure.","keywords":"","version":"Next"}],"options":{"languages":["en"],"id":"default"}}