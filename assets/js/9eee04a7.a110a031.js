"use strict";(self.webpackChunkciroh_docuhub=self.webpackChunkciroh_docuhub||[]).push([["1179"],{82076:function(e,t,s){s.r(t),s.d(t,{frontMatter:()=>o,default:()=>d,toc:()=>u,metadata:()=>i,assets:()=>c,contentTitle:()=>r});var i=JSON.parse('{"id":"services/on-prem/Pantarhei/RunningJobs/computenode","title":"Accessing the Compute Nodes","description":"Accessing compute notes in Pantarhei","source":"@site/docs/services/on-prem/Pantarhei/RunningJobs/computenode.mdx","sourceDirName":"services/on-prem/Pantarhei/RunningJobs","slug":"/services/on-prem/Pantarhei/RunningJobs/computenode","permalink":"/docuhub-staging/docs/services/on-prem/Pantarhei/RunningJobs/computenode","draft":false,"unlisted":false,"editUrl":"https://github.com/CIROH-UA/ciroh-ua_website/edit/main/docs/services/on-prem/Pantarhei/RunningJobs/computenode.mdx","tags":[{"inline":true,"label":"CIROH","permalink":"/docuhub-staging/docs/tags/ciroh"},{"inline":true,"label":"Services","permalink":"/docuhub-staging/docs/tags/services"},{"inline":true,"label":"On-Premises Services","permalink":"/docuhub-staging/docs/tags/on-premises-services"},{"inline":true,"label":"Pantarhei","permalink":"/docuhub-staging/docs/tags/pantarhei"},{"inline":true,"label":"HPC","permalink":"/docuhub-staging/docs/tags/hpc"}],"version":"current","sidebarPosition":1,"frontMatter":{"sidebar_position":1,"title":"Accessing the Compute Nodes","description":"Accessing compute notes in Pantarhei","tags":["CIROH","Services","On-Premises Services","Pantarhei","HPC"]},"sidebar":"services","previous":{"title":"Running Jobs","permalink":"/docuhub-staging/docs/services/on-prem/Pantarhei/RunningJobs/"},"next":{"title":"Wukong","permalink":"/docuhub-staging/docs/services/on-prem/Wukong/"}}'),n=s(85893),a=s(50065);let o={sidebar_position:1,title:"Accessing the Compute Nodes",description:"Accessing compute notes in Pantarhei",tags:["CIROH","Services","On-Premises Services","Pantarhei","HPC"]},r=void 0,c={},u=[];function l(e){let t={a:"a",li:"li",ol:"ol",p:"p",...(0,a.a)(),...e.components};return(0,n.jsxs)(n.Fragment,{children:[(0,n.jsxs)(t.p,{children:["Pantarhei employs the ",(0,n.jsx)(t.a,{href:"https://slurm.schedmd.com/documentation.html",children:"Slurm Workload Manager"})," for the purpose of job scheduling and management. Utilizing Slurm, a user initiates a request for resources and submits a job to a designated queue. Subsequently, the system undertakes the task of extracting jobs from the queues, assigning the requisite compute nodes, and executing the submitted tasks. Although users typically access the Slurm job scheduler by SSH-ing to a Pantarhei login node, it is imperative to emphasize that the recommended practice entails utilizing Slurm to submit work as a job, as opposed to executing computationally intensive tasks directly on a login node. Given that all users share the login nodes, running anything beyond minimal test jobs can adversely affect the collective ability of users to effectively utilize Pantarhei resources."]}),"\n",(0,n.jsx)(t.p,{children:"Pantarhei's framework is tailored to accommodate the moderate-scale computational and data requirements of the majority of CIROH users. Users with allocations possess the capability to submit tasks to a diverse array of queues, each featuring distinct job size and walltime constraints. Dedicated sets of queues are allocated for CPU, GPU, and FPGA nodes, with typically shorter walltime and smaller job size limits translating to expedited turnaround times. Several additional considerations regarding Pantarhei queues merit attention:"}),"\n",(0,n.jsxs)(t.ol,{children:["\n",(0,n.jsx)(t.li,{children:"Pantarhei facilitates shared jobs, whereby multiple tasks can be executed on a single node. This approach enhances job throughput, maximizes overall system utilization, and fosters increased user accessibility to Pantarhei resources."}),"\n",(0,n.jsx)(t.li,{children:"Pantarhei accommodates long-running jobs, with run times extendable up to seven days for tasks utilizing up to 6 full nodes."}),"\n",(0,n.jsx)(t.li,{children:"The maximum permissible job size on Pantarhei is 240 cores. For tasks exceeding this threshold, users are advised to initiate a consulting ticket to engage in further discussion with Pantarhei support personnel."}),"\n"]})]})}function d(e={}){let{wrapper:t}={...(0,a.a)(),...e.components};return t?(0,n.jsx)(t,{...e,children:(0,n.jsx)(l,{...e})}):l(e)}},50065:function(e,t,s){s.d(t,{Z:()=>r,a:()=>o});var i=s(67294);let n={},a=i.createContext(n);function o(e){let t=i.useContext(a);return i.useMemo(function(){return"function"==typeof e?e(t):{...t,...e}},[t,e])}function r(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(n):e.components||n:o(e.components),i.createElement(a.Provider,{value:t},e.children)}}}]);